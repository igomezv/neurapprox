{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elitism succesfully imported\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import time, os\n",
    "import random\n",
    "import seaborn as sns\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Input, Dense, LeakyReLU\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorboard.plugins.hparams import api as hp\n",
    "\n",
    "from tensorflow.keras import activations\n",
    "from tensorflow.keras.losses import CategoricalCrossentropy\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "import random\n",
    "from math import floor\n",
    "from tensorflow.keras import models\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import optimizers\n",
    "from scipy.integrate import odeint\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split as split\n",
    "from scipy.stats import bernoulli\n",
    "\n",
    "# import deap\n",
    "from deap import base, creator, tools, algorithms\n",
    "\n",
    "from bitstring import BitArray\n",
    "\n",
    "from elitism import eaSimpleWithElitism, main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "\n",
    "# tf.config.optimizer.set_jit(True)\n",
    "scaler = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#algorithm for splitting the dataset into training and validation \n",
    "def split(X,Y,porcent): #porcent must be between 0 and 1, it is the asigned porcent to the training dataset.\n",
    "    n=floor(porcent*len(X))\n",
    "    index=random.sample(range(len(X)),n)\n",
    "    X_learn=[]\n",
    "    Y_learn=[]\n",
    "    for i in index:\n",
    "        X_learn.append(X[i])\n",
    "        Y_learn.append(Y[i])\n",
    "    X_val=np.delete(X,index, axis=0)\n",
    "    Y_val=np.delete(Y,index, axis=0)\n",
    "    \n",
    "    X_learn=np.array(X_learn)\n",
    "    Y_learn=np.array(Y_learn)\n",
    "    return X_learn,Y_learn,X_val,Y_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "O_m=np.arange(0.1,0.51,0.01)\n",
    "H_0=np.arange(66,81,1)\n",
    "t=np.linspace(0,-12,50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RHS(Omega_i, lna, gamma=0):\n",
    "    x, y, z, H = Omega_i\n",
    "    #x, y, z = Omega_i\n",
    "    pi = 3*x + 4*y\n",
    "    return [x*(-3 + pi), y*(-4 + pi), z*pi, -0.5*H*pi]\n",
    "    #return [x*(-3 + pi), y*(-4 + pi), z*pi]\n",
    "\n",
    "def EDO(t,Om,H0):\n",
    "    #t,Or,Om,Ol=X\n",
    "    Or=0.0001\n",
    "    Ol=1-Or-Om\n",
    "    #H0 = 70.\n",
    "    y0 = [Om, Or, Ol, H0]\n",
    "    result = odeint(RHS, y0, t)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lets generate the cartesian product between the intervals\n",
    "Y0=[]\n",
    "#este ciclo llena la lista fijando un Om y pasando todos los Or\n",
    "for i in O_m:\n",
    "    for j in H_0:\n",
    "        Y0.extend(EDO(t,i,j))\n",
    "Y0=np.array(Y0)\n",
    "\n",
    "X0=[]\n",
    "for Om in O_m:\n",
    "    for H0 in H_0:\n",
    "        for T in t:\n",
    "            X0.append([T,Om,H0])\n",
    "X0=np.array(X0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "scaler = preprocessing.MinMaxScaler(feature_range = (0,1))\n",
    "Y2 = scaler.fit_transform(Y0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feactures= \n",
      " [[  0.           0.1         66.        ]\n",
      " [ -0.24489796   0.1         66.        ]\n",
      " [ -0.48979592   0.1         66.        ]\n",
      " ...\n",
      " [-11.51020408   0.5         80.        ]\n",
      " [-11.75510204   0.5         80.        ]\n",
      " [-12.           0.5         80.        ]]\n",
      "\n",
      "\n",
      "labels= \n",
      " [[9.47515373e-02 0.00000000e+00 1.00000000e+00 0.00000000e+00]\n",
      " [1.83608702e-01 1.41110961e-04 9.01997379e-01 1.62358493e-10]\n",
      " [3.22333902e-01 4.33937896e-04 7.48914062e-01 4.77144941e-10]\n",
      " ...\n",
      " [4.20164900e-02 9.58104047e-01 5.58399792e-11 3.78984027e-01]\n",
      " [3.19464665e-02 9.68145182e-01 5.73926563e-11 6.15280156e-01]\n",
      " [2.39153274e-02 9.76153281e-01 5.75109456e-11 1.00000000e+00]]\n"
     ]
    }
   ],
   "source": [
    "#Now, here are the datasets\n",
    "print('feactures= \\n',X0)\n",
    "print('\\n')\n",
    "print('labels= \\n',Y2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_size = 0.8\n",
    "X_train, Y_train, X_test, Y_test = split(X0, Y2, split_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "SC_DEEP       = np.array([2,3,4])                           # Number of deep layers (8)\n",
    "SC_NUM_UNITS  = np.array([50,100,200]) # Number of fully conected neurons (16)\n",
    "SC_LEARNING   = np.array([1e-5,1e-4,5e-3])   # Learning rates (8)\n",
    "SC_BATCH      = np.array([4, 8])                            # Batch sizes (4)\n",
    "# SC_ACTIVATION = [f1, f2, f3, f4]                                      # Activation function layers (2)\n",
    "\n",
    "my_callbacks = [tf.keras.callbacks.EarlyStopping(\n",
    "                               monitor='val_loss', mode='min',\n",
    "                               min_delta=0, \n",
    "                               patience=10,\n",
    "                               verbose=1,\n",
    "                            #    baseline=0,\n",
    "                               restore_best_weights=1)\n",
    "#                 keras.callbacks.TensorBoard(\n",
    "#                                log_dir='./logs'),\n",
    "#                 keras.callbacks.ReduceLROnPlateau(\n",
    "#                                monitor='val_loss', factor=0.5,\n",
    "#                                patience=6, min_lr=0,\n",
    "#                                verbose=1)\n",
    "               ] \n",
    "    \n",
    "epochs = 500\n",
    "# epochs = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Produces train and val splits.\n",
    "X_test, Y_test, X_val, Y_val = split(X_test, Y_test, 0.5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_evaluate(ga_individual_solution):   \n",
    "    t = time.time()\n",
    "    t_total = 0\n",
    "    \n",
    "    # Decode GA solution to integer for window_size and num_units\n",
    "    deep_layers_bits   = BitArray(ga_individual_solution[0:1])     # (8)\n",
    "    num_units_bits     = BitArray(ga_individual_solution[1:2])     # (16)\n",
    "    learning_rate_bits = BitArray(ga_individual_solution[2:3])    # (8)\n",
    "    batch_size_bits    = BitArray(ga_individual_solution[3:4])   # (4)\n",
    "# #     activation_f_bits  = BitArray(ga_individual_solution[12:13])   # (2)   Solo se consideran las 2 primeras\n",
    "    \n",
    "    deep_layers   = SC_DEEP[deep_layers_bits.uint]\n",
    "    num_units     = SC_NUM_UNITS[num_units_bits.uint]\n",
    "    learning_rate = SC_LEARNING[learning_rate_bits.uint]\n",
    "    batch_size   = SC_BATCH[batch_size_bits.uint]\n",
    "#     activation_f  = SC_ACTIVATION[activation_f_bits.uint]\n",
    "\n",
    "    \n",
    "    print('\\n--------------- Starting trial:', population_size*(max_generations+1)-len(ss), \"---------------\")\n",
    "    print('Deep layers:',deep_layers,', Number of neurons:',num_units,\", Learning rate:\",learning_rate)\n",
    "#     print(\"-------------------------------------------------\")\n",
    "    \n",
    "    # Train model and predict on validation set\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(Dense(int(X_train.shape[1])))\n",
    "    \n",
    "    for i in range(deep_layers):        \n",
    "        model.add(Dense(num_units, activation='relu'))\n",
    "#             model.add(keras.layers.Dropout(0.3))\n",
    "    model.add(Dense(4, activation='linear'))\n",
    "    \n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate, beta_1=0.9, beta_2=0.999, epsilon=1e-3)\n",
    "    model.compile(optimizer=optimizer, loss='mse', metrics=['mean_squared_error'])\n",
    "    model.fit(X_train, Y_train, epochs=epochs, validation_data=(X_val, Y_val),\n",
    "              callbacks=my_callbacks, batch_size=batch_size, shuffle=False, verbose=0)\n",
    "    \n",
    "    loss, score = model.evaluate(X_val, Y_val)    \n",
    "    t = time.time()-t\n",
    "    ss.pop(0)\n",
    "    print(\"Loss:\", score, \", Elapsed time:\", t)\n",
    "    print(\"-------------------------------------------------\\n\")\n",
    "#     print(loss, score)\n",
    "\n",
    "    datos.append([deep_layers, num_units, learning_rate, batch_size, loss, score, t])\n",
    "    \n",
    "    return loss,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((24600, 3), (3075, 3), (3075, 3))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, X_test.shape, X_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deap import tools\n",
    "from deap import algorithms\n",
    "\n",
    "def eaSimpleWithElitism(population, toolbox, cxpb, mutpb, ngen, stats=None,\n",
    "             halloffame=None, verbose=__debug__):\n",
    "    \"\"\"This algorithm is similar to DEAP eaSimple() algorithm, with the modification that\n",
    "    halloffame is used to implement an elitism mechanism. The individuals contained in the\n",
    "    halloffame are directly injected into the next generation and are not subject to the\n",
    "    genetic operators of selection, crossover and mutation.\n",
    "    \"\"\"\n",
    "    logbook = tools.Logbook()\n",
    "    logbook.header = ['gen', 'nevals'] + (stats.fields if stats else [])\n",
    "\n",
    "    # Evaluate the individuals with an invalid fitness\n",
    "    invalid_ind = [ind for ind in population if not ind.fitness.valid]\n",
    "    fitnesses = toolbox.map(toolbox.evaluate, invalid_ind)\n",
    "    for ind, fit in zip(invalid_ind, fitnesses):\n",
    "        ind.fitness.values = fit\n",
    "\n",
    "    if halloffame is None:\n",
    "        raise ValueError(\"halloffame parameter must not be empty!\")\n",
    "\n",
    "    halloffame.update(population)\n",
    "    hof_size = len(halloffame.items) if halloffame.items else 0\n",
    "\n",
    "    record = stats.compile(population) if stats else {}\n",
    "    logbook.record(gen=0, nevals=len(invalid_ind), **record)\n",
    "    if verbose:\n",
    "        print(logbook.stream)\n",
    "\n",
    "    # Begin the generational process\n",
    "    for gen in range(1, ngen + 1):\n",
    "\n",
    "        # Select the next generation individuals\n",
    "        offspring = toolbox.select(population, len(population) - hof_size)\n",
    "\n",
    "        # Vary the pool of individuals\n",
    "        offspring = algorithms.varAnd(offspring, toolbox, cxpb, mutpb)\n",
    "\n",
    "        # Evaluate the individuals with an invalid fitness\n",
    "        invalid_ind = [ind for ind in offspring if not ind.fitness.valid]\n",
    "        fitnesses = toolbox.map(toolbox.evaluate, invalid_ind)\n",
    "        for ind, fit in zip(invalid_ind, fitnesses):\n",
    "            ind.fitness.values = fit\n",
    "\n",
    "        # add the best back to population:\n",
    "        offspring.extend(halloffame.items)\n",
    "\n",
    "        # Update the hall of fame with the generated individuals\n",
    "        halloffame.update(offspring)\n",
    "\n",
    "        # Replace the current population by the offspring\n",
    "        population[:] = offspring\n",
    "\n",
    "        # Append the current generation statistics to the logbook\n",
    "        record = stats.compile(population) if stats else {}\n",
    "        logbook.record(gen=gen, nevals=len(invalid_ind), **record)\n",
    "        if verbose:\n",
    "            print(logbook.stream)\n",
    "\n",
    "    return population, logbook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def geneticAlgorithm_with_elitism(population_size, max_generations, gene_length, k):\n",
    "    \n",
    "    # Genetic Algorithm constants:\n",
    "    P_CROSSOVER = 0.5        # probability for crossover\n",
    "    P_MUTATION = 0.5         # probability for mutating an individual\n",
    "    HALL_OF_FAME_SIZE = 1    # Best individuals that pass to the other generation\n",
    "    \n",
    "    # set the random seed:\n",
    "    toolbox = base.Toolbox()\n",
    "\n",
    "    # As we are trying to minimize the RMSE score, that's why using -1.0. \n",
    "    # In case, when you want to maximize accuracy for instance, use 1.0\n",
    "    creator.create('FitnessMin', base.Fitness, weights = [-1.0])\n",
    "    creator.create('Individual', list , fitness = creator.FitnessMin)\n",
    "\n",
    "    # create the individual operator to fill up an Individual instance:\n",
    "    toolbox.register('binary', bernoulli.rvs, 0.5)\n",
    "    toolbox.register('individual', tools.initRepeat, creator.Individual, toolbox.binary, n = gene_length)\n",
    "\n",
    "    # create the population operator to generate a list of individuals:\n",
    "    toolbox.register('population', tools.initRepeat, list , toolbox.individual)\n",
    "\n",
    "    # genetic operators:\n",
    "    toolbox.register('evaluate', train_evaluate)\n",
    "    toolbox.register('select', tools.selTournament, tournsize = 2)\n",
    "    toolbox.register('mutate', tools.mutFlipBit, indpb = 0.11)\n",
    "    toolbox.register('mate', tools.cxUniform, indpb = 0.5)\n",
    "    \n",
    "    # create initial population (generation 0):\n",
    "    population = toolbox.population(n=population_size)\n",
    "\n",
    "    # prepare the statistics object:\n",
    "    stats = tools.Statistics(lambda ind: ind.fitness.values)\n",
    "    stats.register(\"min\", np.min)\n",
    "    stats.register(\"avg\", np.mean)\n",
    "    stats.register(\"max\", np.max)\n",
    "\n",
    "    # define the hall-of-fame object:\n",
    "    hof = tools.HallOfFame(HALL_OF_FAME_SIZE)\n",
    "\n",
    "    # Genetic Algorithm flow with elitism:\n",
    "    population, logbook = eaSimpleWithElitism(population, toolbox, cxpb=P_CROSSOVER, mutpb=P_MUTATION,\n",
    "                                              ngen=max_generations, stats=stats, halloffame=hof, verbose=True)\n",
    "\n",
    "    # print info for best solution found:\n",
    "    best = hof.items[0]\n",
    "    print(\"-- Best Individual = \", best)\n",
    "    print(\"-- Best Fitness = \", best.fitness.values[0])\n",
    "\n",
    "    # extract statistics:\n",
    "    minFitnessValues, meanFitnessValues, maxFitnessValues = logbook.select(\"min\", \"max\", \"avg\")\n",
    "\n",
    "    # plot statistics:\n",
    "    sns.set_style(\"whitegrid\")\n",
    "    plt.plot(minFitnessValues, color='blue', label=\"Min\")\n",
    "    plt.plot(meanFitnessValues, color='green', label=\"Mean\")\n",
    "    plt.plot(maxFitnessValues, color='red', label=\"Max\")\n",
    "    plt.xlabel('Generation'); plt.ylabel('Max / Min / Average Fitness')\n",
    "    plt.legend()\n",
    "    plt.title('Max, Min and Average fitness over Generations')\n",
    "    plt.show()\n",
    "    \n",
    "    best_population = tools.selBest(population,k = k)\n",
    "    return best_population"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--------------- Starting trial: 1 ---------------\n",
      "Deep layers: 3 , Number of neurons: 100 , Learning rate: 0.0001\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00136: early stopping\n",
      "97/97 [==============================] - 0s 1ms/step - loss: 5.2789e-05 - mean_squared_error: 5.2789e-05\n",
      "Loss: 5.2788935136049986e-05 , Elapsed time: 827.7036390304565\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 2 ---------------\n",
      "Deep layers: 2 , Number of neurons: 100 , Learning rate: 1e-05\n",
      "97/97 [==============================] - 0s 1ms/step - loss: 0.0014 - mean_squared_error: 0.0014\n",
      "Loss: 0.0013539374340325594 , Elapsed time: 2712.362644433975\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 3 ---------------\n",
      "Deep layers: 3 , Number of neurons: 50 , Learning rate: 0.0001\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00201: early stopping\n",
      "97/97 [==============================] - 0s 718us/step - loss: 1.8021e-04 - mean_squared_error: 1.8021e-04\n",
      "Loss: 0.00018021432333625853 , Elapsed time: 804.290709733963\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 4 ---------------\n",
      "Deep layers: 3 , Number of neurons: 100 , Learning rate: 1e-05\n",
      "97/97 [==============================] - 0s 725us/step - loss: 3.9585e-04 - mean_squared_error: 3.9585e-04\n",
      "Loss: 0.0003958477173000574 , Elapsed time: 2852.961726665497\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 5 ---------------\n",
      "Deep layers: 2 , Number of neurons: 100 , Learning rate: 0.0001\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00150: early stopping\n",
      "97/97 [==============================] - 0s 731us/step - loss: 4.9428e-04 - mean_squared_error: 4.9428e-04\n",
      "Loss: 0.0004942776286043227 , Elapsed time: 385.25672793388367\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 6 ---------------\n",
      "Deep layers: 3 , Number of neurons: 100 , Learning rate: 1e-05\n",
      "97/97 [==============================] - 0s 732us/step - loss: 3.3496e-04 - mean_squared_error: 3.3496e-04\n",
      "Loss: 0.0003349621838424355 , Elapsed time: 1493.2509109973907\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 7 ---------------\n",
      "Deep layers: 2 , Number of neurons: 100 , Learning rate: 1e-05\n",
      "97/97 [==============================] - 0s 749us/step - loss: 0.0025 - mean_squared_error: 0.0025\n",
      "Loss: 0.0025203549303114414 , Elapsed time: 1326.7561688423157\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 8 ---------------\n",
      "Deep layers: 3 , Number of neurons: 50 , Learning rate: 1e-05\n",
      "97/97 [==============================] - 0s 714us/step - loss: 0.0016 - mean_squared_error: 0.0016\n",
      "Loss: 0.0015885204775258899 , Elapsed time: 1336.4106059074402\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 9 ---------------\n",
      "Deep layers: 2 , Number of neurons: 100 , Learning rate: 1e-05\n",
      "97/97 [==============================] - 0s 748us/step - loss: 0.0025 - mean_squared_error: 0.0025\n",
      "Loss: 0.002544017508625984 , Elapsed time: 1308.0399062633514\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 10 ---------------\n",
      "Deep layers: 3 , Number of neurons: 100 , Learning rate: 0.0001\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00088: early stopping\n",
      "97/97 [==============================] - 0s 805us/step - loss: 1.3275e-04 - mean_squared_error: 1.3275e-04\n",
      "Loss: 0.0001327524078078568 , Elapsed time: 265.18419671058655\n",
      "-------------------------------------------------\n",
      "\n",
      "gen\tnevals\tmin        \tavg        \tmax       \n",
      "0  \t10    \t5.27889e-05\t0.000959767\t0.00254402\n",
      "\n",
      "--------------- Starting trial: 11 ---------------\n",
      "Deep layers: 2 , Number of neurons: 100 , Learning rate: 0.0001\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00272: early stopping\n",
      "97/97 [==============================] - 0s 784us/step - loss: 2.0746e-04 - mean_squared_error: 2.0746e-04\n",
      "Loss: 0.00020745754591189325 , Elapsed time: 728.9837403297424\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 12 ---------------\n",
      "Deep layers: 3 , Number of neurons: 100 , Learning rate: 1e-05\n",
      "97/97 [==============================] - 0s 796us/step - loss: 5.4182e-04 - mean_squared_error: 5.4182e-04\n",
      "Loss: 0.0005418173968791962 , Elapsed time: 3039.752556324005\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 13 ---------------\n",
      "Deep layers: 3 , Number of neurons: 100 , Learning rate: 1e-05\n",
      "97/97 [==============================] - 0s 824us/step - loss: 2.5393e-04 - mean_squared_error: 2.5393e-04\n",
      "Loss: 0.00025393132818862796 , Elapsed time: 3033.5513072013855\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 14 ---------------\n",
      "Deep layers: 3 , Number of neurons: 100 , Learning rate: 0.0001\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00209: early stopping\n",
      "97/97 [==============================] - 0s 770us/step - loss: 4.9407e-05 - mean_squared_error: 4.9407e-05\n",
      "Loss: 4.940651706419885e-05 , Elapsed time: 632.0907411575317\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 15 ---------------\n",
      "Deep layers: 2 , Number of neurons: 100 , Learning rate: 1e-05\n",
      "97/97 [==============================] - 0s 664us/step - loss: 0.0011 - mean_squared_error: 0.0011\n",
      "Loss: 0.0010784040205180645 , Elapsed time: 1319.8018004894257\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 16 ---------------\n",
      "Deep layers: 3 , Number of neurons: 100 , Learning rate: 0.0001\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00098: early stopping\n",
      "97/97 [==============================] - 0s 798us/step - loss: 3.1488e-04 - mean_squared_error: 3.1488e-04\n",
      "Loss: 0.0003148771938867867 , Elapsed time: 570.9505121707916\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 17 ---------------\n",
      "Deep layers: 3 , Number of neurons: 100 , Learning rate: 1e-05\n",
      "97/97 [==============================] - 0s 748us/step - loss: 0.0014 - mean_squared_error: 0.0014\n",
      "Loss: 0.0013891136040911078 , Elapsed time: 1469.4527034759521\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 18 ---------------\n",
      "Deep layers: 3 , Number of neurons: 100 , Learning rate: 1e-05\n",
      "97/97 [==============================] - 0s 773us/step - loss: 9.3177e-04 - mean_squared_error: 9.3177e-04\n",
      "Loss: 0.0009317704825662076 , Elapsed time: 1468.519808769226\n",
      "-------------------------------------------------\n",
      "\n",
      "1  \t8     \t4.94065e-05\t0.000515453\t0.00138911\n",
      "\n",
      "--------------- Starting trial: 19 ---------------\n",
      "Deep layers: 3 , Number of neurons: 100 , Learning rate: 0.0001\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00163: early stopping\n",
      "97/97 [==============================] - 0s 799us/step - loss: 1.1166e-04 - mean_squared_error: 1.1166e-04\n",
      "Loss: 0.00011165838077431545 , Elapsed time: 488.3434190750122\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 20 ---------------\n",
      "Deep layers: 3 , Number of neurons: 100 , Learning rate: 0.0001\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00128: early stopping\n",
      "97/97 [==============================] - 0s 769us/step - loss: 1.4462e-04 - mean_squared_error: 1.4462e-04\n",
      "Loss: 0.00014462159015238285 , Elapsed time: 386.6856598854065\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 21 ---------------\n",
      "Deep layers: 3 , Number of neurons: 100 , Learning rate: 0.0001\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00078: early stopping\n",
      "97/97 [==============================] - 0s 782us/step - loss: 3.9084e-04 - mean_squared_error: 3.9084e-04\n",
      "Loss: 0.0003908437502104789 , Elapsed time: 237.3467288017273\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 22 ---------------\n",
      "Deep layers: 3 , Number of neurons: 100 , Learning rate: 1e-05\n",
      "97/97 [==============================] - 0s 710us/step - loss: 5.6847e-04 - mean_squared_error: 5.6847e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.0005684663192369044 , Elapsed time: 1511.0710890293121\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 23 ---------------\n",
      "Deep layers: 3 , Number of neurons: 100 , Learning rate: 0.0001\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00159: early stopping\n",
      "97/97 [==============================] - 0s 767us/step - loss: 8.4594e-05 - mean_squared_error: 8.4594e-05\n",
      "Loss: 8.459424861939624e-05 , Elapsed time: 940.8253786563873\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 24 ---------------\n",
      "Deep layers: 2 , Number of neurons: 100 , Learning rate: 0.0001\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00145: early stopping\n",
      "97/97 [==============================] - 0s 708us/step - loss: 4.0869e-04 - mean_squared_error: 4.0869e-04\n",
      "Loss: 0.00040869464282877743 , Elapsed time: 395.7719101905823\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 25 ---------------\n",
      "Deep layers: 3 , Number of neurons: 100 , Learning rate: 1e-05\n",
      "97/97 [==============================] - 0s 728us/step - loss: 1.4306e-04 - mean_squared_error: 1.4306e-04\n",
      "Loss: 0.0001430629490641877 , Elapsed time: 2945.840215444565\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 26 ---------------\n",
      "Deep layers: 3 , Number of neurons: 100 , Learning rate: 0.0001\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00075: early stopping\n",
      "97/97 [==============================] - 0s 768us/step - loss: 1.0054e-04 - mean_squared_error: 1.0054e-04\n",
      "Loss: 0.00010054229642264545 , Elapsed time: 243.51316118240356\n",
      "-------------------------------------------------\n",
      "\n",
      "2  \t8     \t4.94065e-05\t0.000205468\t0.000568466\n",
      "\n",
      "--------------- Starting trial: 27 ---------------\n",
      "Deep layers: 3 , Number of neurons: 100 , Learning rate: 0.0001\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00077: early stopping\n",
      "97/97 [==============================] - 0s 712us/step - loss: 3.8732e-04 - mean_squared_error: 3.8732e-04\n",
      "Loss: 0.00038732439861632884 , Elapsed time: 487.2087182998657\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 28 ---------------\n",
      "Deep layers: 3 , Number of neurons: 100 , Learning rate: 1e-05\n",
      "97/97 [==============================] - 0s 819us/step - loss: 2.0583e-04 - mean_squared_error: 2.0583e-04\n",
      "Loss: 0.00020583327568601817 , Elapsed time: 3143.7149465084076\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 29 ---------------\n",
      "Deep layers: 3 , Number of neurons: 100 , Learning rate: 0.0001\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00106: early stopping\n",
      "97/97 [==============================] - 0s 745us/step - loss: 3.9097e-04 - mean_squared_error: 3.9097e-04\n",
      "Loss: 0.00039096540422178805 , Elapsed time: 661.6881544589996\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 30 ---------------\n",
      "Deep layers: 2 , Number of neurons: 100 , Learning rate: 1e-05\n",
      "97/97 [==============================] - 0s 692us/step - loss: 0.0014 - mean_squared_error: 0.0014\n",
      "Loss: 0.0013824569759890437 , Elapsed time: 1410.9436259269714\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 31 ---------------\n",
      "Deep layers: 3 , Number of neurons: 100 , Learning rate: 1e-05\n",
      "97/97 [==============================] - 0s 800us/step - loss: 0.0011 - mean_squared_error: 0.0011\n",
      "Loss: 0.0011181904701516032 , Elapsed time: 1552.2462038993835\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 32 ---------------\n",
      "Deep layers: 3 , Number of neurons: 100 , Learning rate: 0.0001\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00092: early stopping\n",
      "97/97 [==============================] - 0s 764us/step - loss: 2.4518e-04 - mean_squared_error: 2.4518e-04\n",
      "Loss: 0.0002451831242069602 , Elapsed time: 532.9564533233643\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 33 ---------------\n",
      "Deep layers: 3 , Number of neurons: 100 , Learning rate: 0.0001\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00103: early stopping\n",
      "97/97 [==============================] - 0s 764us/step - loss: 2.4937e-04 - mean_squared_error: 2.4937e-04\n",
      "Loss: 0.0002493730280548334 , Elapsed time: 301.2800769805908\n",
      "-------------------------------------------------\n",
      "\n",
      "3  \t7     \t4.94065e-05\t0.000416612\t0.00138246 \n",
      "\n",
      "--------------- Starting trial: 34 ---------------\n",
      "Deep layers: 2 , Number of neurons: 100 , Learning rate: 1e-05\n",
      "97/97 [==============================] - 0s 688us/step - loss: 5.6743e-04 - mean_squared_error: 5.6743e-04\n",
      "Loss: 0.0005674256244674325 , Elapsed time: 2573.356537103653\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 35 ---------------\n",
      "Deep layers: 3 , Number of neurons: 50 , Learning rate: 1e-05\n",
      "97/97 [==============================] - 0s 693us/step - loss: 9.9596e-04 - mean_squared_error: 9.9596e-04\n",
      "Loss: 0.0009959560120478272 , Elapsed time: 1321.892100572586\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 36 ---------------\n",
      "Deep layers: 3 , Number of neurons: 100 , Learning rate: 0.0001\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00108: early stopping\n",
      "97/97 [==============================] - 0s 639us/step - loss: 6.7565e-05 - mean_squared_error: 6.7565e-05\n",
      "Loss: 6.756542279617861e-05 , Elapsed time: 612.2885422706604\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 37 ---------------\n",
      "Deep layers: 3 , Number of neurons: 100 , Learning rate: 0.0001\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00202: early stopping\n",
      "97/97 [==============================] - 0s 766us/step - loss: 5.3503e-05 - mean_squared_error: 5.3503e-05\n",
      "Loss: 5.350315404939465e-05 , Elapsed time: 616.8435690402985\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 38 ---------------\n",
      "Deep layers: 3 , Number of neurons: 100 , Learning rate: 0.0001\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00136: early stopping\n",
      "97/97 [==============================] - 0s 842us/step - loss: 1.4854e-04 - mean_squared_error: 1.4854e-04\n",
      "Loss: 0.00014853858738206327 , Elapsed time: 811.4718821048737\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 39 ---------------\n",
      "Deep layers: 3 , Number of neurons: 100 , Learning rate: 0.0001\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00054: early stopping\n",
      "97/97 [==============================] - 0s 624us/step - loss: 9.7383e-04 - mean_squared_error: 9.7383e-04\n",
      "Loss: 0.0009738298249430954 , Elapsed time: 324.8079023361206\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 40 ---------------\n",
      "Deep layers: 3 , Number of neurons: 100 , Learning rate: 0.0001\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00156: early stopping\n",
      "97/97 [==============================] - 0s 795us/step - loss: 2.0978e-05 - mean_squared_error: 2.0978e-05\n",
      "Loss: 2.0978224711143412e-05 , Elapsed time: 929.9468269348145\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 41 ---------------\n",
      "Deep layers: 2 , Number of neurons: 100 , Learning rate: 0.0001\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00122: early stopping\n",
      "97/97 [==============================] - 0s 767us/step - loss: 2.6183e-04 - mean_squared_error: 2.6183e-04\n",
      "Loss: 0.0002618302241899073 , Elapsed time: 639.5113520622253\n",
      "-------------------------------------------------\n",
      "\n",
      "4  \t8     \t2.09782e-05\t0.000319182\t0.000995956\n",
      "\n",
      "--------------- Starting trial: 42 ---------------\n",
      "Deep layers: 3 , Number of neurons: 50 , Learning rate: 1e-05\n",
      "97/97 [==============================] - 0s 740us/step - loss: 0.0020 - mean_squared_error: 0.0020\n",
      "Loss: 0.0019862938206642866 , Elapsed time: 1351.4166836738586\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 43 ---------------\n",
      "Deep layers: 2 , Number of neurons: 100 , Learning rate: 0.0001\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00137: early stopping\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "97/97 [==============================] - 0s 765us/step - loss: 2.4986e-04 - mean_squared_error: 2.4986e-04\n",
      "Loss: 0.0002498605172149837 , Elapsed time: 363.16847920417786\n",
      "-------------------------------------------------\n",
      "\n",
      "5  \t2     \t2.09782e-05\t0.000327995\t0.00198629 \n",
      "-- Best Individual =  [1, 1, 1, 0]\n",
      "-- Best Fitness =  2.0978224711143412e-05\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total elapsed time: 839.3284882505735 minutes\n"
     ]
    }
   ],
   "source": [
    "population_size = 10   # max of individuals per generation\n",
    "max_generations = 5    # number of generations\n",
    "gene_length = 4       # lenght of the gene, depends on how many hiperparameters are tested  \n",
    "k = 1;                 # num. of finalist individuals\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    t = time.time(); \n",
    "    datos = [];\n",
    "    ss = [i for i in range(1,population_size*(max_generations+1))]\n",
    "    best_population = geneticAlgorithm_with_elitism(population_size, max_generations, gene_length, k)\n",
    "    print(\"Total elapsed time:\", (time.time()-t)/60, \"minutes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k= 1 \n",
      "Deep layers: 3\n",
      "Number of neurons: 100 , Batch size 4 , Learning rate: 0.0001\n"
     ]
    }
   ],
   "source": [
    "best_deep_layers   = []\n",
    "best_num_units     = []\n",
    "best_learning_rate = []\n",
    "best_batch_size    = []\n",
    "# best_activation_f  = []\n",
    "best_f_names       = []\n",
    "\n",
    "t = 0\n",
    "\n",
    "for bi in best_population:\n",
    "    deep_layers_bits   = BitArray(bi[0:1])    # (8)\n",
    "    num_units_bits     = BitArray(bi[1:2])    # (16)\n",
    "    learning_rate_bits = BitArray(bi[2:3])   # (8)\n",
    "    batch_size_bits    = BitArray(bi[3:4])  # (4)\n",
    "#     activation_f_bits  = BitArray(bi[12:13])  # (2)\n",
    "    t += 1 \n",
    "    \n",
    "    best_deep_layers.append(SC_DEEP[deep_layers_bits.uint])\n",
    "    best_num_units.append(SC_NUM_UNITS[num_units_bits.uint])\n",
    "    best_learning_rate.append(SC_LEARNING[learning_rate_bits.uint])\n",
    "    best_batch_size.append(SC_BATCH[batch_size_bits.uint])\n",
    "#     best_activation_f.append(SC_ACTIVATION[activation_f_bits.uint])\n",
    "#     best_f_names.append(f_names[activation_f_bits.uint])\n",
    "    \n",
    "    print('k=',t,'\\nDeep layers:', best_deep_layers[-1])\n",
    "    print('Number of neurons:', best_num_units[-1], ', Batch size', best_batch_size[-1], ', Learning rate:', best_learning_rate[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Deep layers</th>\n",
       "      <th>Num units</th>\n",
       "      <th>Learning rate</th>\n",
       "      <th>Batch size</th>\n",
       "      <th>Loss</th>\n",
       "      <th>Score</th>\n",
       "      <th>Elapsed time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>100</td>\n",
       "      <td>0.00010</td>\n",
       "      <td>4</td>\n",
       "      <td>0.000021</td>\n",
       "      <td>0.000021</td>\n",
       "      <td>929.946827</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>100</td>\n",
       "      <td>0.00010</td>\n",
       "      <td>8</td>\n",
       "      <td>0.000049</td>\n",
       "      <td>0.000049</td>\n",
       "      <td>632.090741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>100</td>\n",
       "      <td>0.00010</td>\n",
       "      <td>8</td>\n",
       "      <td>0.000053</td>\n",
       "      <td>0.000053</td>\n",
       "      <td>827.703639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>100</td>\n",
       "      <td>0.00010</td>\n",
       "      <td>8</td>\n",
       "      <td>0.000054</td>\n",
       "      <td>0.000054</td>\n",
       "      <td>616.843569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>100</td>\n",
       "      <td>0.00010</td>\n",
       "      <td>4</td>\n",
       "      <td>0.000068</td>\n",
       "      <td>0.000068</td>\n",
       "      <td>612.288542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>3</td>\n",
       "      <td>100</td>\n",
       "      <td>0.00010</td>\n",
       "      <td>4</td>\n",
       "      <td>0.000085</td>\n",
       "      <td>0.000085</td>\n",
       "      <td>940.825379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>3</td>\n",
       "      <td>100</td>\n",
       "      <td>0.00010</td>\n",
       "      <td>8</td>\n",
       "      <td>0.000101</td>\n",
       "      <td>0.000101</td>\n",
       "      <td>243.513161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>3</td>\n",
       "      <td>100</td>\n",
       "      <td>0.00010</td>\n",
       "      <td>8</td>\n",
       "      <td>0.000112</td>\n",
       "      <td>0.000112</td>\n",
       "      <td>488.343419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>3</td>\n",
       "      <td>100</td>\n",
       "      <td>0.00010</td>\n",
       "      <td>8</td>\n",
       "      <td>0.000133</td>\n",
       "      <td>0.000133</td>\n",
       "      <td>265.184197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>3</td>\n",
       "      <td>100</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>4</td>\n",
       "      <td>0.000143</td>\n",
       "      <td>0.000143</td>\n",
       "      <td>2945.840215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>3</td>\n",
       "      <td>100</td>\n",
       "      <td>0.00010</td>\n",
       "      <td>8</td>\n",
       "      <td>0.000145</td>\n",
       "      <td>0.000145</td>\n",
       "      <td>386.685660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>3</td>\n",
       "      <td>100</td>\n",
       "      <td>0.00010</td>\n",
       "      <td>4</td>\n",
       "      <td>0.000149</td>\n",
       "      <td>0.000149</td>\n",
       "      <td>811.471882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>3</td>\n",
       "      <td>50</td>\n",
       "      <td>0.00010</td>\n",
       "      <td>8</td>\n",
       "      <td>0.000180</td>\n",
       "      <td>0.000180</td>\n",
       "      <td>804.290710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>3</td>\n",
       "      <td>100</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>4</td>\n",
       "      <td>0.000206</td>\n",
       "      <td>0.000206</td>\n",
       "      <td>3143.714947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2</td>\n",
       "      <td>100</td>\n",
       "      <td>0.00010</td>\n",
       "      <td>8</td>\n",
       "      <td>0.000207</td>\n",
       "      <td>0.000207</td>\n",
       "      <td>728.983740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>3</td>\n",
       "      <td>100</td>\n",
       "      <td>0.00010</td>\n",
       "      <td>4</td>\n",
       "      <td>0.000245</td>\n",
       "      <td>0.000245</td>\n",
       "      <td>532.956453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>3</td>\n",
       "      <td>100</td>\n",
       "      <td>0.00010</td>\n",
       "      <td>8</td>\n",
       "      <td>0.000249</td>\n",
       "      <td>0.000249</td>\n",
       "      <td>301.280077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2</td>\n",
       "      <td>100</td>\n",
       "      <td>0.00010</td>\n",
       "      <td>8</td>\n",
       "      <td>0.000250</td>\n",
       "      <td>0.000250</td>\n",
       "      <td>363.168479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>3</td>\n",
       "      <td>100</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>4</td>\n",
       "      <td>0.000254</td>\n",
       "      <td>0.000254</td>\n",
       "      <td>3033.551307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2</td>\n",
       "      <td>100</td>\n",
       "      <td>0.00010</td>\n",
       "      <td>4</td>\n",
       "      <td>0.000262</td>\n",
       "      <td>0.000262</td>\n",
       "      <td>639.511352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>3</td>\n",
       "      <td>100</td>\n",
       "      <td>0.00010</td>\n",
       "      <td>4</td>\n",
       "      <td>0.000315</td>\n",
       "      <td>0.000315</td>\n",
       "      <td>570.950512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>3</td>\n",
       "      <td>100</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>8</td>\n",
       "      <td>0.000335</td>\n",
       "      <td>0.000335</td>\n",
       "      <td>1493.250911</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>3</td>\n",
       "      <td>100</td>\n",
       "      <td>0.00010</td>\n",
       "      <td>4</td>\n",
       "      <td>0.000387</td>\n",
       "      <td>0.000387</td>\n",
       "      <td>487.208718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>3</td>\n",
       "      <td>100</td>\n",
       "      <td>0.00010</td>\n",
       "      <td>8</td>\n",
       "      <td>0.000391</td>\n",
       "      <td>0.000391</td>\n",
       "      <td>237.346729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>3</td>\n",
       "      <td>100</td>\n",
       "      <td>0.00010</td>\n",
       "      <td>4</td>\n",
       "      <td>0.000391</td>\n",
       "      <td>0.000391</td>\n",
       "      <td>661.688154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>3</td>\n",
       "      <td>100</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>4</td>\n",
       "      <td>0.000396</td>\n",
       "      <td>0.000396</td>\n",
       "      <td>2852.961727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>2</td>\n",
       "      <td>100</td>\n",
       "      <td>0.00010</td>\n",
       "      <td>8</td>\n",
       "      <td>0.000409</td>\n",
       "      <td>0.000409</td>\n",
       "      <td>395.771910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>2</td>\n",
       "      <td>100</td>\n",
       "      <td>0.00010</td>\n",
       "      <td>8</td>\n",
       "      <td>0.000494</td>\n",
       "      <td>0.000494</td>\n",
       "      <td>385.256728</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>3</td>\n",
       "      <td>100</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>4</td>\n",
       "      <td>0.000542</td>\n",
       "      <td>0.000542</td>\n",
       "      <td>3039.752556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>2</td>\n",
       "      <td>100</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>4</td>\n",
       "      <td>0.000567</td>\n",
       "      <td>0.000567</td>\n",
       "      <td>2573.356537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>3</td>\n",
       "      <td>100</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>8</td>\n",
       "      <td>0.000568</td>\n",
       "      <td>0.000568</td>\n",
       "      <td>1511.071089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>3</td>\n",
       "      <td>100</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>8</td>\n",
       "      <td>0.000932</td>\n",
       "      <td>0.000932</td>\n",
       "      <td>1468.519809</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>3</td>\n",
       "      <td>100</td>\n",
       "      <td>0.00010</td>\n",
       "      <td>4</td>\n",
       "      <td>0.000974</td>\n",
       "      <td>0.000974</td>\n",
       "      <td>324.807902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>3</td>\n",
       "      <td>50</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>8</td>\n",
       "      <td>0.000996</td>\n",
       "      <td>0.000996</td>\n",
       "      <td>1321.892101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>2</td>\n",
       "      <td>100</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>8</td>\n",
       "      <td>0.001078</td>\n",
       "      <td>0.001078</td>\n",
       "      <td>1319.801800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>3</td>\n",
       "      <td>100</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>8</td>\n",
       "      <td>0.001118</td>\n",
       "      <td>0.001118</td>\n",
       "      <td>1552.246204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>2</td>\n",
       "      <td>100</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>8</td>\n",
       "      <td>0.001354</td>\n",
       "      <td>0.001354</td>\n",
       "      <td>2712.362644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>2</td>\n",
       "      <td>100</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>8</td>\n",
       "      <td>0.001382</td>\n",
       "      <td>0.001382</td>\n",
       "      <td>1410.943626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>3</td>\n",
       "      <td>100</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>8</td>\n",
       "      <td>0.001389</td>\n",
       "      <td>0.001389</td>\n",
       "      <td>1469.452703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>3</td>\n",
       "      <td>50</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>8</td>\n",
       "      <td>0.001589</td>\n",
       "      <td>0.001589</td>\n",
       "      <td>1336.410606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>3</td>\n",
       "      <td>50</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>8</td>\n",
       "      <td>0.001986</td>\n",
       "      <td>0.001986</td>\n",
       "      <td>1351.416684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>2</td>\n",
       "      <td>100</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>8</td>\n",
       "      <td>0.002520</td>\n",
       "      <td>0.002520</td>\n",
       "      <td>1326.756169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>2</td>\n",
       "      <td>100</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>8</td>\n",
       "      <td>0.002544</td>\n",
       "      <td>0.002544</td>\n",
       "      <td>1308.039906</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Deep layers  Num units  Learning rate  Batch size      Loss     Score  \\\n",
       "0             3        100        0.00010           4  0.000021  0.000021   \n",
       "1             3        100        0.00010           8  0.000049  0.000049   \n",
       "2             3        100        0.00010           8  0.000053  0.000053   \n",
       "3             3        100        0.00010           8  0.000054  0.000054   \n",
       "4             3        100        0.00010           4  0.000068  0.000068   \n",
       "5             3        100        0.00010           4  0.000085  0.000085   \n",
       "6             3        100        0.00010           8  0.000101  0.000101   \n",
       "7             3        100        0.00010           8  0.000112  0.000112   \n",
       "8             3        100        0.00010           8  0.000133  0.000133   \n",
       "9             3        100        0.00001           4  0.000143  0.000143   \n",
       "10            3        100        0.00010           8  0.000145  0.000145   \n",
       "11            3        100        0.00010           4  0.000149  0.000149   \n",
       "12            3         50        0.00010           8  0.000180  0.000180   \n",
       "13            3        100        0.00001           4  0.000206  0.000206   \n",
       "14            2        100        0.00010           8  0.000207  0.000207   \n",
       "15            3        100        0.00010           4  0.000245  0.000245   \n",
       "16            3        100        0.00010           8  0.000249  0.000249   \n",
       "17            2        100        0.00010           8  0.000250  0.000250   \n",
       "18            3        100        0.00001           4  0.000254  0.000254   \n",
       "19            2        100        0.00010           4  0.000262  0.000262   \n",
       "20            3        100        0.00010           4  0.000315  0.000315   \n",
       "21            3        100        0.00001           8  0.000335  0.000335   \n",
       "22            3        100        0.00010           4  0.000387  0.000387   \n",
       "23            3        100        0.00010           8  0.000391  0.000391   \n",
       "24            3        100        0.00010           4  0.000391  0.000391   \n",
       "25            3        100        0.00001           4  0.000396  0.000396   \n",
       "26            2        100        0.00010           8  0.000409  0.000409   \n",
       "27            2        100        0.00010           8  0.000494  0.000494   \n",
       "28            3        100        0.00001           4  0.000542  0.000542   \n",
       "29            2        100        0.00001           4  0.000567  0.000567   \n",
       "30            3        100        0.00001           8  0.000568  0.000568   \n",
       "31            3        100        0.00001           8  0.000932  0.000932   \n",
       "32            3        100        0.00010           4  0.000974  0.000974   \n",
       "33            3         50        0.00001           8  0.000996  0.000996   \n",
       "34            2        100        0.00001           8  0.001078  0.001078   \n",
       "35            3        100        0.00001           8  0.001118  0.001118   \n",
       "36            2        100        0.00001           8  0.001354  0.001354   \n",
       "37            2        100        0.00001           8  0.001382  0.001382   \n",
       "38            3        100        0.00001           8  0.001389  0.001389   \n",
       "39            3         50        0.00001           8  0.001589  0.001589   \n",
       "40            3         50        0.00001           8  0.001986  0.001986   \n",
       "41            2        100        0.00001           8  0.002520  0.002520   \n",
       "42            2        100        0.00001           8  0.002544  0.002544   \n",
       "\n",
       "    Elapsed time  \n",
       "0     929.946827  \n",
       "1     632.090741  \n",
       "2     827.703639  \n",
       "3     616.843569  \n",
       "4     612.288542  \n",
       "5     940.825379  \n",
       "6     243.513161  \n",
       "7     488.343419  \n",
       "8     265.184197  \n",
       "9    2945.840215  \n",
       "10    386.685660  \n",
       "11    811.471882  \n",
       "12    804.290710  \n",
       "13   3143.714947  \n",
       "14    728.983740  \n",
       "15    532.956453  \n",
       "16    301.280077  \n",
       "17    363.168479  \n",
       "18   3033.551307  \n",
       "19    639.511352  \n",
       "20    570.950512  \n",
       "21   1493.250911  \n",
       "22    487.208718  \n",
       "23    237.346729  \n",
       "24    661.688154  \n",
       "25   2852.961727  \n",
       "26    395.771910  \n",
       "27    385.256728  \n",
       "28   3039.752556  \n",
       "29   2573.356537  \n",
       "30   1511.071089  \n",
       "31   1468.519809  \n",
       "32    324.807902  \n",
       "33   1321.892101  \n",
       "34   1319.801800  \n",
       "35   1552.246204  \n",
       "36   2712.362644  \n",
       "37   1410.943626  \n",
       "38   1469.452703  \n",
       "39   1336.410606  \n",
       "40   1351.416684  \n",
       "41   1326.756169  \n",
       "42   1308.039906  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filename = \"historial_genetic_ecsdiff.txt\"\n",
    "df = pd.DataFrame(datos, columns = [\"Deep layers\", \"Num units\", \"Learning rate\", \"Batch size\", \"Loss\", \"Score\", \"Elapsed time\"])\n",
    "\n",
    "df.sort_values(by=[\"Loss\", \"Elapsed time\"], ascending=[True, True], ignore_index=True, inplace=True)\n",
    "\n",
    "df.to_csv(filename, header=True, index=False, sep='\\t', mode='w') # a=append, w=overwrite\n",
    "df.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time 839.324 minutes\n"
     ]
    }
   ],
   "source": [
    "total_time = float(np.sum(df[[\"Elapsed time\"]])/60)\n",
    "\n",
    "print(\"Elapsed time {:.3f} minutes\".format(total_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
