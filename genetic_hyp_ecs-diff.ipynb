{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install deap\n",
        "!pip install bitstring\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "!cp /content/drive/MyDrive/neuroapprox/elitism.py /content"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DYc45CWnJvmp",
        "outputId": "913009d2-a2c5-4caf-b0e6-8f70a0ffb456"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting deap\n",
            "  Downloading deap-1.3.1-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (160 kB)\n",
            "\u001b[K     |████████████████████████████████| 160 kB 4.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from deap) (1.21.6)\n",
            "Installing collected packages: deap\n",
            "Successfully installed deap-1.3.1\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting bitstring\n",
            "  Downloading bitstring-3.1.9-py3-none-any.whl (38 kB)\n",
            "Installing collected packages: bitstring\n",
            "Successfully installed bitstring-3.1.9\n",
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WadF7ROCIukU",
        "outputId": "e057ae8c-a2a7-41fe-dcf7-9dcbe9ee6c00"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Elitism succesfully imported\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "import time, os\n",
        "import random\n",
        "import seaborn as sns\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.layers import Input, Dense, LeakyReLU\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorboard.plugins.hparams import api as hp\n",
        "\n",
        "from tensorflow.keras import activations\n",
        "from tensorflow.keras.losses import CategoricalCrossentropy\n",
        "from tensorflow.keras.models import Model\n",
        "\n",
        "import random\n",
        "from math import floor\n",
        "from tensorflow.keras import models\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import optimizers\n",
        "from scipy.integrate import odeint\n",
        "\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split as split\n",
        "from scipy.stats import bernoulli\n",
        "\n",
        "# import deap\n",
        "from deap import base, creator, tools, algorithms\n",
        "\n",
        "from bitstring import BitArray\n",
        "\n",
        "from elitism import eaSimpleWithElitism, main"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "CB_1KdgUIukr"
      },
      "outputs": [],
      "source": [
        "np.random.seed(10)\n",
        "\n",
        "# tf.config.optimizer.set_jit(True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "J9p2IgUMIukt"
      },
      "outputs": [],
      "source": [
        "#algorithm for splitting the dataset into training and validation \n",
        "def split(X,Y,porcent): #porcent must be between 0 and 1, it is the asigned porcent to the training dataset.\n",
        "    n=floor(porcent*len(X))\n",
        "    index=random.sample(range(len(X)),n)\n",
        "    X_learn=[]\n",
        "    Y_learn=[]\n",
        "    for i in index:\n",
        "        X_learn.append(X[i])\n",
        "        Y_learn.append(Y[i])\n",
        "    X_val=np.delete(X,index, axis=0)\n",
        "    Y_val=np.delete(Y,index, axis=0)\n",
        "    \n",
        "    X_learn=np.array(X_learn)\n",
        "    Y_learn=np.array(Y_learn)\n",
        "    return X_learn,Y_learn,X_val,Y_val"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "2cbr88iMIukw"
      },
      "outputs": [],
      "source": [
        "O_m=np.arange(0.1,0.51,0.01)\n",
        "H_0=np.arange(66,81,1)\n",
        "t=np.linspace(0,-12,50)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "rF3YyuMiIuky"
      },
      "outputs": [],
      "source": [
        "def RHS(Omega_i, lna, gamma=0):\n",
        "    x, y, z, H = Omega_i\n",
        "    #x, y, z = Omega_i\n",
        "    pi = 3*x + 4*y\n",
        "    return [x*(-3 + pi), y*(-4 + pi), z*pi, -0.5*H*pi]\n",
        "    #return [x*(-3 + pi), y*(-4 + pi), z*pi]\n",
        "\n",
        "def EDO(t,Om,H0):\n",
        "    #t,Or,Om,Ol=X\n",
        "    Or=0.0001\n",
        "    Ol=1-Or-Om\n",
        "    #H0 = 70.\n",
        "    y0 = [Om, Or, Ol, H0]\n",
        "    result = odeint(RHS, y0, t)\n",
        "    return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "9eyIdxXQIukz"
      },
      "outputs": [],
      "source": [
        "#lets generate the cartesian product between the intervals\n",
        "Y0=[]\n",
        "#este ciclo llena la lista fijando un Om y pasando todos los Or\n",
        "for i in O_m:\n",
        "    for j in H_0:\n",
        "        Y0.extend(EDO(t,i,j))\n",
        "Y0=np.array(Y0)\n",
        "\n",
        "X0=[]\n",
        "for Om in O_m:\n",
        "    for H0 in H_0:\n",
        "        for T in t:\n",
        "            X0.append([T,Om,H0])\n",
        "X0=np.array(X0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "MI4XZKdFIuk1"
      },
      "outputs": [],
      "source": [
        "from sklearn import preprocessing\n",
        "scaler = preprocessing.MinMaxScaler(feature_range = (0,1))\n",
        "Y2 = scaler.fit_transform(Y0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gC1N_CJLIuk3",
        "outputId": "68fd04b9-0fa9-4f34-c7a2-7e6f22fb4d70"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "feactures= \n",
            " [[  0.           0.1         66.        ]\n",
            " [ -0.24489796   0.1         66.        ]\n",
            " [ -0.48979592   0.1         66.        ]\n",
            " ...\n",
            " [-11.51020408   0.5         80.        ]\n",
            " [-11.75510204   0.5         80.        ]\n",
            " [-12.           0.5         80.        ]]\n",
            "\n",
            "\n",
            "labels= \n",
            " [[9.47515373e-02 0.00000000e+00 1.00000000e+00 0.00000000e+00]\n",
            " [1.83608702e-01 1.41110961e-04 9.01997379e-01 1.62358493e-10]\n",
            " [3.22333902e-01 4.33937896e-04 7.48914062e-01 4.77144941e-10]\n",
            " ...\n",
            " [4.20164900e-02 9.58104047e-01 5.58399792e-11 3.78984027e-01]\n",
            " [3.19464665e-02 9.68145182e-01 5.73926563e-11 6.15280156e-01]\n",
            " [2.39153274e-02 9.76153281e-01 5.75109456e-11 1.00000000e+00]]\n"
          ]
        }
      ],
      "source": [
        "#Now, here are the datasets\n",
        "print('feactures= \\n',X0)\n",
        "print('\\n')\n",
        "print('labels= \\n',Y2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "cg5rKwuTIuk6"
      },
      "outputs": [],
      "source": [
        "split_size = 0.5\n",
        "X_train, Y_train, X_test, Y_test = split(X0, Y2, split_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "CpTfwnfVIuk8"
      },
      "outputs": [],
      "source": [
        "SC_DEEP       = np.array([2, 4, 6, 8, 10, 12], dtype=int)      # Number of deep layers (4)\n",
        "SC_NUM_UNITS  = np.array([10, 50, 100, 150], dtype=int)         # Number of  neurons (3)\n",
        "SC_LEARNING   = np.array([-4,-3], dtype=float)                 # Learning rates (3)\n",
        "SC_BATCH      = np.array([4, 8, 16, 32], dtype=int)                         # Batch sizes (2)\n",
        "# SC_ACTIVATION = [f1, f2, f3, f4]                 # Activation function layers (2)\n",
        "\n",
        "BOUNDS = [SC_DEEP,SC_NUM_UNITS,SC_LEARNING,SC_BATCH]\n",
        "DOWN, UP = [], []\n",
        "for i in range(len(BOUNDS)):\n",
        "    DOWN.append(BOUNDS[i][0])\n",
        "    UP.append(BOUNDS[i][-1])\n",
        "\n",
        "DIMS = len(BOUNDS)             # boundaries for all dimensions\n",
        "\n",
        "my_callbacks = [tf.keras.callbacks.EarlyStopping(\n",
        "                               monitor='val_loss', mode='min',\n",
        "                               min_delta=1e-7, \n",
        "                               patience=10,\n",
        "                               verbose=1,\n",
        "                            #    baseline=0,\n",
        "                               restore_best_weights=1)\n",
        "#                 keras.callbacks.ReduceLROnPlateau(\n",
        "#                                monitor='val_loss', factor=0.5,\n",
        "#                                patience=6, min_lr=0,\n",
        "#                                verbose=1)\n",
        "               ] \n",
        "    \n",
        "epochs = 100\n",
        "# epochs = 1"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# BOUNDS[1][0]\n",
        "# DOWN, \n",
        "# UP"
      ],
      "metadata": {
        "id": "RkWQH1r1zGly"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "5e07dsBBIulB"
      },
      "outputs": [],
      "source": [
        "# Produces train and val splits.\n",
        "X_test, Y_test, X_val, Y_val = split(X_test, Y_test, 0.5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "gNc-UrBaIulF"
      },
      "outputs": [],
      "source": [
        "def train_evaluate(ga_individual_solution):   \n",
        "    t = time.time()\n",
        "    t_total = 0\n",
        "    \n",
        "    # Decode GA solution to integer for window_size and num_units\n",
        "    deep_layers   = int(ga_individual_solution[0])   \n",
        "    num_units     = int(ga_individual_solution[1])\n",
        "    learning_rate = 10**(ga_individual_solution[2])\n",
        "    batch_size    = int(ga_individual_solution[3])    \n",
        "# #     activation_f  = BitArray(ga_individual_solution[4])   # (2)   Solo se consideran las 2 primeras\n",
        "    \n",
        "    print('\\n--------------- Starting trial:', population_size*(max_generations+1)-len(ss), \"---------------\")\n",
        "    print('Deep layers:',deep_layers,', Number of neurons:',num_units,\", Learning rate:\",learning_rate,', Batch size:',batch_size)\n",
        "    \n",
        "    # Train model and predict on validation set\n",
        "    model = tf.keras.Sequential()\n",
        "    model.add(Dense(int(X_train.shape[1])))\n",
        "    \n",
        "    for i in range(deep_layers):        \n",
        "        model.add(Dense(num_units, activation='elu'))\n",
        "#             model.add(keras.layers.Dropout(0.3))\n",
        "    model.add(Dense(4, activation='linear'))\n",
        "    \n",
        "    optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate, beta_1=0.9, beta_2=0.999, epsilon=1e-3)\n",
        "    model.compile(optimizer=optimizer, loss='mse', metrics=['mean_squared_error'])\n",
        "    # model.compile(optimizer=optimizer, loss='mse')\n",
        "    model.fit(X_train, Y_train, epochs=epochs, validation_data=(X_val, Y_val),\n",
        "              callbacks=my_callbacks, batch_size=batch_size, shuffle=False, verbose=0)\n",
        "    \n",
        "    loss, score = model.evaluate(X_val, Y_val)    \n",
        "    t = time.time()-t\n",
        "    ss.pop(0)\n",
        "    print(\"Loss:\", score, \", Elapsed time:\", t)\n",
        "    print(\"-------------------------------------------------\\n\")\n",
        "#     print(loss, score)\n",
        "\n",
        "    # datos.append([deep_layers, num_units, learning_rate, batch_size loss, score, t])\n",
        "    datos.append([deep_layers, num_units, learning_rate, batch_size, loss, score, t])\n",
        "    \n",
        "    return loss,"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bbb5Ka2WIulH",
        "outputId": "daf61e50-f29b-41ca-f493-26d5ba1e393d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((15375, 3), (7687, 3), (7688, 3))"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "X_train.shape, X_test.shape, X_val.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "UfeWOrpHIulJ"
      },
      "outputs": [],
      "source": [
        "from deap import tools\n",
        "from deap import algorithms\n",
        "\n",
        "def eaSimpleWithElitism(population, toolbox, cxpb, mutpb, ngen, stats=None,\n",
        "             halloffame=None, verbose=__debug__):\n",
        "    \"\"\"This algorithm is similar to DEAP eaSimple() algorithm, with the modification that\n",
        "    halloffame is used to implement an elitism mechanism. The individuals contained in the\n",
        "    halloffame are directly injected into the next generation and are not subject to the\n",
        "    genetic operators of selection, crossover and mutation.\n",
        "    \"\"\"\n",
        "    logbook = tools.Logbook()\n",
        "    logbook.header = ['gen', 'nevals'] + (stats.fields if stats else [])\n",
        "\n",
        "    # Evaluate the individuals with an invalid fitness\n",
        "    invalid_ind = [ind for ind in population if not ind.fitness.valid]\n",
        "    fitnesses = toolbox.map(toolbox.evaluate, invalid_ind)\n",
        "    for ind, fit in zip(invalid_ind, fitnesses):\n",
        "        ind.fitness.values = fit\n",
        "\n",
        "    if halloffame is None:\n",
        "        raise ValueError(\"halloffame parameter must not be empty!\")\n",
        "\n",
        "    halloffame.update(population)\n",
        "    hof_size = len(halloffame.items) if halloffame.items else 0\n",
        "\n",
        "    record = stats.compile(population) if stats else {}\n",
        "    logbook.record(gen=0, nevals=len(invalid_ind), **record)\n",
        "    if verbose:\n",
        "        print(logbook.stream)\n",
        "\n",
        "    # Begin the generational process\n",
        "    for gen in range(1, ngen + 1):\n",
        "\n",
        "        # Select the next generation individuals\n",
        "        offspring = toolbox.select(population, len(population) - hof_size)\n",
        "\n",
        "        # Vary the pool of individuals\n",
        "        offspring = algorithms.varAnd(offspring, toolbox, cxpb, mutpb)\n",
        "\n",
        "        # Evaluate the individuals with an invalid fitness\n",
        "        invalid_ind = [ind for ind in offspring if not ind.fitness.valid]\n",
        "        fitnesses = toolbox.map(toolbox.evaluate, invalid_ind)\n",
        "        for ind, fit in zip(invalid_ind, fitnesses):\n",
        "            ind.fitness.values = fit\n",
        "\n",
        "        # add the best back to population:\n",
        "        offspring.extend(halloffame.items)\n",
        "\n",
        "        # Update the hall of fame with the generated individuals\n",
        "        halloffame.update(offspring)\n",
        "\n",
        "        # Replace the current population by the offspring\n",
        "        population[:] = offspring\n",
        "\n",
        "        # Append the current generation statistics to the logbook\n",
        "        record = stats.compile(population) if stats else {}\n",
        "        logbook.record(gen=gen, nevals=len(invalid_ind), **record)\n",
        "        if verbose:\n",
        "            print(logbook.stream)\n",
        "\n",
        "    return population, logbook"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "Ye9sghsFIulM"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import math\n",
        "\n",
        "def geneticAlgorithm_with_elitism(population_size, max_generations, gene_length, k):\n",
        "    \n",
        "    # Genetic Algorithm constants:\n",
        "    P_CROSSOVER = 0.9        # probability for crossover\n",
        "    P_MUTATION = 0.9         # probability for mutating an individual\n",
        "    HALL_OF_FAME_SIZE = 1    # Best individuals that pass to the other generation\n",
        "    CROWDING_FACTOR = 5.0   # crowding factor for crossover and mutation\n",
        "    \n",
        "    # set the random seed:\n",
        "    toolbox = base.Toolbox()\n",
        "\n",
        "    # As we are trying to minimize the RMSE score, that's why using -1.0. \n",
        "    # In case, when you want to maximize accuracy for instance, use 1.0\n",
        "    creator.create('FitnessMin', base.Fitness, weights = [-1.0])\n",
        "    creator.create('Individual', list , fitness = creator.FitnessMin)\n",
        "\n",
        "    # helper function for creating random real numbers uniformly distributed within a given range [low, up]\n",
        "    def randomInt(BOUNDS, DIMS):\n",
        "        temp = []\n",
        "        for i in range(DIMS):\n",
        "            temp.append(random.choices(BOUNDS[i])[0])\n",
        "        return temp\n",
        "\n",
        "    # create an operator that randomly returns a float in the desired range and dimension:\n",
        "    toolbox.register(\"attrInt\", randomInt, BOUNDS, DIMS)\n",
        "\n",
        "    # create the individual operator to fill up an Individual instance:\n",
        "    toolbox.register('individual', tools.initIterate, creator.Individual, toolbox.attrInt)\n",
        "\n",
        "    # create the population operator to generate a list of individuals:\n",
        "    toolbox.register('population', tools.initRepeat, list , toolbox.individual)\n",
        "\n",
        "    # genetic operators:\n",
        "    toolbox.register('evaluate', train_evaluate)\n",
        "    toolbox.register('select', tools.selTournament, tournsize = 2)\n",
        "    toolbox.register(\"mutate\", tools.mutUniformInt, low=DOWN, up=UP, indpb=1.2/DIMS)\n",
        "    toolbox.register(\"mate\", tools.cxSimulatedBinaryBounded, low=DOWN, up=UP, eta=CROWDING_FACTOR)\n",
        "    # toolbox.register(\"mutate\", tools.mutPolynomialBounded, low=DOWN, up=UP, eta=CROWDING_FACTOR, indpb=1.0/DIMS)\n",
        "    # toolbox.register('mate', tools.cxUniform, indpb = 0.5)\n",
        "    \n",
        "    # create initial population (generation 0):\n",
        "    population = toolbox.population(n=population_size)\n",
        "\n",
        "    # prepare the statistics object:\n",
        "    stats = tools.Statistics(lambda ind: ind.fitness.values)\n",
        "    stats.register(\"min\", np.min)\n",
        "    stats.register(\"avg\", np.mean)\n",
        "    stats.register(\"max\", np.max)\n",
        "\n",
        "    # define the hall-of-fame object:\n",
        "    hof = tools.HallOfFame(HALL_OF_FAME_SIZE)\n",
        "\n",
        "    # Genetic Algorithm flow with elitism:\n",
        "    population, logbook = eaSimpleWithElitism(population, toolbox, cxpb=P_CROSSOVER, mutpb=P_MUTATION,\n",
        "                                              ngen=max_generations, stats=stats, halloffame=hof, verbose=True)\n",
        "\n",
        "    # print info for best solution found:\n",
        "    best = hof.items[0]\n",
        "    print(\"-- Best Individual = \", best)\n",
        "    print(\"-- Best Fitness = \", best.fitness.values[0])\n",
        "\n",
        "    # extract statistics:\n",
        "    minFitnessValues, meanFitnessValues, maxFitnessValues = logbook.select(\"min\", \"avg\", \"max\")\n",
        "\n",
        "    # plot statistics:\n",
        "    sns.set_style(\"whitegrid\")\n",
        "    plt.plot(minFitnessValues, color='blue', label=\"Min\")\n",
        "    plt.plot(meanFitnessValues, color='green', label=\"Mean\")\n",
        "    plt.plot(maxFitnessValues, color='red', label=\"Max\")\n",
        "    plt.xlabel('Generation'); plt.ylabel('Max / Min / Average Fitness')\n",
        "    plt.legend()\n",
        "    plt.title('Max, Min and Average fitness over Generations')\n",
        "    plt.show()\n",
        "    \n",
        "    best_population = tools.selBest(population,k = k)\n",
        "    return best_population"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "0ZV3VdbnIulP",
        "outputId": "158d978c-47a6-4808-d9de-5a96f855b3b4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--------------- Starting trial: 1 ---------------\n",
            "Deep layers: 6 , Number of neurons: 150 , Learning rate: 0.0001 , Batch size: 8\n",
            "Restoring model weights from the end of the best epoch: 79.\n",
            "Epoch 89: early stopping\n",
            "241/241 [==============================] - 1s 3ms/step - loss: 7.7606e-05 - mean_squared_error: 7.7606e-05\n",
            "Loss: 7.760573498671874e-05 , Elapsed time: 624.3387084007263\n",
            "-------------------------------------------------\n",
            "\n",
            "\n",
            "--------------- Starting trial: 2 ---------------\n",
            "Deep layers: 10 , Number of neurons: 150 , Learning rate: 0.0001 , Batch size: 32\n",
            "Restoring model weights from the end of the best epoch: 59.\n",
            "Epoch 69: early stopping\n",
            "241/241 [==============================] - 1s 3ms/step - loss: 0.0023 - mean_squared_error: 0.0023\n",
            "Loss: 0.0023486579302698374 , Elapsed time: 226.73950600624084\n",
            "-------------------------------------------------\n",
            "\n",
            "\n",
            "--------------- Starting trial: 3 ---------------\n",
            "Deep layers: 2 , Number of neurons: 10 , Learning rate: 0.0001 , Batch size: 16\n",
            "241/241 [==============================] - 0s 1ms/step - loss: 0.0033 - mean_squared_error: 0.0033\n",
            "Loss: 0.003293805057182908 , Elapsed time: 622.881139755249\n",
            "-------------------------------------------------\n",
            "\n",
            "\n",
            "--------------- Starting trial: 4 ---------------\n",
            "Deep layers: 12 , Number of neurons: 10 , Learning rate: 0.0001 , Batch size: 8\n",
            "241/241 [==============================] - 1s 2ms/step - loss: 6.3749e-05 - mean_squared_error: 6.3749e-05\n",
            "Loss: 6.374949589371681e-05 , Elapsed time: 1564.1621277332306\n",
            "-------------------------------------------------\n",
            "\n",
            "\n",
            "--------------- Starting trial: 5 ---------------\n",
            "Deep layers: 4 , Number of neurons: 100 , Learning rate: 0.001 , Batch size: 4\n",
            "Restoring model weights from the end of the best epoch: 22.\n",
            "Epoch 32: early stopping\n",
            "241/241 [==============================] - 1s 2ms/step - loss: 1.1282e-04 - mean_squared_error: 1.1282e-04\n",
            "Loss: 0.00011281953629804775 , Elapsed time: 300.3789122104645\n",
            "-------------------------------------------------\n",
            "\n",
            "\n",
            "--------------- Starting trial: 6 ---------------\n",
            "Deep layers: 6 , Number of neurons: 150 , Learning rate: 0.001 , Batch size: 4\n",
            "Restoring model weights from the end of the best epoch: 36.\n",
            "Epoch 46: early stopping\n",
            "241/241 [==============================] - 1s 3ms/step - loss: 4.7622e-05 - mean_squared_error: 4.7622e-05\n",
            "Loss: 4.7621990233892575e-05 , Elapsed time: 571.0565764904022\n",
            "-------------------------------------------------\n",
            "\n",
            "\n",
            "--------------- Starting trial: 7 ---------------\n",
            "Deep layers: 8 , Number of neurons: 50 , Learning rate: 0.0001 , Batch size: 32\n",
            "241/241 [==============================] - 0s 2ms/step - loss: 3.6430e-05 - mean_squared_error: 3.6430e-05\n",
            "Loss: 3.643043964984827e-05 , Elapsed time: 443.4538154602051\n",
            "-------------------------------------------------\n",
            "\n",
            "gen\tnevals\tmin        \tavg        \tmax       \n",
            "0  \t7     \t3.64304e-05\t0.000854384\t0.00329381\n",
            "\n",
            "--------------- Starting trial: 8 ---------------\n",
            "Deep layers: 11 , Number of neurons: 81 , Learning rate: 0.00011149507486087533 , Batch size: 4\n",
            "Restoring model weights from the end of the best epoch: 19.\n",
            "Epoch 29: early stopping\n",
            "241/241 [==============================] - 1s 3ms/step - loss: 0.0029 - mean_squared_error: 0.0029\n",
            "Loss: 0.00289866141974926 , Elapsed time: 385.51585030555725\n",
            "-------------------------------------------------\n",
            "\n",
            "\n",
            "--------------- Starting trial: 9 ---------------\n",
            "Deep layers: 4 , Number of neurons: 31 , Learning rate: 0.0008969006041278596 , Batch size: 8\n",
            "Restoring model weights from the end of the best epoch: 210.\n",
            "Epoch 220: early stopping\n",
            "241/241 [==============================] - 1s 2ms/step - loss: 9.1038e-06 - mean_squared_error: 9.1038e-06\n",
            "Loss: 9.103820048039779e-06 , Elapsed time: 939.7571306228638\n",
            "-------------------------------------------------\n",
            "\n",
            "\n",
            "--------------- Starting trial: 10 ---------------\n",
            "Deep layers: 4 , Number of neurons: 100 , Learning rate: 0.001 , Batch size: 4\n",
            "Restoring model weights from the end of the best epoch: 63.\n",
            "Epoch 73: early stopping\n",
            "241/241 [==============================] - 1s 2ms/step - loss: 1.1091e-04 - mean_squared_error: 1.1091e-04\n",
            "Loss: 0.00011090628686361015 , Elapsed time: 743.2453017234802\n",
            "-------------------------------------------------\n",
            "\n",
            "\n",
            "--------------- Starting trial: 11 ---------------\n",
            "Deep layers: 6 , Number of neurons: 150 , Learning rate: 0.001 , Batch size: 4\n",
            "Restoring model weights from the end of the best epoch: 41.\n",
            "Epoch 51: early stopping\n",
            "241/241 [==============================] - 1s 3ms/step - loss: 3.6224e-05 - mean_squared_error: 3.6224e-05\n",
            "Loss: 3.6224031646270305e-05 , Elapsed time: 683.5067820549011\n",
            "-------------------------------------------------\n",
            "\n",
            "\n",
            "--------------- Starting trial: 12 ---------------\n",
            "Deep layers: 6 , Number of neurons: 103 , Learning rate: 0.001 , Batch size: 4\n",
            "Restoring model weights from the end of the best epoch: 38.\n",
            "Epoch 48: early stopping\n",
            "241/241 [==============================] - 1s 3ms/step - loss: 6.1360e-05 - mean_squared_error: 6.1360e-05\n",
            "Loss: 6.13595184404403e-05 , Elapsed time: 520.5724651813507\n",
            "-------------------------------------------------\n",
            "\n",
            "\n",
            "--------------- Starting trial: 13 ---------------\n",
            "Deep layers: 6 , Number of neurons: 109 , Learning rate: 0.001 , Batch size: 4\n",
            "Restoring model weights from the end of the best epoch: 24.\n",
            "Epoch 34: early stopping\n",
            "241/241 [==============================] - 1s 2ms/step - loss: 0.0025 - mean_squared_error: 0.0025\n",
            "Loss: 0.002527946140617132 , Elapsed time: 374.2037205696106\n",
            "-------------------------------------------------\n",
            "\n",
            "1  \t6     \t9.10382e-06\t0.000811519\t0.00289866\n",
            "\n",
            "--------------- Starting trial: 14 ---------------\n",
            "Deep layers: 3 , Number of neurons: 126 , Learning rate: 0.001 , Batch size: 32\n",
            "Restoring model weights from the end of the best epoch: 39.\n",
            "Epoch 49: early stopping\n",
            "241/241 [==============================] - 0s 2ms/step - loss: 1.9828e-04 - mean_squared_error: 1.9828e-04\n",
            "Loss: 0.00019828471704386175 , Elapsed time: 68.55555844306946\n",
            "-------------------------------------------------\n",
            "\n",
            "\n",
            "--------------- Starting trial: 15 ---------------\n",
            "Deep layers: 6 , Number of neurons: 55 , Learning rate: 0.001 , Batch size: 4\n",
            "Restoring model weights from the end of the best epoch: 27.\n",
            "Epoch 37: early stopping\n",
            "241/241 [==============================] - 1s 2ms/step - loss: 1.4626e-04 - mean_squared_error: 1.4626e-04\n",
            "Loss: 0.0001462619547965005 , Elapsed time: 361.188378572464\n",
            "-------------------------------------------------\n",
            "\n",
            "\n",
            "--------------- Starting trial: 16 ---------------\n",
            "Deep layers: 4 , Number of neurons: 45 , Learning rate: 0.001 , Batch size: 4\n",
            "Restoring model weights from the end of the best epoch: 21.\n",
            "Epoch 31: early stopping\n",
            "241/241 [==============================] - 1s 2ms/step - loss: 1.0692e-04 - mean_squared_error: 1.0692e-04\n",
            "Loss: 0.000106916377262678 , Elapsed time: 279.4867572784424\n",
            "-------------------------------------------------\n",
            "\n",
            "\n",
            "--------------- Starting trial: 17 ---------------\n",
            "Deep layers: 6 , Number of neurons: 31 , Learning rate: 0.0009946155348226208 , Batch size: 8\n",
            "Restoring model weights from the end of the best epoch: 35.\n",
            "Epoch 45: early stopping\n",
            "241/241 [==============================] - 1s 2ms/step - loss: 1.4714e-04 - mean_squared_error: 1.4714e-04\n",
            "Loss: 0.00014714286953676492 , Elapsed time: 264.1603786945343\n",
            "-------------------------------------------------\n",
            "\n",
            "\n",
            "--------------- Starting trial: 18 ---------------\n",
            "Deep layers: 4 , Number of neurons: 140 , Learning rate: 0.0008969006041278596 , Batch size: 8\n",
            "Restoring model weights from the end of the best epoch: 19.\n",
            "Epoch 29: early stopping\n",
            "241/241 [==============================] - 1s 3ms/step - loss: 2.1124e-04 - mean_squared_error: 2.1124e-04\n",
            "Loss: 0.0002112375368596986 , Elapsed time: 161.70317673683167\n",
            "-------------------------------------------------\n",
            "\n",
            "\n",
            "--------------- Starting trial: 19 ---------------\n",
            "Deep layers: 7 , Number of neurons: 35 , Learning rate: 0.001 , Batch size: 4\n",
            "Restoring model weights from the end of the best epoch: 29.\n",
            "Epoch 39: early stopping\n",
            "241/241 [==============================] - 1s 2ms/step - loss: 1.0587e-04 - mean_squared_error: 1.0587e-04\n",
            "Loss: 0.0001058673151419498 , Elapsed time: 383.58858156204224\n",
            "-------------------------------------------------\n",
            "\n",
            "2  \t6     \t9.10382e-06\t0.000132116\t0.000211238\n",
            "\n",
            "--------------- Starting trial: 20 ---------------\n",
            "Deep layers: 4 , Number of neurons: 31 , Learning rate: 0.0008969006041278596 , Batch size: 6\n",
            "Restoring model weights from the end of the best epoch: 23.\n",
            "Epoch 33: early stopping\n",
            "241/241 [==============================] - 0s 2ms/step - loss: 1.7510e-04 - mean_squared_error: 1.7510e-04\n",
            "Loss: 0.0001751036907080561 , Elapsed time: 263.0988247394562\n",
            "-------------------------------------------------\n",
            "\n",
            "\n",
            "--------------- Starting trial: 21 ---------------\n",
            "Deep layers: 4 , Number of neurons: 31 , Learning rate: 0.0001 , Batch size: 13\n",
            "241/241 [==============================] - 0s 1ms/step - loss: 1.0225e-04 - mean_squared_error: 1.0225e-04\n",
            "Loss: 0.00010225322330370545 , Elapsed time: 864.5614037513733\n",
            "-------------------------------------------------\n",
            "\n",
            "\n",
            "--------------- Starting trial: 22 ---------------\n",
            "Deep layers: 6 , Number of neurons: 140 , Learning rate: 0.0009946155348226208 , Batch size: 8\n",
            "Restoring model weights from the end of the best epoch: 36.\n",
            "Epoch 46: early stopping\n",
            "241/241 [==============================] - 1s 2ms/step - loss: 3.0715e-05 - mean_squared_error: 3.0715e-05\n",
            "Loss: 3.071476749028079e-05 , Elapsed time: 302.0126361846924\n",
            "-------------------------------------------------\n",
            "\n",
            "\n",
            "--------------- Starting trial: 23 ---------------\n",
            "Deep layers: 4 , Number of neurons: 68 , Learning rate: 0.0008969006041278596 , Batch size: 8\n",
            "Restoring model weights from the end of the best epoch: 54.\n",
            "Epoch 64: early stopping\n",
            "241/241 [==============================] - 1s 2ms/step - loss: 6.4119e-05 - mean_squared_error: 6.4119e-05\n",
            "Loss: 6.411890353774652e-05 , Elapsed time: 294.76378178596497\n",
            "-------------------------------------------------\n",
            "\n",
            "\n",
            "--------------- Starting trial: 24 ---------------\n",
            "Deep layers: 3 , Number of neurons: 126 , Learning rate: 0.001 , Batch size: 13\n",
            "Restoring model weights from the end of the best epoch: 31.\n",
            "Epoch 41: early stopping\n",
            "241/241 [==============================] - 0s 2ms/step - loss: 1.6446e-04 - mean_squared_error: 1.6446e-04\n",
            "Loss: 0.00016445759683847427 , Elapsed time: 135.42465996742249\n",
            "-------------------------------------------------\n",
            "\n",
            "\n",
            "--------------- Starting trial: 25 ---------------\n",
            "Deep layers: 12 , Number of neurons: 20 , Learning rate: 0.001 , Batch size: 32\n",
            "Restoring model weights from the end of the best epoch: 34.\n",
            "Epoch 44: early stopping\n",
            "241/241 [==============================] - 0s 2ms/step - loss: 1.2222e-04 - mean_squared_error: 1.2222e-04\n",
            "Loss: 0.00012221725773997605 , Elapsed time: 59.27788853645325\n",
            "-------------------------------------------------\n",
            "\n",
            "3  \t6     \t9.10382e-06\t9.54242e-05\t0.000175104\n",
            "\n",
            "--------------- Starting trial: 26 ---------------\n",
            "Deep layers: 9 , Number of neurons: 31 , Learning rate: 0.000981631549963661 , Batch size: 30\n",
            "Restoring model weights from the end of the best epoch: 49.\n",
            "Epoch 59: early stopping\n",
            "241/241 [==============================] - 0s 2ms/step - loss: 1.3574e-04 - mean_squared_error: 1.3574e-04\n",
            "Loss: 0.00013574205513577908 , Elapsed time: 143.85261869430542\n",
            "-------------------------------------------------\n",
            "\n",
            "\n",
            "--------------- Starting trial: 27 ---------------\n",
            "Deep layers: 6 , Number of neurons: 119 , Learning rate: 0.001 , Batch size: 8\n",
            "Restoring model weights from the end of the best epoch: 53.\n",
            "Epoch 63: early stopping\n",
            "241/241 [==============================] - 1s 3ms/step - loss: 2.4370e-05 - mean_squared_error: 2.4370e-05\n",
            "Loss: 2.4369548555114307e-05 , Elapsed time: 374.5725796222687\n",
            "-------------------------------------------------\n",
            "\n",
            "\n",
            "--------------- Starting trial: 28 ---------------\n",
            "Deep layers: 8 , Number of neurons: 31 , Learning rate: 0.0008969006041278596 , Batch size: 8\n",
            "Restoring model weights from the end of the best epoch: 108.\n",
            "Epoch 118: early stopping\n",
            "241/241 [==============================] - 1s 2ms/step - loss: 9.0288e-06 - mean_squared_error: 9.0288e-06\n",
            "Loss: 9.02883675735211e-06 , Elapsed time: 579.0160162448883\n",
            "-------------------------------------------------\n",
            "\n",
            "\n",
            "--------------- Starting trial: 29 ---------------\n",
            "Deep layers: 4 , Number of neurons: 31 , Learning rate: 0.0008969006041278596 , Batch size: 8\n",
            "Restoring model weights from the end of the best epoch: 140.\n",
            "Epoch 150: early stopping\n",
            "241/241 [==============================] - 1s 2ms/step - loss: 1.4355e-05 - mean_squared_error: 1.4355e-05\n",
            "Loss: 1.435500780644361e-05 , Elapsed time: 639.3626008033752\n",
            "-------------------------------------------------\n",
            "\n",
            "\n",
            "--------------- Starting trial: 30 ---------------\n",
            "Deep layers: 4 , Number of neurons: 35 , Learning rate: 0.0001 , Batch size: 17\n",
            "241/241 [==============================] - 0s 2ms/step - loss: 1.7131e-04 - mean_squared_error: 1.7131e-04\n",
            "Loss: 0.00017130729975178838 , Elapsed time: 629.6378059387207\n",
            "-------------------------------------------------\n",
            "\n",
            "\n",
            "--------------- Starting trial: 31 ---------------\n",
            "Deep layers: 5 , Number of neurons: 31 , Learning rate: 0.0008969006041278596 , Batch size: 8\n",
            "Restoring model weights from the end of the best epoch: 35.\n",
            "Epoch 45: early stopping\n",
            "241/241 [==============================] - 1s 2ms/step - loss: 2.5242e-04 - mean_squared_error: 2.5242e-04\n",
            "Loss: 0.00025242374977096915 , Elapsed time: 263.42869091033936\n",
            "-------------------------------------------------\n",
            "\n",
            "4  \t6     \t9.02884e-06\t8.80472e-05\t0.000252424\n",
            "\n",
            "--------------- Starting trial: 32 ---------------\n",
            "Deep layers: 8 , Number of neurons: 31 , Learning rate: 0.0008969006041278596 , Batch size: 27\n",
            "Restoring model weights from the end of the best epoch: 46.\n",
            "Epoch 56: early stopping\n",
            "241/241 [==============================] - 0s 2ms/step - loss: 2.1324e-04 - mean_squared_error: 2.1324e-04\n",
            "Loss: 0.0002132394874934107 , Elapsed time: 143.49527525901794\n",
            "-------------------------------------------------\n",
            "\n",
            "\n",
            "--------------- Starting trial: 33 ---------------\n",
            "Deep layers: 8 , Number of neurons: 64 , Learning rate: 0.000981631549963661 , Batch size: 14\n",
            "Restoring model weights from the end of the best epoch: 98.\n",
            "Epoch 108: early stopping\n",
            "241/241 [==============================] - 0s 2ms/step - loss: 5.3195e-06 - mean_squared_error: 5.3195e-06\n",
            "Loss: 5.319530828273855e-06 , Elapsed time: 383.48502802848816\n",
            "-------------------------------------------------\n",
            "\n",
            "\n",
            "--------------- Starting trial: 34 ---------------\n",
            "Deep layers: 4 , Number of neurons: 31 , Learning rate: 0.0008969006041278596 , Batch size: 8\n",
            "Restoring model weights from the end of the best epoch: 210.\n",
            "Epoch 220: early stopping\n",
            "241/241 [==============================] - 1s 2ms/step - loss: 5.6396e-06 - mean_squared_error: 5.6396e-06\n",
            "Loss: 5.6396106629108544e-06 , Elapsed time: 979.9629220962524\n",
            "-------------------------------------------------\n",
            "\n",
            "\n",
            "--------------- Starting trial: 35 ---------------\n",
            "Deep layers: 4 , Number of neurons: 31 , Learning rate: 0.0008969006041278596 , Batch size: 8\n",
            "Restoring model weights from the end of the best epoch: 86.\n",
            "Epoch 96: early stopping\n",
            "241/241 [==============================] - 1s 2ms/step - loss: 9.1212e-05 - mean_squared_error: 9.1212e-05\n",
            "Loss: 9.12124160095118e-05 , Elapsed time: 443.39721155166626\n",
            "-------------------------------------------------\n",
            "\n",
            "\n",
            "--------------- Starting trial: 36 ---------------\n",
            "Deep layers: 9 , Number of neurons: 95 , Learning rate: 0.0009823623304640184 , Batch size: 32\n",
            "Restoring model weights from the end of the best epoch: 15.\n",
            "Epoch 25: early stopping\n",
            "241/241 [==============================] - 0s 2ms/step - loss: 0.0027 - mean_squared_error: 0.0027\n",
            "Loss: 0.002714733127504587 , Elapsed time: 54.960304260253906\n",
            "-------------------------------------------------\n",
            "\n",
            "\n",
            "--------------- Starting trial: 37 ---------------\n",
            "Deep layers: 10 , Number of neurons: 71 , Learning rate: 0.0009983270859605378 , Batch size: 8\n",
            "Restoring model weights from the end of the best epoch: 74.\n",
            "Epoch 84: early stopping\n",
            "241/241 [==============================] - 1s 3ms/step - loss: 3.6624e-05 - mean_squared_error: 3.6624e-05\n",
            "Loss: 3.6623670894186944e-05 , Elapsed time: 558.03160572052\n",
            "-------------------------------------------------\n",
            "\n",
            "5  \t6     \t5.31953e-06\t0.0004394  \t0.00271473 \n",
            "-- Best Individual =  [8.082126856079187, 64, -3.0080514916880134, 14]\n",
            "-- Best Fitness =  5.319530828273855e-06\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total elapsed time: 277.1887450257937 minutes\n"
          ]
        }
      ],
      "source": [
        "population_size = 7   # max of individuals per generation\n",
        "max_generations = 5   # number of generations\n",
        "gene_length = 4       # lenght of the gene, depends on how many hyperparameters are tested \n",
        "\n",
        "k = 1;                # num. of finalist individuals\n",
        "epochs = 300 \n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    t = time.time(); \n",
        "    datos = [];\n",
        "    ss = [i for i in range(1,population_size*(max_generations+1))]\n",
        "    best_population = geneticAlgorithm_with_elitism(population_size, max_generations, gene_length, k)\n",
        "    print(\"Total elapsed time:\", (time.time()-t)/60, \"minutes\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "E9J0x5YzIulR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f0c32adb-e361-43f4-c50f-8210b938882e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "k= 1 \n",
            "Deep layers: 8.082126856079187 , Number of neurons: 64 , Learning rate: -3.0080514916880134 , Batch size: 14\n"
          ]
        }
      ],
      "source": [
        "best_deep_layers   = []\n",
        "best_num_units     = []\n",
        "best_learning_rate = []\n",
        "best_batch_size    = []\n",
        "# best_activation_f  = []\n",
        "# best_f_names       = []\n",
        "\n",
        "t = 0\n",
        "\n",
        "for bi in best_population:\n",
        "    deep_layers   = bi[0]  \n",
        "    num_units     = bi[1]\n",
        "    learning_rate = bi[2]   \n",
        "    batch_size    = bi[3]    \n",
        "#     activation_f  = bi[4]  \n",
        "    t += 1 \n",
        "    \n",
        "    best_deep_layers.append(deep_layers)\n",
        "    best_num_units.append(num_units)\n",
        "    best_learning_rate.append(learning_rate)\n",
        "    best_batch_size.append(batch_size)\n",
        "#     best_activation_f.append(activation_f)\n",
        "#     best_f_names.append(activation_f)\n",
        "    \n",
        "    print('k=',t,'\\nDeep layers:', best_deep_layers[-1], ', Number of neurons:', best_num_units[-1], ', Learning rate:', best_learning_rate[-1],', Batch size:', best_batch_size[-1])\n",
        "    # print('k=',t,'\\nDeep layers:', best_deep_layers[-1], ', Number of neurons:', best_num_units[-1], ', Learning rate:', best_learning_rate[-1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "vgMBb6ZDIulS",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "550b2a32-38ca-4a7a-a580-0fcf37832675"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "    Deep layers  Num units  Learning rate  Batch size      Loss     Score  \\\n",
              "0             8         64       0.000982          14  0.000005  0.000005   \n",
              "1             4         31       0.000897           8  0.000006  0.000006   \n",
              "2             8         31       0.000897           8  0.000009  0.000009   \n",
              "3             4         31       0.000897           8  0.000009  0.000009   \n",
              "4             4         31       0.000897           8  0.000014  0.000014   \n",
              "5             6        119       0.001000           8  0.000024  0.000024   \n",
              "6             6        140       0.000995           8  0.000031  0.000031   \n",
              "7             6        150       0.001000           4  0.000036  0.000036   \n",
              "8             8         50       0.000100          32  0.000036  0.000036   \n",
              "9            10         71       0.000998           8  0.000037  0.000037   \n",
              "10            6        150       0.001000           4  0.000048  0.000048   \n",
              "11            6        103       0.001000           4  0.000061  0.000061   \n",
              "12           12         10       0.000100           8  0.000064  0.000064   \n",
              "13            4         68       0.000897           8  0.000064  0.000064   \n",
              "14            6        150       0.000100           8  0.000078  0.000078   \n",
              "15            4         31       0.000897           8  0.000091  0.000091   \n",
              "16            4         31       0.000100          13  0.000102  0.000102   \n",
              "17            7         35       0.001000           4  0.000106  0.000106   \n",
              "18            4         45       0.001000           4  0.000107  0.000107   \n",
              "19            4        100       0.001000           4  0.000111  0.000111   \n",
              "20            4        100       0.001000           4  0.000113  0.000113   \n",
              "21           12         20       0.001000          32  0.000122  0.000122   \n",
              "22            9         31       0.000982          30  0.000136  0.000136   \n",
              "23            6         55       0.001000           4  0.000146  0.000146   \n",
              "24            6         31       0.000995           8  0.000147  0.000147   \n",
              "25            3        126       0.001000          13  0.000164  0.000164   \n",
              "26            4         35       0.000100          17  0.000171  0.000171   \n",
              "27            4         31       0.000897           6  0.000175  0.000175   \n",
              "28            3        126       0.001000          32  0.000198  0.000198   \n",
              "29            4        140       0.000897           8  0.000211  0.000211   \n",
              "30            8         31       0.000897          27  0.000213  0.000213   \n",
              "31            5         31       0.000897           8  0.000252  0.000252   \n",
              "32           10        150       0.000100          32  0.002349  0.002349   \n",
              "33            6        109       0.001000           4  0.002528  0.002528   \n",
              "34            9         95       0.000982          32  0.002715  0.002715   \n",
              "35           11         81       0.000111           4  0.002899  0.002899   \n",
              "36            2         10       0.000100          16  0.003294  0.003294   \n",
              "\n",
              "    Elapsed time  \n",
              "0     383.485028  \n",
              "1     979.962922  \n",
              "2     579.016016  \n",
              "3     939.757131  \n",
              "4     639.362601  \n",
              "5     374.572580  \n",
              "6     302.012636  \n",
              "7     683.506782  \n",
              "8     443.453815  \n",
              "9     558.031606  \n",
              "10    571.056576  \n",
              "11    520.572465  \n",
              "12   1564.162128  \n",
              "13    294.763782  \n",
              "14    624.338708  \n",
              "15    443.397212  \n",
              "16    864.561404  \n",
              "17    383.588582  \n",
              "18    279.486757  \n",
              "19    743.245302  \n",
              "20    300.378912  \n",
              "21     59.277889  \n",
              "22    143.852619  \n",
              "23    361.188379  \n",
              "24    264.160379  \n",
              "25    135.424660  \n",
              "26    629.637806  \n",
              "27    263.098825  \n",
              "28     68.555558  \n",
              "29    161.703177  \n",
              "30    143.495275  \n",
              "31    263.428691  \n",
              "32    226.739506  \n",
              "33    374.203721  \n",
              "34     54.960304  \n",
              "35    385.515850  \n",
              "36    622.881140  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-26c43cff-d479-4dd4-aa20-1f2c4d7aa467\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Deep layers</th>\n",
              "      <th>Num units</th>\n",
              "      <th>Learning rate</th>\n",
              "      <th>Batch size</th>\n",
              "      <th>Loss</th>\n",
              "      <th>Score</th>\n",
              "      <th>Elapsed time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>8</td>\n",
              "      <td>64</td>\n",
              "      <td>0.000982</td>\n",
              "      <td>14</td>\n",
              "      <td>0.000005</td>\n",
              "      <td>0.000005</td>\n",
              "      <td>383.485028</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>4</td>\n",
              "      <td>31</td>\n",
              "      <td>0.000897</td>\n",
              "      <td>8</td>\n",
              "      <td>0.000006</td>\n",
              "      <td>0.000006</td>\n",
              "      <td>979.962922</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>8</td>\n",
              "      <td>31</td>\n",
              "      <td>0.000897</td>\n",
              "      <td>8</td>\n",
              "      <td>0.000009</td>\n",
              "      <td>0.000009</td>\n",
              "      <td>579.016016</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>31</td>\n",
              "      <td>0.000897</td>\n",
              "      <td>8</td>\n",
              "      <td>0.000009</td>\n",
              "      <td>0.000009</td>\n",
              "      <td>939.757131</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>31</td>\n",
              "      <td>0.000897</td>\n",
              "      <td>8</td>\n",
              "      <td>0.000014</td>\n",
              "      <td>0.000014</td>\n",
              "      <td>639.362601</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>6</td>\n",
              "      <td>119</td>\n",
              "      <td>0.001000</td>\n",
              "      <td>8</td>\n",
              "      <td>0.000024</td>\n",
              "      <td>0.000024</td>\n",
              "      <td>374.572580</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>6</td>\n",
              "      <td>140</td>\n",
              "      <td>0.000995</td>\n",
              "      <td>8</td>\n",
              "      <td>0.000031</td>\n",
              "      <td>0.000031</td>\n",
              "      <td>302.012636</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>6</td>\n",
              "      <td>150</td>\n",
              "      <td>0.001000</td>\n",
              "      <td>4</td>\n",
              "      <td>0.000036</td>\n",
              "      <td>0.000036</td>\n",
              "      <td>683.506782</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>8</td>\n",
              "      <td>50</td>\n",
              "      <td>0.000100</td>\n",
              "      <td>32</td>\n",
              "      <td>0.000036</td>\n",
              "      <td>0.000036</td>\n",
              "      <td>443.453815</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>10</td>\n",
              "      <td>71</td>\n",
              "      <td>0.000998</td>\n",
              "      <td>8</td>\n",
              "      <td>0.000037</td>\n",
              "      <td>0.000037</td>\n",
              "      <td>558.031606</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>6</td>\n",
              "      <td>150</td>\n",
              "      <td>0.001000</td>\n",
              "      <td>4</td>\n",
              "      <td>0.000048</td>\n",
              "      <td>0.000048</td>\n",
              "      <td>571.056576</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>6</td>\n",
              "      <td>103</td>\n",
              "      <td>0.001000</td>\n",
              "      <td>4</td>\n",
              "      <td>0.000061</td>\n",
              "      <td>0.000061</td>\n",
              "      <td>520.572465</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>12</td>\n",
              "      <td>10</td>\n",
              "      <td>0.000100</td>\n",
              "      <td>8</td>\n",
              "      <td>0.000064</td>\n",
              "      <td>0.000064</td>\n",
              "      <td>1564.162128</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>4</td>\n",
              "      <td>68</td>\n",
              "      <td>0.000897</td>\n",
              "      <td>8</td>\n",
              "      <td>0.000064</td>\n",
              "      <td>0.000064</td>\n",
              "      <td>294.763782</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>6</td>\n",
              "      <td>150</td>\n",
              "      <td>0.000100</td>\n",
              "      <td>8</td>\n",
              "      <td>0.000078</td>\n",
              "      <td>0.000078</td>\n",
              "      <td>624.338708</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>4</td>\n",
              "      <td>31</td>\n",
              "      <td>0.000897</td>\n",
              "      <td>8</td>\n",
              "      <td>0.000091</td>\n",
              "      <td>0.000091</td>\n",
              "      <td>443.397212</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>4</td>\n",
              "      <td>31</td>\n",
              "      <td>0.000100</td>\n",
              "      <td>13</td>\n",
              "      <td>0.000102</td>\n",
              "      <td>0.000102</td>\n",
              "      <td>864.561404</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>7</td>\n",
              "      <td>35</td>\n",
              "      <td>0.001000</td>\n",
              "      <td>4</td>\n",
              "      <td>0.000106</td>\n",
              "      <td>0.000106</td>\n",
              "      <td>383.588582</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>4</td>\n",
              "      <td>45</td>\n",
              "      <td>0.001000</td>\n",
              "      <td>4</td>\n",
              "      <td>0.000107</td>\n",
              "      <td>0.000107</td>\n",
              "      <td>279.486757</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>4</td>\n",
              "      <td>100</td>\n",
              "      <td>0.001000</td>\n",
              "      <td>4</td>\n",
              "      <td>0.000111</td>\n",
              "      <td>0.000111</td>\n",
              "      <td>743.245302</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>4</td>\n",
              "      <td>100</td>\n",
              "      <td>0.001000</td>\n",
              "      <td>4</td>\n",
              "      <td>0.000113</td>\n",
              "      <td>0.000113</td>\n",
              "      <td>300.378912</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>12</td>\n",
              "      <td>20</td>\n",
              "      <td>0.001000</td>\n",
              "      <td>32</td>\n",
              "      <td>0.000122</td>\n",
              "      <td>0.000122</td>\n",
              "      <td>59.277889</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>9</td>\n",
              "      <td>31</td>\n",
              "      <td>0.000982</td>\n",
              "      <td>30</td>\n",
              "      <td>0.000136</td>\n",
              "      <td>0.000136</td>\n",
              "      <td>143.852619</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>6</td>\n",
              "      <td>55</td>\n",
              "      <td>0.001000</td>\n",
              "      <td>4</td>\n",
              "      <td>0.000146</td>\n",
              "      <td>0.000146</td>\n",
              "      <td>361.188379</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>6</td>\n",
              "      <td>31</td>\n",
              "      <td>0.000995</td>\n",
              "      <td>8</td>\n",
              "      <td>0.000147</td>\n",
              "      <td>0.000147</td>\n",
              "      <td>264.160379</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>3</td>\n",
              "      <td>126</td>\n",
              "      <td>0.001000</td>\n",
              "      <td>13</td>\n",
              "      <td>0.000164</td>\n",
              "      <td>0.000164</td>\n",
              "      <td>135.424660</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>4</td>\n",
              "      <td>35</td>\n",
              "      <td>0.000100</td>\n",
              "      <td>17</td>\n",
              "      <td>0.000171</td>\n",
              "      <td>0.000171</td>\n",
              "      <td>629.637806</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>4</td>\n",
              "      <td>31</td>\n",
              "      <td>0.000897</td>\n",
              "      <td>6</td>\n",
              "      <td>0.000175</td>\n",
              "      <td>0.000175</td>\n",
              "      <td>263.098825</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>3</td>\n",
              "      <td>126</td>\n",
              "      <td>0.001000</td>\n",
              "      <td>32</td>\n",
              "      <td>0.000198</td>\n",
              "      <td>0.000198</td>\n",
              "      <td>68.555558</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>4</td>\n",
              "      <td>140</td>\n",
              "      <td>0.000897</td>\n",
              "      <td>8</td>\n",
              "      <td>0.000211</td>\n",
              "      <td>0.000211</td>\n",
              "      <td>161.703177</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>8</td>\n",
              "      <td>31</td>\n",
              "      <td>0.000897</td>\n",
              "      <td>27</td>\n",
              "      <td>0.000213</td>\n",
              "      <td>0.000213</td>\n",
              "      <td>143.495275</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31</th>\n",
              "      <td>5</td>\n",
              "      <td>31</td>\n",
              "      <td>0.000897</td>\n",
              "      <td>8</td>\n",
              "      <td>0.000252</td>\n",
              "      <td>0.000252</td>\n",
              "      <td>263.428691</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32</th>\n",
              "      <td>10</td>\n",
              "      <td>150</td>\n",
              "      <td>0.000100</td>\n",
              "      <td>32</td>\n",
              "      <td>0.002349</td>\n",
              "      <td>0.002349</td>\n",
              "      <td>226.739506</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33</th>\n",
              "      <td>6</td>\n",
              "      <td>109</td>\n",
              "      <td>0.001000</td>\n",
              "      <td>4</td>\n",
              "      <td>0.002528</td>\n",
              "      <td>0.002528</td>\n",
              "      <td>374.203721</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>34</th>\n",
              "      <td>9</td>\n",
              "      <td>95</td>\n",
              "      <td>0.000982</td>\n",
              "      <td>32</td>\n",
              "      <td>0.002715</td>\n",
              "      <td>0.002715</td>\n",
              "      <td>54.960304</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35</th>\n",
              "      <td>11</td>\n",
              "      <td>81</td>\n",
              "      <td>0.000111</td>\n",
              "      <td>4</td>\n",
              "      <td>0.002899</td>\n",
              "      <td>0.002899</td>\n",
              "      <td>385.515850</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36</th>\n",
              "      <td>2</td>\n",
              "      <td>10</td>\n",
              "      <td>0.000100</td>\n",
              "      <td>16</td>\n",
              "      <td>0.003294</td>\n",
              "      <td>0.003294</td>\n",
              "      <td>622.881140</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-26c43cff-d479-4dd4-aa20-1f2c4d7aa467')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-26c43cff-d479-4dd4-aa20-1f2c4d7aa467 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-26c43cff-d479-4dd4-aa20-1f2c4d7aa467');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ],
      "source": [
        "filename = \"historial_genetic_ecsdiff.txt\"\n",
        "df = pd.DataFrame(datos, columns = [\"Deep layers\", \"Num units\", \"Learning rate\", \"Batch size\", \"Loss\", \"Score\", \"Elapsed time\"])\n",
        "# df = pd.DataFrame(datos, columns = [\"Deep layers\", \"Num units\", \"Learning rate\", \"Loss\", \"Score\", \"Elapsed time\"])\n",
        "\n",
        "df.sort_values(by=[\"Loss\", \"Elapsed time\"], ascending=[True, True], ignore_index=True, inplace=True)\n",
        "\n",
        "df.to_csv(filename, header=True, index=False, sep='\\t', mode='w') # a=append, w=overwrite\n",
        "df.head(50)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "NCaTIHYJIulT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "563c5ead-bc5c-4634-a30d-0c5bc8d30a10"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Elapsed time 277.181 minutes\n"
          ]
        }
      ],
      "source": [
        "total_time = float(np.sum(df[[\"Elapsed time\"]])/60)\n",
        "\n",
        "print(\"Elapsed time {:.3f} minutes\".format(total_time))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "qh947Ib9IulW"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "colab": {
      "name": "genetic_hyp_ecs-diff.ipynb",
      "provenance": []
    },
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}