{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\"rm\" no se reconoce como un comando interno o externo,\n",
      "programa o archivo por lotes ejecutable.\n"
     ]
    }
   ],
   "source": [
    "%reload_ext tensorboard\n",
    "# Clear any logs from previous runs\n",
    "!rm -rf ./logs/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorboard.plugins.hparams import api as hp\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.utils import to_categorical\n",
    "from keras.losses import CategoricalCrossentropy\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split as split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conjunto de datos: SDSS DR17"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        alpha      delta         u         g         r         i         z  \\\n",
      "0  135.689107  32.494632  23.87882  22.27530  20.39501  19.16573  18.79371   \n",
      "1  144.826101  31.274185  24.77759  22.83188  22.58444  21.16812  21.61427   \n",
      "2  142.188790  35.582444  25.26307  22.66389  20.60976  19.34857  18.94827   \n",
      "3  338.741038  -0.402828  22.13682  23.77656  21.61162  20.50454  19.25010   \n",
      "4  345.282593  21.183866  19.43718  17.58028  16.49747  15.97711  15.54461   \n",
      "\n",
      "   redshift  class  \n",
      "0  0.634794      0  \n",
      "1  0.779136      0  \n",
      "2  0.644195      0  \n",
      "3  0.932346      0  \n",
      "4  0.116123      0  \n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv('./SDSS/star_classification.csv')\n",
    "cols = ['alpha','delta','u','g','r','i','z','redshift','class']\n",
    "data = data[cols]\n",
    "data[\"class\"]=[0 if i == \"GALAXY\" else 1 if i == \"STAR\" else 2 for i in data[\"class\"]]\n",
    "print(data.head())\n",
    "data = data.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataset(data):\n",
    "    X, Y = np.empty((0)), np.empty((0))\n",
    "    X = data[:, :8]\n",
    "    Y = data[:, 8]\n",
    "    Y = to_categorical(Y, num_classes=3)\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y = prepare_dataset(data)\n",
    "X_train, X_test, Y_train, Y_test = split(X, Y, test_size = 0.3, random_state = 0)\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test  = scaler.transform(X_test)\n",
    "\n",
    "lenx, input_shape = np.shape(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hiperpar√°metros del modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "HP_LAYERS = hp.HParam('layers', hp.Discrete([8, 16]))\n",
    "HP_NUM_UNITS = hp.HParam('num_units', hp.Discrete([4,8,16,32]))\n",
    "HP_LEARNING  = hp.HParam('learning_rate', hp.Discrete([4,8,16,32]))\n",
    "# HP_BATCHSIZE = hp.HParam('batch_size', hp.Discrete([16, 32]))\n",
    "\n",
    "callbacks = [tf.keras.callbacks.EarlyStopping(monitor='val_categorical_accuracy', mode='max',\n",
    "                                   min_delta=0,\n",
    "                                   patience=5,\n",
    "                                   restore_best_weights=True)]\n",
    "batch_size = 32\n",
    "epochs = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# METRIC_ACCURACY = 'accuracy'\n",
    "with tf.summary.create_file_writer('logs/hparam_tuning').as_default():\n",
    "# with tf.summary.FileWriter('logs/hparam_tuning', sess.graph):\n",
    "#     init = tf.initialize_all_variables()\n",
    "#     sess.run(init)\n",
    "    hp.hparams_config(\n",
    "        hparams=[HP_LAYERS, HP_NUM_UNITS, HP_LEARNING],\n",
    "        metrics=[hp.Metric('loss', display_name=\"Loss\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_model(hparams):\n",
    "    # Train LSTM model and predict on validation set\n",
    "    model = keras.Sequential()\n",
    "    model.add(Input(shape=(int(X_train.shape[1]),)))\n",
    "    model.add(Dense(hparams[HP_NUM_UNITS], input_shape=(int(X_train.shape[1]),)))\n",
    "    \n",
    "    for i in range(hparams[HP_LAYERS]):        \n",
    "        model.add(Dense(hparams[HP_NUM_UNITS], activation='relu'))\n",
    "    model.add(Dense(3, activation=tf.nn.softmax))\n",
    "     \n",
    "    optimizer = keras.optimizers.Adam(learning_rate=hparams[HP_LEARNING]*10**(-4), beta_1=0.9, beta_2=0.999, epsilon=1e-3)\n",
    "    model.compile(\n",
    "            optimizer=optimizer,\n",
    "            loss=CategoricalCrossentropy(),\n",
    "            metrics=[\"categorical_accuracy\"]\n",
    "          )\n",
    "    # Run with 1 epoch to speed things up for demo purposes\n",
    "    # model.fit(x_train, y_train, epochs=1, callbacks=callbacks) \n",
    "    \n",
    "\n",
    "    model.fit(X_train, Y_train, epochs=epochs, validation_data=(X_test, Y_test),\n",
    "              callbacks=callbacks, batch_size=batch_size, shuffle=True)\n",
    "#               , verbose=0)\n",
    "\n",
    "    _, loss = model.evaluate(X_test, Y_test)\n",
    "\n",
    "\n",
    "    return loss\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(run_dir, hparams):\n",
    "    with tf.summary.create_file_writer(run_dir).as_default():\n",
    "        hp.hparams(hparams)  # record the values used in this trial\n",
    "        loss = train_test_model(hparams)\n",
    "        tf.summary.scalar(\"loss\", loss, step=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Starting trial: run-0\n",
      "{'layers': 8, 'num_units': 4, 'learning_rate': 4}\n",
      "Train on 70000 samples, validate on 30000 samples\n",
      "70000/70000 [==============================] - 24s 347us/sample - loss: 0.9009 - categorical_accuracy: 0.6115 - val_loss: 0.7982 - val_categorical_accuracy: 0.7347\n",
      "30000/30000 [==============================] - 3s 115us/sample - loss: 0.7982 - categorical_accuracy: 0.7347 - loss: 0.8062 - categorical_ac\n",
      "\n",
      "--- Starting trial: run-1\n",
      "{'layers': 8, 'num_units': 4, 'learning_rate': 8}\n",
      "Train on 70000 samples, validate on 30000 samples\n",
      "70000/70000 [==============================] - 22s 315us/sample - loss: 0.7186 - categorical_accuracy: 0.7150 - val_loss: 0.5234 - val_categorical_accuracy: 0.7996\n",
      "30000/30000 [==============================] - 4s 124us/sample - loss: 0.5234 - categorical_accuracy: 0.7996\n",
      "\n",
      "--- Starting trial: run-2\n",
      "{'layers': 8, 'num_units': 4, 'learning_rate': 16}\n",
      "Train on 70000 samples, validate on 30000 samples\n",
      "70000/70000 [==============================] - 21s 306us/sample - loss: 0.6131 - categorical_accuracy: 0.7359 - val_loss: 0.5583 - val_categorical_accuracy: 0.7535\n",
      "30000/30000 [==============================] - 5s 154us/sample - loss: 0.5583 - categorical_accuracy: 0.7535\n",
      "\n",
      "--- Starting trial: run-3\n",
      "{'layers': 8, 'num_units': 4, 'learning_rate': 32}\n",
      "Train on 70000 samples, validate on 30000 samples\n",
      "70000/70000 [==============================] - 23s 326us/sample - loss: 0.2719 - categorical_accuracy: 0.9043 - val_loss: 0.4006 - val_categorical_accuracy: 0.9606\n",
      "30000/30000 [==============================] - 5s 153us/sample - loss: 0.4006 - categorical_accuracy: 0.9606\n",
      "\n",
      "--- Starting trial: run-4\n",
      "{'layers': 8, 'num_units': 8, 'learning_rate': 4}\n",
      "Train on 70000 samples, validate on 30000 samples\n",
      "70000/70000 [==============================] - 25s 363us/sample - loss: 0.5314 - categorical_accuracy: 0.7851 - val_loss: 0.2497 - val_categorical_accuracy: 0.9182\n",
      "30000/30000 [==============================] - 4s 139us/sample - loss: 0.2497 - categorical_accuracy: 0.9182\n",
      "\n",
      "--- Starting trial: run-5\n",
      "{'layers': 8, 'num_units': 8, 'learning_rate': 8}\n",
      "Train on 70000 samples, validate on 30000 samples\n",
      "70000/70000 [==============================] - 21s 305us/sample - loss: 0.4242 - categorical_accuracy: 0.8456 - val_loss: 1.0626 - val_categorical_accuracy: 0.9452\n",
      "30000/30000 [==============================] - 4s 138us/sample - loss: 1.0626 - categorical_accuracy: 0.9452\n",
      "\n",
      "--- Starting trial: run-6\n",
      "{'layers': 8, 'num_units': 8, 'learning_rate': 16}\n",
      "Train on 70000 samples, validate on 30000 samples\n",
      "70000/70000 [==============================] - 26s 373us/sample - loss: 0.3587 - categorical_accuracy: 0.8609 - val_loss: 0.5601 - val_categorical_accuracy: 0.9561\n",
      "30000/30000 [==============================] - 5s 173us/sample - loss: 0.5601 - categorical_accuracy: 0.9561\n",
      "\n",
      "--- Starting trial: run-7\n",
      "{'layers': 8, 'num_units': 8, 'learning_rate': 32}\n",
      "Train on 70000 samples, validate on 30000 samples\n",
      "70000/70000 [==============================] - 23s 323us/sample - loss: 0.2848 - categorical_accuracy: 0.8985 - val_loss: 0.1925 - val_categorical_accuracy: 0.9604\n",
      "30000/30000 [==============================] - 5s 174us/sample - loss: 0.1925 - categorical_accuracy: 0.9604\n",
      "\n",
      "--- Starting trial: run-8\n",
      "{'layers': 8, 'num_units': 16, 'learning_rate': 4}\n",
      "Train on 70000 samples, validate on 30000 samples\n",
      "70000/70000 [==============================] - 19s 274us/sample - loss: 0.3278 - categorical_accuracy: 0.8699 - val_loss: 0.1423 - val_categorical_accuracy: 0.9595\n",
      "30000/30000 [==============================] - 4s 135us/sample - loss: 0.1423 - categorical_accuracy: 0.9595 - loss: 0.1442 - categorica\n",
      "\n",
      "--- Starting trial: run-9\n",
      "{'layers': 8, 'num_units': 16, 'learning_rate': 8}\n",
      "Train on 70000 samples, validate on 30000 samples\n",
      "70000/70000 [==============================] - 23s 329us/sample - loss: 0.3175 - categorical_accuracy: 0.8827 - val_loss: 0.1434 - val_categorical_accuracy: 0.9554\n",
      "30000/30000 [==============================] - 4s 138us/sample - loss: 0.1434 - categorical_accuracy: 0.9554\n",
      "\n",
      "--- Starting trial: run-10\n",
      "{'layers': 8, 'num_units': 16, 'learning_rate': 16}\n",
      "Train on 70000 samples, validate on 30000 samples\n",
      "70000/70000 [==============================] - 23s 323us/sample - loss: 0.2412 - categorical_accuracy: 0.9143 - val_loss: 0.1312 - val_categorical_accuracy: 0.9603\n",
      "30000/30000 [==============================] - 4s 131us/sample - loss: 0.1312 - categorical_accuracy: 0.9603\n",
      "\n",
      "--- Starting trial: run-11\n",
      "{'layers': 8, 'num_units': 16, 'learning_rate': 32}\n",
      "Train on 70000 samples, validate on 30000 samples\n",
      "70000/70000 [==============================] - 23s 330us/sample - loss: 0.2167 - categorical_accuracy: 0.9229 - val_loss: 0.1361 - val_categorical_accuracy: 0.9582\n",
      "30000/30000 [==============================] - 4s 139us/sample - loss: 0.1361 - categorical_accuracy: 0.9582\n",
      "\n",
      "--- Starting trial: run-12\n",
      "{'layers': 8, 'num_units': 32, 'learning_rate': 4}\n",
      "Train on 70000 samples, validate on 30000 samples\n",
      "70000/70000 [==============================] - 22s 319us/sample - loss: 0.2832 - categorical_accuracy: 0.8948 - val_loss: 0.4826 - val_categorical_accuracy: 0.9497\n",
      "30000/30000 [==============================] - 4s 142us/sample - loss: 0.4826 - categorical_accuracy: 0.9497\n",
      "\n",
      "--- Starting trial: run-13\n",
      "{'layers': 8, 'num_units': 32, 'learning_rate': 8}\n",
      "Train on 70000 samples, validate on 30000 samples\n",
      "70000/70000 [==============================] - 23s 333us/sample - loss: 0.2257 - categorical_accuracy: 0.9191 - val_loss: 0.1798 - val_categorical_accuracy: 0.9539\n",
      "30000/30000 [==============================] - 4s 139us/sample - loss: 0.1798 - categorical_accuracy: 0.9539 - loss:\n",
      "\n",
      "--- Starting trial: run-14\n",
      "{'layers': 8, 'num_units': 32, 'learning_rate': 16}\n",
      "Train on 70000 samples, validate on 30000 samples\n",
      "70000/70000 [==============================] - 21s 306us/sample - loss: 0.2195 - categorical_accuracy: 0.9219 - val_loss: 0.1367 - val_categorical_accuracy: 0.9577\n",
      "30000/30000 [==============================] - 4s 131us/sample - loss: 0.1367 - categorical_accuracy: 0.9577\n",
      "\n",
      "--- Starting trial: run-15\n",
      "{'layers': 8, 'num_units': 32, 'learning_rate': 32}\n",
      "Train on 70000 samples, validate on 30000 samples\n",
      "70000/70000 [==============================] - 23s 326us/sample - loss: 0.2174 - categorical_accuracy: 0.9235 - val_loss: 0.3303 - val_categorical_accuracy: 0.9426\n",
      "30000/30000 [==============================] - 4s 124us/sample - loss: 0.3303 - categorical_accuracy: 0.9426\n",
      "\n",
      "--- Starting trial: run-16\n",
      "{'layers': 16, 'num_units': 4, 'learning_rate': 4}\n",
      "Train on 70000 samples, validate on 30000 samples\n",
      "70000/70000 [==============================] - 27s 390us/sample - loss: 0.9970 - categorical_accuracy: 0.5948 - val_loss: 0.9586 - val_categorical_accuracy: 0.5936\n",
      "30000/30000 [==============================] - 6s 191us/sample - loss: 0.9586 - categorical_accuracy: 0.5936 - loss: 0.9587 - categorical_accu\n",
      "\n",
      "--- Starting trial: run-17\n",
      "{'layers': 16, 'num_units': 4, 'learning_rate': 8}\n",
      "Train on 70000 samples, validate on 30000 samples\n",
      "70000/70000 [==============================] - 29s 416us/sample - loss: 0.9748 - categorical_accuracy: 0.5948 - val_loss: 0.9561 - val_categorical_accuracy: 0.5936\n",
      "30000/30000 [==============================] - 5s 157us/sample - loss: 0.9561 - categorical_accuracy: 0.5936\n",
      "\n",
      "--- Starting trial: run-18\n",
      "{'layers': 16, 'num_units': 4, 'learning_rate': 16}\n",
      "Train on 70000 samples, validate on 30000 samples\n",
      "70000/70000 [==============================] - 29s 412us/sample - loss: 0.9659 - categorical_accuracy: 0.5948 - val_loss: 0.9562 - val_categorical_accuracy: 0.5936\n",
      "30000/30000 [==============================] - 5s 151us/sample - loss: 0.9562 - categorical_accuracy: 0.5936\n",
      "\n",
      "--- Starting trial: run-19\n",
      "{'layers': 16, 'num_units': 4, 'learning_rate': 32}\n",
      "Train on 70000 samples, validate on 30000 samples\n",
      "70000/70000 [==============================] - 27s 391us/sample - loss: 0.9595 - categorical_accuracy: 0.5948 - val_loss: 0.9566 - val_categorical_accuracy: 0.5936\n",
      "30000/30000 [==============================] - 5s 183us/sample - loss: 0.9566 - categorical_accuracy: 0.5936\n",
      "\n",
      "--- Starting trial: run-20\n",
      "{'layers': 16, 'num_units': 8, 'learning_rate': 4}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 70000 samples, validate on 30000 samples\n",
      "70000/70000 [==============================] - 35s 505us/sample - loss: 0.6469 - categorical_accuracy: 0.7669 - val_loss: 0.3646 - val_categorical_accuracy: 0.9212\n",
      "30000/30000 [==============================] - 4s 143us/sample - loss: 0.3646 - categorical_accuracy: 0.9212\n",
      "\n",
      "--- Starting trial: run-21\n",
      "{'layers': 16, 'num_units': 8, 'learning_rate': 8}\n",
      "Train on 70000 samples, validate on 30000 samples\n",
      "70000/70000 [==============================] - 28s 401us/sample - loss: 0.3111 - categorical_accuracy: 0.8803 - val_loss: 1.6887 - val_categorical_accuracy: 0.9597\n",
      "30000/30000 [==============================] - 4s 138us/sample - loss: 1.6887 - categorical_accuracy: 0.9597\n",
      "\n",
      "--- Starting trial: run-22\n",
      "{'layers': 16, 'num_units': 8, 'learning_rate': 16}\n",
      "Train on 70000 samples, validate on 30000 samples\n",
      "70000/70000 [==============================] - 26s 377us/sample - loss: 0.3887 - categorical_accuracy: 0.8595 - val_loss: 0.2494 - val_categorical_accuracy: 0.9112\n",
      "30000/30000 [==============================] - 4s 135us/sample - loss: 0.2494 - categorical_accuracy: 0.9112\n",
      "\n",
      "--- Starting trial: run-23\n",
      "{'layers': 16, 'num_units': 8, 'learning_rate': 32}\n",
      "Train on 70000 samples, validate on 30000 samples\n",
      "70000/70000 [==============================] - 31s 443us/sample - loss: 0.3603 - categorical_accuracy: 0.8755 - val_loss: 0.2092 - val_categorical_accuracy: 0.9400\n",
      "30000/30000 [==============================] - 5s 173us/sample - loss: 0.2092 - categorical_accuracy: 0.9400 - loss: 0.2088 - categorical_accuracy: 0.94\n",
      "\n",
      "--- Starting trial: run-24\n",
      "{'layers': 16, 'num_units': 16, 'learning_rate': 4}\n",
      "Train on 70000 samples, validate on 30000 samples\n",
      "70000/70000 [==============================] - 28s 400us/sample - loss: 0.4185 - categorical_accuracy: 0.8357 - val_loss: 0.3394 - val_categorical_accuracy: 0.9331\n",
      "30000/30000 [==============================] - 4s 143us/sample - loss: 0.3394 - categorical_accuracy: 0.9331\n",
      "\n",
      "--- Starting trial: run-25\n",
      "{'layers': 16, 'num_units': 16, 'learning_rate': 8}\n",
      "Train on 70000 samples, validate on 30000 samples\n",
      "70000/70000 [==============================] - 28s 393us/sample - loss: 0.3341 - categorical_accuracy: 0.8789 - val_loss: 0.3206 - val_categorical_accuracy: 0.9506\n",
      "30000/30000 [==============================] - 5s 163us/sample - loss: 0.3206 - categorical_accuracy: 0.9506\n",
      "\n",
      "--- Starting trial: run-26\n",
      "{'layers': 16, 'num_units': 16, 'learning_rate': 16}\n",
      "Train on 70000 samples, validate on 30000 samples\n",
      "70000/70000 [==============================] - 29s 408us/sample - loss: 0.3012 - categorical_accuracy: 0.8932 - val_loss: 0.2189 - val_categorical_accuracy: 0.9210\n",
      "30000/30000 [==============================] - 5s 162us/sample - loss: 0.2189 - categorical_accuracy: 0.9210\n",
      "\n",
      "--- Starting trial: run-27\n",
      "{'layers': 16, 'num_units': 16, 'learning_rate': 32}\n",
      "Train on 70000 samples, validate on 30000 samples\n",
      "70000/70000 [==============================] - 27s 383us/sample - loss: 0.3557 - categorical_accuracy: 0.8656 - val_loss: 0.1877 - val_categorical_accuracy: 0.9396\n",
      "30000/30000 [==============================] - 5s 156us/sample - loss: 0.1877 - categorical_accuracy: 0.9396\n",
      "\n",
      "--- Starting trial: run-28\n",
      "{'layers': 16, 'num_units': 32, 'learning_rate': 4}\n",
      "Train on 70000 samples, validate on 30000 samples\n",
      "70000/70000 [==============================] - 31s 444us/sample - loss: 0.3623 - categorical_accuracy: 0.8619 - val_loss: 0.3322 - val_categorical_accuracy: 0.9404\n",
      "30000/30000 [==============================] - 5s 155us/sample - loss: 0.3322 - categorical_accuracy: 0.9404 - loss: 0.3555 - categori\n",
      "\n",
      "--- Starting trial: run-29\n",
      "{'layers': 16, 'num_units': 32, 'learning_rate': 8}\n",
      "Train on 70000 samples, validate on 30000 samples\n",
      "70000/70000 [==============================] - 32s 457us/sample - loss: 0.2890 - categorical_accuracy: 0.8948 - val_loss: 0.5452 - val_categorical_accuracy: 0.9594\n",
      "30000/30000 [==============================] - 4s 150us/sample - loss: 0.5452 - categorical_accuracy: 0.9594\n",
      "\n",
      "--- Starting trial: run-30\n",
      "{'layers': 16, 'num_units': 32, 'learning_rate': 16}\n",
      "Train on 70000 samples, validate on 30000 samples\n",
      "70000/70000 [==============================] - 31s 445us/sample - loss: 0.2587 - categorical_accuracy: 0.9075 - val_loss: 0.2422 - val_categorical_accuracy: 0.9125\n",
      "30000/30000 [==============================] - 4s 148us/sample - loss: 0.2422 - categorical_accuracy: 0.9125\n",
      "\n",
      "--- Starting trial: run-31\n",
      "{'layers': 16, 'num_units': 32, 'learning_rate': 32}\n",
      "Train on 70000 samples, validate on 30000 samples\n",
      "70000/70000 [==============================] - 31s 437us/sample - loss: 0.2473 - categorical_accuracy: 0.9125 - val_loss: 0.2707 - val_categorical_accuracy: 0.9604\n",
      "30000/30000 [==============================] - 5s 158us/sample - loss: 0.2707 - categorical_accuracy: 0.9604\n",
      "32\n"
     ]
    }
   ],
   "source": [
    "session_num = 0\n",
    "\n",
    "for deep_layers in HP_LAYERS.domain.values:\n",
    "    for num_units in HP_NUM_UNITS.domain.values:\n",
    "        for learning_rate in HP_LEARNING.domain.values:\n",
    "#             for batch_size in HP_BATCHSIZE.domain.values:\n",
    "            hparams = {\n",
    "\n",
    "                HP_LAYERS: deep_layers,\n",
    "                HP_NUM_UNITS: num_units,\n",
    "                HP_LEARNING: learning_rate,\n",
    "#                     HP_BATCHSIZE: batch_size\n",
    "            }\n",
    "            run_name = \"run-%d\" % session_num\n",
    "            print('\\n--- Starting trial: %s' % run_name)\n",
    "            print({h.name: hparams[h] for h in hparams})\n",
    "            run('logs/hparam_tuning/' + run_name, hparams)\n",
    "            session_num += 1\n",
    "\n",
    "print(session_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-89-8a7b594d7468>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-89-8a7b594d7468>\"\u001b[1;36m, line \u001b[1;32m1\u001b[0m\n\u001b[1;33m    rm -rf /tmp/tb_logs/\u001b[0m\n\u001b[1;37m                        ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "rm -rf /tmp/tb_logs/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !kill 16011"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Launching TensorBoard..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%tensorboard --logdir logs/hparam_tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
