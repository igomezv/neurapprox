{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gS9bn7bhqpCG",
    "outputId": "38bcc44c-8019-449a-e1ee-2de1e086c68e"
   },
   "outputs": [],
   "source": [
    "# Clear any logs from previous runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "iD0I8HwdqpCb"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time, os\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorboard.plugins.hparams import api as hp\n",
    "\n",
    "from tensorflow.keras.losses import CategoricalCrossentropy\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split as split\n",
    "\n",
    "import random\n",
    "from math import floor\n",
    "from tensorflow.keras import models\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import optimizers\n",
    "from scipy.integrate import odeint\n",
    "import scipy\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "aFY-8_lrqpCg"
   },
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "\n",
    "tf.config.optimizer.set_jit(True)\n",
    "scaler = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "fsRv8kuNqpCw",
    "outputId": "773d68d4-3c4e-42e4-c9ca-be5c60ac5412"
   },
   "outputs": [],
   "source": [
    "#algorithm for splitting the dataset into training and validation \n",
    "def split(X,Y,porcent): #porcent must be between 0 and 1, it is the asigned porcent to the training dataset.\n",
    "    n=floor(porcent*len(X))\n",
    "    index=random.sample(range(len(X)),n)\n",
    "    X_learn=[]\n",
    "    Y_learn=[]\n",
    "    for i in index:\n",
    "        X_learn.append(X[i])\n",
    "        Y_learn.append(Y[i])\n",
    "    X_val=np.delete(X,index, axis=0)\n",
    "    Y_val=np.delete(Y,index, axis=0)\n",
    "    \n",
    "    X_learn=np.array(X_learn)\n",
    "    Y_learn=np.array(Y_learn)\n",
    "    return X_learn,Y_learn,X_val,Y_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "O_m=np.arange(0.1,0.51,0.01)\n",
    "H_0=np.arange(66,81,1)\n",
    "t=np.linspace(0,-12,50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RHS(Omega_i, lna, gamma=0):\n",
    "    x, y, z, H = Omega_i\n",
    "    #x, y, z = Omega_i\n",
    "    pi = 3*x + 4*y\n",
    "    return [x*(-3 + pi), y*(-4 + pi), z*pi, -0.5*H*pi]\n",
    "    #return [x*(-3 + pi), y*(-4 + pi), z*pi]\n",
    "\n",
    "def EDO(t,Om,H0):\n",
    "    #t,Or,Om,Ol=X\n",
    "    Or=0.0001\n",
    "    Ol=1-Or-Om\n",
    "    #H0 = 70.\n",
    "    y0 = [Om, Or, Ol, H0]\n",
    "    result = odeint(RHS, y0, t)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lets generate the cartesian product between the intervals\n",
    "Y0=[]\n",
    "#este ciclo llena la lista fijando un Om y pasando todos los Or\n",
    "for i in O_m:\n",
    "    for j in H_0:\n",
    "        Y0.extend(EDO(t,i,j))\n",
    "Y0=np.array(Y0)\n",
    "\n",
    "X0=[]\n",
    "for Om in O_m:\n",
    "    for H0 in H_0:\n",
    "        for T in t:\n",
    "            X0.append([T,Om,H0])\n",
    "X0=np.array(X0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "scaler = preprocessing.MinMaxScaler(feature_range = (0,1))\n",
    "Y2 = scaler.fit_transform(Y0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feactures= \n",
      " [[  0.           0.1         66.        ]\n",
      " [ -0.24489796   0.1         66.        ]\n",
      " [ -0.48979592   0.1         66.        ]\n",
      " ...\n",
      " [-11.51020408   0.5         80.        ]\n",
      " [-11.75510204   0.5         80.        ]\n",
      " [-12.           0.5         80.        ]]\n",
      "\n",
      "\n",
      "labels= \n",
      " [[9.47515373e-02 0.00000000e+00 1.00000000e+00 0.00000000e+00]\n",
      " [1.83608702e-01 1.41110961e-04 9.01997379e-01 1.62358493e-10]\n",
      " [3.22333902e-01 4.33937896e-04 7.48914062e-01 4.77144941e-10]\n",
      " ...\n",
      " [4.20164900e-02 9.58104047e-01 5.58399792e-11 3.78984027e-01]\n",
      " [3.19464665e-02 9.68145182e-01 5.73926563e-11 6.15280156e-01]\n",
      " [2.39153274e-02 9.76153281e-01 5.75109456e-11 1.00000000e+00]]\n"
     ]
    }
   ],
   "source": [
    "#Now, here are the datasets\n",
    "print('feactures= \\n',X0)\n",
    "print('\\n')\n",
    "print('labels= \\n',Y2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_size = 0.8\n",
    "X_train, Y_train, X_test, Y_test = split(X0, Y2, split_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CgjOFB8kqpC_"
   },
   "source": [
    "### Hiperpar√°metros del modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "ABViOAgTqpDI"
   },
   "outputs": [],
   "source": [
    "# HP_NUM_UNITS3 = hp.HParam('num_units3', hp.Discrete([50, 100, 150, 200]))\n",
    "# HP_NUM_UNITS4 = hp.HParam('num_units4', hp.Discrete([2, 5, 10]))\n",
    "# HP_DROPOUT = hp.HParam('dropout', hp.RealInterval(0.0, 0.2))\n",
    "# HP_OPTIMIZER = hp.HParam('optimizer', hp.Discrete(['adam', 'sgd']))\n",
    "# HP_OPTIMIZER = hp.HParam('optimizer', hp.Discrete(['adam', 'Adadelta']))\n",
    "# HP_OPTIMIZER = hp.HParam('optimizer', hp.Discrete(['adam']))\n",
    "HP_BATCHSIZE = hp.HParam('batch_size', hp.Discrete([4, 8]))\n",
    "\n",
    "\n",
    "HP_LAYERS =    hp.HParam('layers', hp.Discrete([2, 3, 4]))\n",
    "HP_NUM_UNITS = hp.HParam('num_units', hp.Discrete([50, 100, 200]))\n",
    "HP_LEARNING  = hp.HParam('learning_rate', hp.Discrete([1e-5,1e-4,1e-3]))\n",
    "\n",
    "callbacks = [tf.keras.callbacks.EarlyStopping(monitor='val_loss', mode='min',\n",
    "                                   min_delta=0,\n",
    "                                   patience=10,\n",
    "                                   restore_best_weights=True)]\n",
    "# batch_size = 128\n",
    "epochs = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "lXRw_6G8qpDL"
   },
   "outputs": [],
   "source": [
    "# METRIC_ACCURACY = 'accuracy'\n",
    "with tf.summary.create_file_writer('logs/hparam_tuning2').as_default():\n",
    "# with tf.summary.FileWriter('logs/hparam_tuning', sess.graph):\n",
    "#     init = tf.initialize_all_variables()\n",
    "#     sess.run(init)\n",
    "    hp.hparams_config(\n",
    "        hparams=[HP_LAYERS,\n",
    "                 HP_NUM_UNITS,\n",
    "                 HP_LEARNING, \n",
    "                 HP_BATCHSIZE],\n",
    "        metrics=[hp.Metric('loss', display_name=\"Loss\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "6JG3WeEOqpDY"
   },
   "outputs": [],
   "source": [
    "def train_test_model(hparams):    \n",
    "    \n",
    "    # Train LSTM model and predict on validation set\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(Dense(int(X_train.shape[1])))\n",
    "    \n",
    "    for i in range(hparams[HP_LAYERS]):        \n",
    "        model.add(Dense(hparams[HP_NUM_UNITS], activation='relu'))\n",
    "    model.add(Dense(4, activation='linear'))\n",
    "     \n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=hparams[HP_LEARNING], beta_1=0.9, beta_2=0.999, epsilon=1e-3)\n",
    "    model.compile(\n",
    "            optimizer=optimizer,\n",
    "            loss='mse', \n",
    "            metrics=['mean_squared_error'])\n",
    "    \n",
    "    # Run with 1 epoch to speed things up for demo purposes\n",
    "\n",
    "    model.fit(X_train, Y_train, epochs=epochs, validation_data=(X_test, Y_test),\n",
    "              callbacks=callbacks, batch_size=hparams[HP_BATCHSIZE], shuffle=False, verbose=0)\n",
    "\n",
    "    _, loss = model.evaluate(X_test, Y_test)\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "fnLlAKGuqpDp"
   },
   "outputs": [],
   "source": [
    "def run(run_dir, hparams):\n",
    "    with tf.summary.create_file_writer(run_dir).as_default():\n",
    "        hp.hparams(hparams)  # record the values used in this trial\n",
    "        loss = train_test_model(hparams)\n",
    "        tf.summary.scalar(\"loss\", loss, step=1)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5tu8q13lqpDs",
    "outputId": "80f9eb4f-dc3a-445e-ff6d-3d42dcb085c3",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Starting trial: run-0\n",
      "{'layers': 2, 'num_units': 50, 'learning_rate': 1e-05, 'batch_size': 4}\n",
      "193/193 [==============================] - 0s 745us/step - loss: 0.0020 - mean_squared_error: 0.0020\n",
      "Loss: 0.0020131533965468407 Tiempo transcurrido: 4730.5225303173065\n",
      "\n",
      "--- Starting trial: run-1\n",
      "{'layers': 2, 'num_units': 50, 'learning_rate': 1e-05, 'batch_size': 8}\n",
      "193/193 [==============================] - 0s 634us/step - loss: 0.0022 - mean_squared_error: 0.0022\n",
      "Loss: 0.0021797586232423782 Tiempo transcurrido: 1345.9331152439117\n",
      "\n",
      "--- Starting trial: run-2\n",
      "{'layers': 2, 'num_units': 50, 'learning_rate': 0.0001, 'batch_size': 4}\n",
      "193/193 [==============================] - 0s 699us/step - loss: 1.0718e-04 - mean_squared_error: 1.0718e-04\n",
      "Loss: 0.00010718284465838224 Tiempo transcurrido: 771.9668474197388\n",
      "\n",
      "--- Starting trial: run-3\n",
      "{'layers': 2, 'num_units': 50, 'learning_rate': 0.0001, 'batch_size': 8}\n",
      "193/193 [==============================] - 0s 707us/step - loss: 2.6392e-04 - mean_squared_error: 2.6392e-04\n",
      "Loss: 0.0002639235754031688 Tiempo transcurrido: 712.2670438289642\n",
      "\n",
      "--- Starting trial: run-4\n",
      "{'layers': 2, 'num_units': 50, 'learning_rate': 0.001, 'batch_size': 4}\n",
      "193/193 [==============================] - 0s 730us/step - loss: 1.2453e-04 - mean_squared_error: 1.2453e-04\n",
      "Loss: 0.00012452711234800518 Tiempo transcurrido: 211.39858174324036\n",
      "\n",
      "--- Starting trial: run-5\n",
      "{'layers': 2, 'num_units': 50, 'learning_rate': 0.001, 'batch_size': 8}\n",
      "193/193 [==============================] - 0s 646us/step - loss: 7.4074e-05 - mean_squared_error: 7.4074e-05\n",
      "Loss: 7.407442171825096e-05 Tiempo transcurrido: 157.68184113502502\n",
      "\n",
      "--- Starting trial: run-6\n",
      "{'layers': 2, 'num_units': 100, 'learning_rate': 1e-05, 'batch_size': 4}\n",
      "193/193 [==============================] - 0s 625us/step - loss: 3.3341e-04 - mean_squared_error: 3.3341e-04\n",
      "Loss: 0.0003334062057547271 Tiempo transcurrido: 2801.256538629532\n",
      "\n",
      "--- Starting trial: run-7\n",
      "{'layers': 2, 'num_units': 100, 'learning_rate': 1e-05, 'batch_size': 8}\n",
      "193/193 [==============================] - 0s 720us/step - loss: 5.9561e-04 - mean_squared_error: 5.9561e-04\n",
      "Loss: 0.0005956068052910268 Tiempo transcurrido: 1397.297986984253\n",
      "\n",
      "--- Starting trial: run-8\n",
      "{'layers': 2, 'num_units': 100, 'learning_rate': 0.0001, 'batch_size': 4}\n",
      "193/193 [==============================] - 0s 723us/step - loss: 1.6655e-04 - mean_squared_error: 1.6655e-04\n",
      "Loss: 0.00016655218496453017 Tiempo transcurrido: 545.1245772838593\n",
      "\n",
      "--- Starting trial: run-9\n",
      "{'layers': 2, 'num_units': 100, 'learning_rate': 0.0001, 'batch_size': 8}\n",
      "193/193 [==============================] - 0s 745us/step - loss: 6.8128e-05 - mean_squared_error: 6.8128e-05\n",
      "Loss: 6.812799256294966e-05 Tiempo transcurrido: 746.6576068401337\n",
      "\n",
      "--- Starting trial: run-10\n",
      "{'layers': 2, 'num_units': 100, 'learning_rate': 0.001, 'batch_size': 4}\n",
      "193/193 [==============================] - 0s 758us/step - loss: 2.4739e-04 - mean_squared_error: 2.4739e-04\n",
      "Loss: 0.0002473881177138537 Tiempo transcurrido: 153.2604112625122\n",
      "\n",
      "--- Starting trial: run-11\n",
      "{'layers': 2, 'num_units': 100, 'learning_rate': 0.001, 'batch_size': 8}\n",
      "193/193 [==============================] - 0s 693us/step - loss: 1.1632e-04 - mean_squared_error: 1.1632e-04\n",
      "Loss: 0.00011632191308308393 Tiempo transcurrido: 123.79425549507141\n",
      "\n",
      "--- Starting trial: run-12\n",
      "{'layers': 2, 'num_units': 200, 'learning_rate': 1e-05, 'batch_size': 4}\n",
      "193/193 [==============================] - 0s 821us/step - loss: 1.9443e-04 - mean_squared_error: 1.9443e-04\n",
      "Loss: 0.00019442610209807754 Tiempo transcurrido: 3210.731435775757\n",
      "\n",
      "--- Starting trial: run-13\n",
      "{'layers': 2, 'num_units': 200, 'learning_rate': 1e-05, 'batch_size': 8}\n",
      "193/193 [==============================] - 0s 745us/step - loss: 0.0013 - mean_squared_error: 0.0013\n",
      "Loss: 0.0012639302294701338 Tiempo transcurrido: 1638.414737701416\n",
      "\n",
      "--- Starting trial: run-14\n",
      "{'layers': 2, 'num_units': 200, 'learning_rate': 0.0001, 'batch_size': 4}\n",
      "193/193 [==============================] - 0s 732us/step - loss: 8.8155e-05 - mean_squared_error: 8.8155e-05\n",
      "Loss: 8.81551459315233e-05 Tiempo transcurrido: 732.597160577774\n",
      "\n",
      "--- Starting trial: run-15\n",
      "{'layers': 2, 'num_units': 200, 'learning_rate': 0.0001, 'batch_size': 8}\n",
      "193/193 [==============================] - 0s 734us/step - loss: 1.0204e-04 - mean_squared_error: 1.0204e-04\n",
      "Loss: 0.00010204215504927561 Tiempo transcurrido: 502.4558551311493\n",
      "\n",
      "--- Starting trial: run-16\n",
      "{'layers': 2, 'num_units': 200, 'learning_rate': 0.001, 'batch_size': 4}\n",
      "193/193 [==============================] - 0s 779us/step - loss: 1.8972e-04 - mean_squared_error: 1.8972e-04\n",
      "Loss: 0.00018972092948388308 Tiempo transcurrido: 244.47448182106018\n",
      "\n",
      "--- Starting trial: run-17\n",
      "{'layers': 2, 'num_units': 200, 'learning_rate': 0.001, 'batch_size': 8}\n",
      "193/193 [==============================] - 0s 693us/step - loss: 3.3867e-05 - mean_squared_error: 3.3867e-05\n",
      "Loss: 3.3866796002257615e-05 Tiempo transcurrido: 254.5328929424286\n",
      "\n",
      "--- Starting trial: run-18\n",
      "{'layers': 3, 'num_units': 50, 'learning_rate': 1e-05, 'batch_size': 4}\n",
      "193/193 [==============================] - 0s 764us/step - loss: 3.2034e-04 - mean_squared_error: 3.2034e-04\n",
      "Loss: 0.0003203378582838923 Tiempo transcurrido: 2869.7203347682953\n",
      "\n",
      "--- Starting trial: run-19\n",
      "{'layers': 3, 'num_units': 50, 'learning_rate': 1e-05, 'batch_size': 8}\n",
      "193/193 [==============================] - 0s 652us/step - loss: 7.4141e-04 - mean_squared_error: 7.4141e-04\n",
      "Loss: 0.0007414129213429987 Tiempo transcurrido: 1460.2724695205688\n",
      "\n",
      "--- Starting trial: run-20\n",
      "{'layers': 3, 'num_units': 50, 'learning_rate': 0.0001, 'batch_size': 4}\n",
      "193/193 [==============================] - 0s 756us/step - loss: 1.3519e-04 - mean_squared_error: 1.3519e-04\n",
      "Loss: 0.00013519414642360061 Tiempo transcurrido: 668.6730155944824\n",
      "\n",
      "--- Starting trial: run-21\n",
      "{'layers': 3, 'num_units': 50, 'learning_rate': 0.0001, 'batch_size': 8}\n",
      "193/193 [==============================] - 0s 681us/step - loss: 7.4651e-04 - mean_squared_error: 7.4651e-04\n",
      "Loss: 0.0007465105736628175 Tiempo transcurrido: 272.04115891456604\n",
      "\n",
      "--- Starting trial: run-22\n",
      "{'layers': 3, 'num_units': 50, 'learning_rate': 0.001, 'batch_size': 4}\n",
      "193/193 [==============================] - 0s 687us/step - loss: 1.5170e-04 - mean_squared_error: 1.5170e-04\n",
      "Loss: 0.00015170285769272596 Tiempo transcurrido: 191.29102849960327\n",
      "\n",
      "--- Starting trial: run-23\n",
      "{'layers': 3, 'num_units': 50, 'learning_rate': 0.001, 'batch_size': 8}\n",
      "193/193 [==============================] - 0s 708us/step - loss: 6.6196e-05 - mean_squared_error: 6.6196e-05\n",
      "Loss: 6.619629130000249e-05 Tiempo transcurrido: 135.83753275871277\n",
      "\n",
      "--- Starting trial: run-24\n",
      "{'layers': 3, 'num_units': 100, 'learning_rate': 1e-05, 'batch_size': 4}\n",
      "193/193 [==============================] - 0s 676us/step - loss: 2.8601e-04 - mean_squared_error: 2.8601e-04\n",
      "Loss: 0.0002860112290363759 Tiempo transcurrido: 3185.1875998973846\n",
      "\n",
      "--- Starting trial: run-25\n",
      "{'layers': 3, 'num_units': 100, 'learning_rate': 1e-05, 'batch_size': 8}\n",
      "193/193 [==============================] - 0s 767us/step - loss: 5.7710e-04 - mean_squared_error: 5.7710e-04\n",
      "Loss: 0.0005770966527052224 Tiempo transcurrido: 1597.9291334152222\n",
      "\n",
      "--- Starting trial: run-26\n",
      "{'layers': 3, 'num_units': 100, 'learning_rate': 0.0001, 'batch_size': 4}\n",
      "193/193 [==============================] - 0s 729us/step - loss: 2.7650e-04 - mean_squared_error: 2.7650e-04\n",
      "Loss: 0.0002764998353086412 Tiempo transcurrido: 382.51919889450073\n",
      "\n",
      "--- Starting trial: run-27\n",
      "{'layers': 3, 'num_units': 100, 'learning_rate': 0.0001, 'batch_size': 8}\n",
      "193/193 [==============================] - 0s 757us/step - loss: 6.2991e-05 - mean_squared_error: 6.2991e-05\n",
      "Loss: 6.299050437519327e-05 Tiempo transcurrido: 437.5137369632721\n",
      "\n",
      "--- Starting trial: run-28\n",
      "{'layers': 3, 'num_units': 100, 'learning_rate': 0.001, 'batch_size': 4}\n",
      "193/193 [==============================] - 0s 776us/step - loss: 4.6706e-05 - mean_squared_error: 4.6706e-05\n",
      "Loss: 4.6706481953151524e-05 Tiempo transcurrido: 352.5962851047516\n",
      "\n",
      "--- Starting trial: run-29\n",
      "{'layers': 3, 'num_units': 100, 'learning_rate': 0.001, 'batch_size': 8}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "193/193 [==============================] - 0s 692us/step - loss: 2.1668e-05 - mean_squared_error: 2.1668e-05\n",
      "Loss: 2.1668063709512353e-05 Tiempo transcurrido: 285.2394669055939\n",
      "\n",
      "--- Starting trial: run-30\n",
      "{'layers': 3, 'num_units': 200, 'learning_rate': 1e-05, 'batch_size': 4}\n",
      "193/193 [==============================] - 0s 749us/step - loss: 9.3977e-05 - mean_squared_error: 9.3977e-05\n",
      "Loss: 9.397675603395328e-05 Tiempo transcurrido: 3118.094927549362\n",
      "\n",
      "--- Starting trial: run-31\n",
      "{'layers': 3, 'num_units': 200, 'learning_rate': 1e-05, 'batch_size': 8}\n",
      "193/193 [==============================] - 0s 844us/step - loss: 2.2829e-04 - mean_squared_error: 2.2829e-04\n",
      "Loss: 0.00022828910732641816 Tiempo transcurrido: 1993.593955039978\n",
      "\n",
      "--- Starting trial: run-32\n",
      "{'layers': 3, 'num_units': 200, 'learning_rate': 0.0001, 'batch_size': 4}\n",
      "193/193 [==============================] - 0s 849us/step - loss: 1.3670e-04 - mean_squared_error: 1.3670e-04\n",
      "Loss: 0.00013670085172634572 Tiempo transcurrido: 518.0872182846069\n",
      "\n",
      "--- Starting trial: run-33\n",
      "{'layers': 3, 'num_units': 200, 'learning_rate': 0.0001, 'batch_size': 8}\n",
      "193/193 [==============================] - 0s 859us/step - loss: 3.0937e-05 - mean_squared_error: 3.0937e-05\n",
      "Loss: 3.0937262636143714e-05 Tiempo transcurrido: 779.6192529201508\n",
      "\n",
      "--- Starting trial: run-34\n",
      "{'layers': 3, 'num_units': 200, 'learning_rate': 0.001, 'batch_size': 4}\n",
      "193/193 [==============================] - 0s 755us/step - loss: 8.7425e-05 - mean_squared_error: 8.7425e-05\n",
      "Loss: 8.742528007132933e-05 Tiempo transcurrido: 448.16570377349854\n",
      "\n",
      "--- Starting trial: run-35\n",
      "{'layers': 3, 'num_units': 200, 'learning_rate': 0.001, 'batch_size': 8}\n",
      "193/193 [==============================] - 0s 872us/step - loss: 1.7661e-05 - mean_squared_error: 1.7661e-05\n",
      "Loss: 1.7661084712017328e-05 Tiempo transcurrido: 506.0320653915405\n",
      "\n",
      "--- Starting trial: run-36\n",
      "{'layers': 4, 'num_units': 50, 'learning_rate': 1e-05, 'batch_size': 4}\n",
      "193/193 [==============================] - 0s 654us/step - loss: 3.1650e-04 - mean_squared_error: 3.1650e-04\n",
      "Loss: 0.00031649720040149987 Tiempo transcurrido: 2813.05335521698\n",
      "\n",
      "--- Starting trial: run-37\n",
      "{'layers': 4, 'num_units': 50, 'learning_rate': 1e-05, 'batch_size': 8}\n",
      "193/193 [==============================] - 0s 699us/step - loss: 5.0804e-04 - mean_squared_error: 5.0804e-04\n",
      "Loss: 0.0005080433911643922 Tiempo transcurrido: 1517.5322096347809\n",
      "\n",
      "--- Starting trial: run-38\n",
      "{'layers': 4, 'num_units': 50, 'learning_rate': 0.0001, 'batch_size': 4}\n",
      "193/193 [==============================] - 0s 690us/step - loss: 9.1392e-05 - mean_squared_error: 9.1392e-05\n",
      "Loss: 9.139219764620066e-05 Tiempo transcurrido: 554.2784388065338\n",
      "\n",
      "--- Starting trial: run-39\n",
      "{'layers': 4, 'num_units': 50, 'learning_rate': 0.0001, 'batch_size': 8}\n",
      "193/193 [==============================] - 0s 700us/step - loss: 3.2037e-05 - mean_squared_error: 3.2037e-05\n",
      "Loss: 3.2036503398558125e-05 Tiempo transcurrido: 619.3950109481812\n",
      "\n",
      "--- Starting trial: run-40\n",
      "{'layers': 4, 'num_units': 50, 'learning_rate': 0.001, 'batch_size': 4}\n",
      "193/193 [==============================] - 0s 601us/step - loss: 4.5273e-05 - mean_squared_error: 4.5273e-05\n",
      "Loss: 4.527337659965269e-05 Tiempo transcurrido: 200.92561841011047\n",
      "\n",
      "--- Starting trial: run-41\n",
      "{'layers': 4, 'num_units': 50, 'learning_rate': 0.001, 'batch_size': 8}\n",
      "193/193 [==============================] - 0s 729us/step - loss: 6.5552e-05 - mean_squared_error: 6.5552e-05\n",
      "Loss: 6.555152504006401e-05 Tiempo transcurrido: 156.96803617477417\n",
      "\n",
      "--- Starting trial: run-42\n",
      "{'layers': 4, 'num_units': 100, 'learning_rate': 1e-05, 'batch_size': 4}\n",
      "193/193 [==============================] - 0s 830us/step - loss: 6.4603e-05 - mean_squared_error: 6.4603e-05\n",
      "Loss: 6.460330041591078e-05 Tiempo transcurrido: 2263.5951788425446\n",
      "\n",
      "--- Starting trial: run-43\n",
      "{'layers': 4, 'num_units': 100, 'learning_rate': 1e-05, 'batch_size': 8}\n",
      "193/193 [==============================] - 0s 784us/step - loss: 1.7220e-04 - mean_squared_error: 1.7220e-04\n",
      "Loss: 0.00017219669825863093 Tiempo transcurrido: 1746.3207802772522\n",
      "\n",
      "--- Starting trial: run-44\n",
      "{'layers': 4, 'num_units': 100, 'learning_rate': 0.0001, 'batch_size': 4}\n",
      "193/193 [==============================] - 0s 770us/step - loss: 4.1857e-05 - mean_squared_error: 4.1857e-05\n",
      "Loss: 4.1857449105009437e-05 Tiempo transcurrido: 674.0600836277008\n",
      "\n",
      "--- Starting trial: run-45\n",
      "{'layers': 4, 'num_units': 100, 'learning_rate': 0.0001, 'batch_size': 8}\n",
      "193/193 [==============================] - 0s 565us/step - loss: 5.2715e-05 - mean_squared_error: 5.2715e-05\n",
      "Loss: 5.2714745834236965e-05 Tiempo transcurrido: 315.4168210029602\n",
      "\n",
      "--- Starting trial: run-46\n",
      "{'layers': 4, 'num_units': 100, 'learning_rate': 0.001, 'batch_size': 4}\n",
      "193/193 [==============================] - 0s 588us/step - loss: 0.0024 - mean_squared_error: 0.0024\n",
      "Loss: 0.0024223062209784985 Tiempo transcurrido: 149.1238260269165\n",
      "\n",
      "--- Starting trial: run-47\n",
      "{'layers': 4, 'num_units': 100, 'learning_rate': 0.001, 'batch_size': 8}\n",
      "193/193 [==============================] - 0s 568us/step - loss: 1.1006e-04 - mean_squared_error: 1.1006e-04\n",
      "Loss: 0.00011006352724507451 Tiempo transcurrido: 82.30145859718323\n",
      "\n",
      "--- Starting trial: run-48\n",
      "{'layers': 4, 'num_units': 200, 'learning_rate': 1e-05, 'batch_size': 4}\n",
      "193/193 [==============================] - 0s 620us/step - loss: 1.3464e-04 - mean_squared_error: 1.3464e-04\n",
      "Loss: 0.00013463590585161 Tiempo transcurrido: 1732.1581358909607\n",
      "\n",
      "--- Starting trial: run-49\n",
      "{'layers': 4, 'num_units': 200, 'learning_rate': 1e-05, 'batch_size': 8}\n",
      "193/193 [==============================] - 0s 643us/step - loss: 4.9385e-05 - mean_squared_error: 4.9385e-05\n",
      "Loss: 4.938474376103841e-05 Tiempo transcurrido: 1608.7945110797882\n",
      "\n",
      "--- Starting trial: run-50\n",
      "{'layers': 4, 'num_units': 200, 'learning_rate': 0.0001, 'batch_size': 4}\n",
      "193/193 [==============================] - 0s 701us/step - loss: 2.9333e-05 - mean_squared_error: 2.9333e-05\n",
      "Loss: 2.93328794214176e-05 Tiempo transcurrido: 641.1506681442261\n",
      "\n",
      "--- Starting trial: run-51\n",
      "{'layers': 4, 'num_units': 200, 'learning_rate': 0.0001, 'batch_size': 8}\n",
      "193/193 [==============================] - 0s 660us/step - loss: 7.7946e-06 - mean_squared_error: 7.7946e-06\n",
      "Loss: 7.794612429279368e-06 Tiempo transcurrido: 512.2304184436798\n",
      "\n",
      "--- Starting trial: run-52\n",
      "{'layers': 4, 'num_units': 200, 'learning_rate': 0.001, 'batch_size': 4}\n",
      "193/193 [==============================] - 0s 645us/step - loss: 0.0026 - mean_squared_error: 0.0026\n",
      "Loss: 0.002590788993984461 Tiempo transcurrido: 167.74342727661133\n",
      "\n",
      "--- Starting trial: run-53\n",
      "{'layers': 4, 'num_units': 200, 'learning_rate': 0.001, 'batch_size': 8}\n",
      "193/193 [==============================] - 0s 619us/step - loss: 8.1967e-05 - mean_squared_error: 8.1967e-05\n",
      "Loss: 8.1967307778541e-05 Tiempo transcurrido: 96.32768607139587\n",
      "54\n"
     ]
    }
   ],
   "source": [
    "session_num = 0\n",
    "datos = []\n",
    "\n",
    "for deep_layers in HP_LAYERS.domain.values:\n",
    "    for num_units in HP_NUM_UNITS.domain.values:\n",
    "        for learning_rate in HP_LEARNING.domain.values:\n",
    "            for batch_size in HP_BATCHSIZE.domain.values:\n",
    "                t = time.time()\n",
    "                hparams = {\n",
    "\n",
    "                    HP_LAYERS: deep_layers,\n",
    "                    HP_NUM_UNITS: num_units,\n",
    "                    HP_LEARNING: learning_rate,\n",
    "                    HP_BATCHSIZE: batch_size,\n",
    "                }\n",
    "                run_name = \"run-%d\" % session_num\n",
    "                print('\\n--- Starting trial: %s' % run_name)\n",
    "                print({h.name: hparams[h] for h in hparams})\n",
    "                score = run('logs/hparam_tuning2/' + run_name, hparams)\n",
    "                t = time.time()-t\n",
    "                session_num += 1\n",
    "                print(\"Loss:\", score, \"Tiempo transcurrido:\", t)\n",
    "            \n",
    "            datos.append([deep_layers, num_units, learning_rate, batch_size, score, t])\n",
    "\n",
    "print(session_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X1ct8OIfqpD3"
   },
   "source": [
    "### Guardar datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "_6xZZqEhqpD5"
   },
   "outputs": [],
   "source": [
    "filename = \"historial_ecsdiff_tunning.txt\"\n",
    "df = pd.DataFrame(datos, columns = [\"Deep size\", \"Num units\", \"Learning rate\", \"Batch size\", \"MSE\", \"Tiempo de ejecuci√≥n\"])\n",
    "\n",
    "df.sort_values(by=[\"MSE\", \"Tiempo de ejecuci√≥n\"], ascending=[True, True], ignore_index=True, inplace=True)\n",
    "\n",
    "df.to_csv(filename, header=True, index=False, sep='\\t', mode='w') # a=append, w=overwrite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "id": "z9BerX2yqpD-",
    "outputId": "992e08c9-0adb-4f57-e50e-df65c91fe142",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Deep size</th>\n",
       "      <th>Num units</th>\n",
       "      <th>Learning rate</th>\n",
       "      <th>Batch size</th>\n",
       "      <th>MSE</th>\n",
       "      <th>Tiempo de ejecuci√≥n</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>200</td>\n",
       "      <td>0.00010</td>\n",
       "      <td>8</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>512.230418</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>200</td>\n",
       "      <td>0.00100</td>\n",
       "      <td>8</td>\n",
       "      <td>0.000018</td>\n",
       "      <td>506.032065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>100</td>\n",
       "      <td>0.00100</td>\n",
       "      <td>8</td>\n",
       "      <td>0.000022</td>\n",
       "      <td>285.239467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>200</td>\n",
       "      <td>0.00010</td>\n",
       "      <td>8</td>\n",
       "      <td>0.000031</td>\n",
       "      <td>779.619253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>50</td>\n",
       "      <td>0.00010</td>\n",
       "      <td>8</td>\n",
       "      <td>0.000032</td>\n",
       "      <td>619.395011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2</td>\n",
       "      <td>200</td>\n",
       "      <td>0.00100</td>\n",
       "      <td>8</td>\n",
       "      <td>0.000034</td>\n",
       "      <td>254.532893</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>4</td>\n",
       "      <td>200</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>8</td>\n",
       "      <td>0.000049</td>\n",
       "      <td>1608.794511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>4</td>\n",
       "      <td>100</td>\n",
       "      <td>0.00010</td>\n",
       "      <td>8</td>\n",
       "      <td>0.000053</td>\n",
       "      <td>315.416821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>3</td>\n",
       "      <td>100</td>\n",
       "      <td>0.00010</td>\n",
       "      <td>8</td>\n",
       "      <td>0.000063</td>\n",
       "      <td>437.513737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>4</td>\n",
       "      <td>50</td>\n",
       "      <td>0.00100</td>\n",
       "      <td>8</td>\n",
       "      <td>0.000066</td>\n",
       "      <td>156.968036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>3</td>\n",
       "      <td>50</td>\n",
       "      <td>0.00100</td>\n",
       "      <td>8</td>\n",
       "      <td>0.000066</td>\n",
       "      <td>135.837533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2</td>\n",
       "      <td>100</td>\n",
       "      <td>0.00010</td>\n",
       "      <td>8</td>\n",
       "      <td>0.000068</td>\n",
       "      <td>746.657607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2</td>\n",
       "      <td>50</td>\n",
       "      <td>0.00100</td>\n",
       "      <td>8</td>\n",
       "      <td>0.000074</td>\n",
       "      <td>157.681841</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>4</td>\n",
       "      <td>200</td>\n",
       "      <td>0.00100</td>\n",
       "      <td>8</td>\n",
       "      <td>0.000082</td>\n",
       "      <td>96.327686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2</td>\n",
       "      <td>200</td>\n",
       "      <td>0.00010</td>\n",
       "      <td>8</td>\n",
       "      <td>0.000102</td>\n",
       "      <td>502.455855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>4</td>\n",
       "      <td>100</td>\n",
       "      <td>0.00100</td>\n",
       "      <td>8</td>\n",
       "      <td>0.000110</td>\n",
       "      <td>82.301459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2</td>\n",
       "      <td>100</td>\n",
       "      <td>0.00100</td>\n",
       "      <td>8</td>\n",
       "      <td>0.000116</td>\n",
       "      <td>123.794255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>4</td>\n",
       "      <td>100</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>8</td>\n",
       "      <td>0.000172</td>\n",
       "      <td>1746.320780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>3</td>\n",
       "      <td>200</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>8</td>\n",
       "      <td>0.000228</td>\n",
       "      <td>1993.593955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2</td>\n",
       "      <td>50</td>\n",
       "      <td>0.00010</td>\n",
       "      <td>8</td>\n",
       "      <td>0.000264</td>\n",
       "      <td>712.267044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>4</td>\n",
       "      <td>50</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>8</td>\n",
       "      <td>0.000508</td>\n",
       "      <td>1517.532210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>3</td>\n",
       "      <td>100</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>8</td>\n",
       "      <td>0.000577</td>\n",
       "      <td>1597.929133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>2</td>\n",
       "      <td>100</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>8</td>\n",
       "      <td>0.000596</td>\n",
       "      <td>1397.297987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>3</td>\n",
       "      <td>50</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>8</td>\n",
       "      <td>0.000741</td>\n",
       "      <td>1460.272470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>3</td>\n",
       "      <td>50</td>\n",
       "      <td>0.00010</td>\n",
       "      <td>8</td>\n",
       "      <td>0.000747</td>\n",
       "      <td>272.041159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>2</td>\n",
       "      <td>200</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>8</td>\n",
       "      <td>0.001264</td>\n",
       "      <td>1638.414738</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>2</td>\n",
       "      <td>50</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>8</td>\n",
       "      <td>0.002180</td>\n",
       "      <td>1345.933115</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Deep size  Num units  Learning rate  Batch size       MSE  \\\n",
       "0           4        200        0.00010           8  0.000008   \n",
       "1           3        200        0.00100           8  0.000018   \n",
       "2           3        100        0.00100           8  0.000022   \n",
       "3           3        200        0.00010           8  0.000031   \n",
       "4           4         50        0.00010           8  0.000032   \n",
       "5           2        200        0.00100           8  0.000034   \n",
       "6           4        200        0.00001           8  0.000049   \n",
       "7           4        100        0.00010           8  0.000053   \n",
       "8           3        100        0.00010           8  0.000063   \n",
       "9           4         50        0.00100           8  0.000066   \n",
       "10          3         50        0.00100           8  0.000066   \n",
       "11          2        100        0.00010           8  0.000068   \n",
       "12          2         50        0.00100           8  0.000074   \n",
       "13          4        200        0.00100           8  0.000082   \n",
       "14          2        200        0.00010           8  0.000102   \n",
       "15          4        100        0.00100           8  0.000110   \n",
       "16          2        100        0.00100           8  0.000116   \n",
       "17          4        100        0.00001           8  0.000172   \n",
       "18          3        200        0.00001           8  0.000228   \n",
       "19          2         50        0.00010           8  0.000264   \n",
       "20          4         50        0.00001           8  0.000508   \n",
       "21          3        100        0.00001           8  0.000577   \n",
       "22          2        100        0.00001           8  0.000596   \n",
       "23          3         50        0.00001           8  0.000741   \n",
       "24          3         50        0.00010           8  0.000747   \n",
       "25          2        200        0.00001           8  0.001264   \n",
       "26          2         50        0.00001           8  0.002180   \n",
       "\n",
       "    Tiempo de ejecuci√≥n  \n",
       "0            512.230418  \n",
       "1            506.032065  \n",
       "2            285.239467  \n",
       "3            779.619253  \n",
       "4            619.395011  \n",
       "5            254.532893  \n",
       "6           1608.794511  \n",
       "7            315.416821  \n",
       "8            437.513737  \n",
       "9            156.968036  \n",
       "10           135.837533  \n",
       "11           746.657607  \n",
       "12           157.681841  \n",
       "13            96.327686  \n",
       "14           502.455855  \n",
       "15            82.301459  \n",
       "16           123.794255  \n",
       "17          1746.320780  \n",
       "18          1993.593955  \n",
       "19           712.267044  \n",
       "20          1517.532210  \n",
       "21          1597.929133  \n",
       "22          1397.297987  \n",
       "23          1460.272470  \n",
       "24           272.041159  \n",
       "25          1638.414738  \n",
       "26          1345.933115  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vZ3JJWuZqpEB",
    "outputId": "230a8ed2-6429-49df-f611-d68381c31fc3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Tiempo de ejecuci√≥n    350.040017\n",
       "dtype: float64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(df[[\"Tiempo de ejecuci√≥n\"]])/60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "xh9rbb8kqpED"
   },
   "outputs": [],
   "source": [
    "# rm -rf /tmp/tb_logs/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Gm044iKXqpEM"
   },
   "source": [
    "### Now in terminal:\n",
    "`python3 -m tensorboard.main --logdir='/home/isidro/Documents/github/neurapprox/logs/hparam_tuning'`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/sh: 1: kill: No such process\r\n",
      "\r\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Line magic function `%tensorboard` not found.\n"
     ]
    }
   ],
   "source": [
    "!kill 7439\n",
    "%tensorboard --logdir logs/hparam_tuning2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "tunning_classifier.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
