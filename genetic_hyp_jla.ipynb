{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elitism succesfully imported\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import time, os\n",
    "import random\n",
    "import seaborn as sns\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Input, Dense, LeakyReLU\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorboard.plugins.hparams import api as hp\n",
    "\n",
    "from tensorflow.keras import activations\n",
    "from tensorflow.keras.losses import CategoricalCrossentropy\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "import random\n",
    "from math import floor\n",
    "from tensorflow.keras import models\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import optimizers\n",
    "from scipy.integrate import odeint\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split as split\n",
    "from scipy.stats import bernoulli\n",
    "\n",
    "# import deap\n",
    "from deap import base, creator, tools, algorithms\n",
    "\n",
    "from bitstring import BitArray\n",
    "\n",
    "from elitism import eaSimpleWithElitism, main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "\n",
    "# tf.config.optimizer.set_jit(True)\n",
    "scaler = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df=pd.read_csv('data/jla.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = len(df.values)\n",
    "randomize = np.random.permutation(N)\n",
    "data = df.values[randomize]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(740, 2)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N = len(df.values)\n",
    "z = data[:,0] \n",
    "y = data[:,1:3] ### coge el resto de variables a predecir \n",
    "y[:,1] = y[:,1]**2+data[:,2]\n",
    "np.shape(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "scalerz = StandardScaler()\n",
    "scalerz.fit(z.reshape(-1,1))\n",
    "z = scalerz.transform(z.reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((555, 1), (185, 1), (555, 2), (185, 2))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Modificar para incluir phanteon como test\n",
    "split = 0.75\n",
    "ntrain = int(split * len(z))\n",
    "indx = [ntrain]\n",
    "X_train, X_val = np.split(z, indx)\n",
    "Y_train, Y_val = np.split(y, indx)\n",
    "# X_train, X_test = np.split(z, indx)\n",
    "# Y_train, Y_test = np.split(y, indx)\n",
    "np.shape(X_train), np.shape(X_val), Y_train.shape, Y_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "SC_DEEP       = np.array([2,3,4])                           # Number of deep layers (8)\n",
    "SC_NUM_UNITS  = np.array([50,100, 150, 200]) # Number of fully conected neurons (16)\n",
    "SC_LEARNING   = np.array([1e-5,1e-4,5e-3])   # Learning rates (8)\n",
    "SC_BATCH      = np.array([1, 4, 8])                            # Batch sizes (4)\n",
    "# SC_ACTIVATION = [f1, f2, f3, f4]                                      # Activation function layers (2)\n",
    "\n",
    "my_callbacks = [tf.keras.callbacks.EarlyStopping(\n",
    "                               monitor='val_loss', mode='min',\n",
    "                               min_delta=0, \n",
    "                               patience=10,\n",
    "                               verbose=1,\n",
    "                            #    baseline=0,\n",
    "                               restore_best_weights=True)\n",
    "#                 keras.callbacks.TensorBoard(\n",
    "#                                log_dir='./logs'),\n",
    "#                 keras.callbacks.ReduceLROnPlateau(\n",
    "#                                monitor='val_loss', factor=0.5,\n",
    "#                                patience=6, min_lr=0,\n",
    "#                                verbose=1)\n",
    "               ] \n",
    "    \n",
    "epochs = 500\n",
    "# epochs = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_evaluate(ga_individual_solution):   \n",
    "    t = time.time()\n",
    "    t_total = 0\n",
    "    \n",
    "    # Decode GA solution to integer for window_size and num_units\n",
    "    deep_layers_bits   = BitArray(ga_individual_solution[0:1])     # (8)\n",
    "    num_units_bits     = BitArray(ga_individual_solution[1:2])     # (16)\n",
    "    learning_rate_bits = BitArray(ga_individual_solution[2:3])    # (8)\n",
    "    batch_size_bits    = BitArray(ga_individual_solution[3:4])   # (4)\n",
    "# #     activation_f_bits  = BitArray(ga_individual_solution[12:13])   # (2)   Solo se consideran las 2 primeras\n",
    "    \n",
    "    deep_layers   = SC_DEEP[deep_layers_bits.uint]\n",
    "    num_units     = SC_NUM_UNITS[num_units_bits.uint]\n",
    "    learning_rate = SC_LEARNING[learning_rate_bits.uint]\n",
    "    batch_size   = SC_BATCH[batch_size_bits.uint]\n",
    "#     activation_f  = SC_ACTIVATION[activation_f_bits.uint]\n",
    "\n",
    "    \n",
    "    print('\\n--------------- Starting trial:', population_size*(max_generations+1)-len(ss), \"---------------\")\n",
    "    print('Deep layers:',deep_layers,', Number of neurons:',num_units,\", Learning rate:\",learning_rate)\n",
    "#     print(\"-------------------------------------------------\")\n",
    "    \n",
    "    # Train model and predict on validation set\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(Input(shape=(int(X_train.shape[1]),)))\n",
    "#     model.add(Dense(num_units, input_shape=(int(X_train.shape[1]),)))\n",
    "    \n",
    "    for i in range(deep_layers):        \n",
    "        model.add(Dense(num_units, activation='relu'))\n",
    "#             model.add(keras.layers.Dropout(0.3))\n",
    "    model.add(Dense(2, activation='linear'))\n",
    "    \n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate, beta_1=0.9, beta_2=0.999, epsilon=1e-3)\n",
    "    model.compile(optimizer=optimizer, loss='mse', metrics=['mean_squared_error'])\n",
    "    model.fit(X_train, Y_train, epochs=epochs, validation_data=(X_val, Y_val),\n",
    "              callbacks=my_callbacks, batch_size=batch_size, shuffle=1, verbose=0)\n",
    "    \n",
    "    loss, score = model.evaluate(X_val, Y_val)    \n",
    "    t = time.time()-t\n",
    "    ss.pop(0)\n",
    "    print(\"Loss:\", score, \", Elapsed time:\", t)\n",
    "    print(\"-------------------------------------------------\\n\")\n",
    "#     print(loss, score)\n",
    "\n",
    "    datos.append([deep_layers, num_units, learning_rate, batch_size, loss, score, t])\n",
    "    \n",
    "    return loss,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deap import tools\n",
    "from deap import algorithms\n",
    "\n",
    "def eaSimpleWithElitism(population, toolbox, cxpb, mutpb, ngen, stats=None,\n",
    "             halloffame=None, verbose=__debug__):\n",
    "    \"\"\"This algorithm is similar to DEAP eaSimple() algorithm, with the modification that\n",
    "    halloffame is used to implement an elitism mechanism. The individuals contained in the\n",
    "    halloffame are directly injected into the next generation and are not subject to the\n",
    "    genetic operators of selection, crossover and mutation.\n",
    "    \"\"\"\n",
    "    logbook = tools.Logbook()\n",
    "    logbook.header = ['gen', 'nevals'] + (stats.fields if stats else [])\n",
    "\n",
    "    # Evaluate the individuals with an invalid fitness\n",
    "    invalid_ind = [ind for ind in population if not ind.fitness.valid]\n",
    "    fitnesses = toolbox.map(toolbox.evaluate, invalid_ind)\n",
    "    for ind, fit in zip(invalid_ind, fitnesses):\n",
    "        ind.fitness.values = fit\n",
    "\n",
    "    if halloffame is None:\n",
    "        raise ValueError(\"halloffame parameter must not be empty!\")\n",
    "\n",
    "    halloffame.update(population)\n",
    "    hof_size = len(halloffame.items) if halloffame.items else 0\n",
    "\n",
    "    record = stats.compile(population) if stats else {}\n",
    "    logbook.record(gen=0, nevals=len(invalid_ind), **record)\n",
    "    if verbose:\n",
    "        print(logbook.stream)\n",
    "\n",
    "    # Begin the generational process\n",
    "    for gen in range(1, ngen + 1):\n",
    "\n",
    "        # Select the next generation individuals\n",
    "        offspring = toolbox.select(population, len(population) - hof_size)\n",
    "\n",
    "        # Vary the pool of individuals\n",
    "        offspring = algorithms.varAnd(offspring, toolbox, cxpb, mutpb)\n",
    "\n",
    "        # Evaluate the individuals with an invalid fitness\n",
    "        invalid_ind = [ind for ind in offspring if not ind.fitness.valid]\n",
    "        fitnesses = toolbox.map(toolbox.evaluate, invalid_ind)\n",
    "        for ind, fit in zip(invalid_ind, fitnesses):\n",
    "            ind.fitness.values = fit\n",
    "\n",
    "        # add the best back to population:\n",
    "        offspring.extend(halloffame.items)\n",
    "\n",
    "        # Update the hall of fame with the generated individuals\n",
    "        halloffame.update(offspring)\n",
    "\n",
    "        # Replace the current population by the offspring\n",
    "        population[:] = offspring\n",
    "\n",
    "        # Append the current generation statistics to the logbook\n",
    "        record = stats.compile(population) if stats else {}\n",
    "        logbook.record(gen=gen, nevals=len(invalid_ind), **record)\n",
    "        if verbose:\n",
    "            print(logbook.stream)\n",
    "\n",
    "    return population, logbook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def geneticAlgorithm_with_elitism(population_size, max_generations, gene_length, k):\n",
    "    \n",
    "    # Genetic Algorithm constants:\n",
    "    P_CROSSOVER = 0.5        # probability for crossover\n",
    "    P_MUTATION = 0.5         # probability for mutating an individual\n",
    "    HALL_OF_FAME_SIZE = 1    # Best individuals that pass to the other generation\n",
    "    \n",
    "    # set the random seed:\n",
    "    toolbox = base.Toolbox()\n",
    "\n",
    "    # As we are trying to minimize the RMSE score, that's why using -1.0. \n",
    "    # In case, when you want to maximize accuracy for instance, use 1.0\n",
    "    creator.create('FitnessMin', base.Fitness, weights = [-1.0])\n",
    "    creator.create('Individual', list , fitness = creator.FitnessMin)\n",
    "\n",
    "    # create the individual operator to fill up an Individual instance:\n",
    "    toolbox.register('binary', bernoulli.rvs, 0.5)\n",
    "    toolbox.register('individual', tools.initRepeat, creator.Individual, toolbox.binary, n = gene_length)\n",
    "\n",
    "    # create the population operator to generate a list of individuals:\n",
    "    toolbox.register('population', tools.initRepeat, list , toolbox.individual)\n",
    "\n",
    "    # genetic operators:\n",
    "    toolbox.register('evaluate', train_evaluate)\n",
    "    toolbox.register('select', tools.selTournament, tournsize = 2)\n",
    "    toolbox.register('mutate', tools.mutFlipBit, indpb = 0.11)\n",
    "    toolbox.register('mate', tools.cxUniform, indpb = 0.5)\n",
    "    \n",
    "    # create initial population (generation 0):\n",
    "    population = toolbox.population(n=population_size)\n",
    "\n",
    "    # prepare the statistics object:\n",
    "    stats = tools.Statistics(lambda ind: ind.fitness.values)\n",
    "    stats.register(\"min\", np.min)\n",
    "    stats.register(\"avg\", np.mean)\n",
    "    stats.register(\"max\", np.max)\n",
    "\n",
    "    # define the hall-of-fame object:\n",
    "    hof = tools.HallOfFame(HALL_OF_FAME_SIZE)\n",
    "\n",
    "    # Genetic Algorithm flow with elitism:\n",
    "    population, logbook = eaSimpleWithElitism(population, toolbox, cxpb=P_CROSSOVER, mutpb=P_MUTATION,\n",
    "                                              ngen=max_generations, stats=stats, halloffame=hof, verbose=True)\n",
    "\n",
    "    # print info for best solution found:\n",
    "    best = hof.items[0]\n",
    "    print(\"-- Best Individual = \", best)\n",
    "    print(\"-- Best Fitness = \", best.fitness.values[0])\n",
    "\n",
    "    # extract statistics:\n",
    "    minFitnessValues, meanFitnessValues, maxFitnessValues = logbook.select(\"min\", \"max\", \"avg\")\n",
    "\n",
    "    # plot statistics:\n",
    "    sns.set_style(\"whitegrid\")\n",
    "    plt.plot(minFitnessValues, color='blue', label=\"Min\")\n",
    "    plt.plot(meanFitnessValues, color='green', label=\"Mean\")\n",
    "    plt.plot(maxFitnessValues, color='red', label=\"Max\")\n",
    "    plt.xlabel('Generation'); plt.ylabel('Max / Min / Average Fitness')\n",
    "    plt.legend()\n",
    "    plt.title('Max, Min and Average fitness over Generations')\n",
    "    plt.show()\n",
    "    \n",
    "    best_population = tools.selBest(population,k = k)\n",
    "    return best_population"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--------------- Starting trial: 1 ---------------\n",
      "Deep layers: 2 , Number of neurons: 50 , Learning rate: 0.0001\n",
      "WARNING:tensorflow:Please add `keras.layers.InputLayer` instead of `keras.Input` to Sequential model. `keras.Input` is intended to be used by Functional model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/isidro/.local/lib/python3.8/site-packages/deap/creator.py:138: RuntimeWarning: A class named 'FitnessMin' has already been created and it will be overwritten. Consider deleting previous creation of that class or rename it.\n",
      "  warnings.warn(\"A class named '{0}' has already been created and it \"\n",
      "/home/isidro/.local/lib/python3.8/site-packages/deap/creator.py:138: RuntimeWarning: A class named 'Individual' has already been created and it will be overwritten. Consider deleting previous creation of that class or rename it.\n",
      "  warnings.warn(\"A class named '{0}' has already been created and it \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00133: early stopping\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.0330 - mean_squared_error: 0.0330\n",
      "Loss: 0.03298033028841019 , Elapsed time: 91.14679861068726\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 2 ---------------\n",
      "Deep layers: 2 , Number of neurons: 100 , Learning rate: 0.0001\n",
      "WARNING:tensorflow:Please add `keras.layers.InputLayer` instead of `keras.Input` to Sequential model. `keras.Input` is intended to be used by Functional model.\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00226: early stopping\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.0360 - mean_squared_error: 0.0360\n",
      "Loss: 0.03599897399544716 , Elapsed time: 52.46119236946106\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 3 ---------------\n",
      "Deep layers: 2 , Number of neurons: 50 , Learning rate: 1e-05\n",
      "WARNING:tensorflow:Please add `keras.layers.InputLayer` instead of `keras.Input` to Sequential model. `keras.Input` is intended to be used by Functional model.\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.1488 - mean_squared_error: 0.1488\n",
      "Loss: 0.1488226354122162 , Elapsed time: 110.55974626541138\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 4 ---------------\n",
      "Deep layers: 3 , Number of neurons: 100 , Learning rate: 0.0001\n",
      "WARNING:tensorflow:Please add `keras.layers.InputLayer` instead of `keras.Input` to Sequential model. `keras.Input` is intended to be used by Functional model.\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00051: early stopping\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.0385 - mean_squared_error: 0.0385\n",
      "Loss: 0.038475364446640015 , Elapsed time: 38.77179002761841\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 5 ---------------\n",
      "Deep layers: 2 , Number of neurons: 100 , Learning rate: 1e-05\n",
      "WARNING:tensorflow:Please add `keras.layers.InputLayer` instead of `keras.Input` to Sequential model. `keras.Input` is intended to be used by Functional model.\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00440: early stopping\n",
      "6/6 [==============================] - 0s 3ms/step - loss: 0.0445 - mean_squared_error: 0.0445\n",
      "Loss: 0.044531527906656265 , Elapsed time: 365.8559181690216\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 6 ---------------\n",
      "Deep layers: 2 , Number of neurons: 100 , Learning rate: 1e-05\n",
      "WARNING:tensorflow:Please add `keras.layers.InputLayer` instead of `keras.Input` to Sequential model. `keras.Input` is intended to be used by Functional model.\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00322: early stopping\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.0480 - mean_squared_error: 0.0480\n",
      "Loss: 0.04797838628292084 , Elapsed time: 280.1052055358887\n",
      "-------------------------------------------------\n",
      "\n",
      "gen\tnevals\tmin      \tavg      \tmax     \n",
      "0  \t6     \t0.0329803\t0.0581312\t0.148823\n",
      "\n",
      "--------------- Starting trial: 7 ---------------\n",
      "Deep layers: 2 , Number of neurons: 50 , Learning rate: 0.0001\n",
      "WARNING:tensorflow:Please add `keras.layers.InputLayer` instead of `keras.Input` to Sequential model. `keras.Input` is intended to be used by Functional model.\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00063: early stopping\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.0392 - mean_squared_error: 0.0392\n",
      "Loss: 0.03915981203317642 , Elapsed time: 50.616424560546875\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 8 ---------------\n",
      "Deep layers: 2 , Number of neurons: 100 , Learning rate: 0.0001\n",
      "WARNING:tensorflow:Please add `keras.layers.InputLayer` instead of `keras.Input` to Sequential model. `keras.Input` is intended to be used by Functional model.\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00184: early stopping\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.0339 - mean_squared_error: 0.0339\n",
      "Loss: 0.03385906293988228 , Elapsed time: 45.68424367904663\n",
      "-------------------------------------------------\n",
      "\n",
      "1  \t2     \t0.0329803\t0.0360819\t0.0445315\n",
      "\n",
      "--------------- Starting trial: 9 ---------------\n",
      "Deep layers: 2 , Number of neurons: 50 , Learning rate: 0.0001\n",
      "WARNING:tensorflow:Please add `keras.layers.InputLayer` instead of `keras.Input` to Sequential model. `keras.Input` is intended to be used by Functional model.\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00130: early stopping\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.0346 - mean_squared_error: 0.0346\n",
      "Loss: 0.03461641073226929 , Elapsed time: 104.0429036617279\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 10 ---------------\n",
      "Deep layers: 2 , Number of neurons: 100 , Learning rate: 0.0001\n",
      "WARNING:tensorflow:Please add `keras.layers.InputLayer` instead of `keras.Input` to Sequential model. `keras.Input` is intended to be used by Functional model.\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00148: early stopping\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.0358 - mean_squared_error: 0.0358\n",
      "Loss: 0.03580601513385773 , Elapsed time: 41.57104682922363\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 11 ---------------\n",
      "Deep layers: 2 , Number of neurons: 50 , Learning rate: 0.0001\n",
      "WARNING:tensorflow:Please add `keras.layers.InputLayer` instead of `keras.Input` to Sequential model. `keras.Input` is intended to be used by Functional model.\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00144: early stopping\n",
      "6/6 [==============================] - 0s 3ms/step - loss: 0.0337 - mean_squared_error: 0.0337\n",
      "Loss: 0.03371237590909004 , Elapsed time: 113.15356826782227\n",
      "-------------------------------------------------\n",
      "\n",
      "2  \t3     \t0.0329803\t0.033846 \t0.035806 \n",
      "\n",
      "--------------- Starting trial: 12 ---------------\n",
      "Deep layers: 2 , Number of neurons: 100 , Learning rate: 1e-05\n",
      "WARNING:tensorflow:Please add `keras.layers.InputLayer` instead of `keras.Input` to Sequential model. `keras.Input` is intended to be used by Functional model.\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00398: early stopping\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.0473 - mean_squared_error: 0.0473\n",
      "Loss: 0.04731389507651329 , Elapsed time: 335.54838371276855\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 13 ---------------\n",
      "Deep layers: 2 , Number of neurons: 50 , Learning rate: 0.0001\n",
      "WARNING:tensorflow:Please add `keras.layers.InputLayer` instead of `keras.Input` to Sequential model. `keras.Input` is intended to be used by Functional model.\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00091: early stopping\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.0418 - mean_squared_error: 0.0418\n",
      "Loss: 0.04178044572472572 , Elapsed time: 74.54257535934448\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 14 ---------------\n",
      "Deep layers: 2 , Number of neurons: 50 , Learning rate: 0.0001\n",
      "WARNING:tensorflow:Please add `keras.layers.InputLayer` instead of `keras.Input` to Sequential model. `keras.Input` is intended to be used by Functional model.\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00130: early stopping\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.0324 - mean_squared_error: 0.0324\n",
      "Loss: 0.032400019466876984 , Elapsed time: 95.77052903175354\n",
      "-------------------------------------------------\n",
      "\n",
      "3  \t3     \t0.0324   \t0.0367392\t0.0473139\n",
      "\n",
      "--------------- Starting trial: 15 ---------------\n",
      "Deep layers: 2 , Number of neurons: 50 , Learning rate: 0.0001\n",
      "WARNING:tensorflow:Please add `keras.layers.InputLayer` instead of `keras.Input` to Sequential model. `keras.Input` is intended to be used by Functional model.\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00108: early stopping\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 0s 2ms/step - loss: 0.0319 - mean_squared_error: 0.0319\n",
      "Loss: 0.03189583122730255 , Elapsed time: 88.46672821044922\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 16 ---------------\n",
      "Deep layers: 2 , Number of neurons: 50 , Learning rate: 0.0001\n",
      "WARNING:tensorflow:Please add `keras.layers.InputLayer` instead of `keras.Input` to Sequential model. `keras.Input` is intended to be used by Functional model.\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00113: early stopping\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.0326 - mean_squared_error: 0.0326\n",
      "Loss: 0.032554518431425095 , Elapsed time: 92.46610641479492\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 17 ---------------\n",
      "Deep layers: 2 , Number of neurons: 100 , Learning rate: 0.0001\n",
      "WARNING:tensorflow:Please add `keras.layers.InputLayer` instead of `keras.Input` to Sequential model. `keras.Input` is intended to be used by Functional model.\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00074: early stopping\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.0333 - mean_squared_error: 0.0333\n",
      "Loss: 0.03332602605223656 , Elapsed time: 64.82733106613159\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 18 ---------------\n",
      "Deep layers: 2 , Number of neurons: 50 , Learning rate: 1e-05\n",
      "WARNING:tensorflow:Please add `keras.layers.InputLayer` instead of `keras.Input` to Sequential model. `keras.Input` is intended to be used by Functional model.\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00395: early stopping\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.0579 - mean_squared_error: 0.0579\n",
      "Loss: 0.057896655052900314 , Elapsed time: 332.72636103630066\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 19 ---------------\n",
      "Deep layers: 3 , Number of neurons: 50 , Learning rate: 0.0001\n",
      "WARNING:tensorflow:Please add `keras.layers.InputLayer` instead of `keras.Input` to Sequential model. `keras.Input` is intended to be used by Functional model.\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00084: early stopping\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.0312 - mean_squared_error: 0.0312\n",
      "Loss: 0.031172219663858414 , Elapsed time: 61.5582058429718\n",
      "-------------------------------------------------\n",
      "\n",
      "4  \t5     \t0.0311722\t0.0365409\t0.0578967\n",
      "\n",
      "--------------- Starting trial: 20 ---------------\n",
      "Deep layers: 2 , Number of neurons: 50 , Learning rate: 0.0001\n",
      "WARNING:tensorflow:Please add `keras.layers.InputLayer` instead of `keras.Input` to Sequential model. `keras.Input` is intended to be used by Functional model.\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00115: early stopping\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.0343 - mean_squared_error: 0.0343\n",
      "Loss: 0.034313708543777466 , Elapsed time: 92.51275396347046\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 21 ---------------\n",
      "Deep layers: 2 , Number of neurons: 50 , Learning rate: 0.0001\n",
      "WARNING:tensorflow:Please add `keras.layers.InputLayer` instead of `keras.Input` to Sequential model. `keras.Input` is intended to be used by Functional model.\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00095: early stopping\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.0337 - mean_squared_error: 0.0337\n",
      "Loss: 0.03367111459374428 , Elapsed time: 82.11910724639893\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 22 ---------------\n",
      "Deep layers: 2 , Number of neurons: 50 , Learning rate: 1e-05\n",
      "WARNING:tensorflow:Please add `keras.layers.InputLayer` instead of `keras.Input` to Sequential model. `keras.Input` is intended to be used by Functional model.\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.1816 - mean_squared_error: 0.1816\n",
      "Loss: 0.18163509666919708 , Elapsed time: 127.36051535606384\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 23 ---------------\n",
      "Deep layers: 3 , Number of neurons: 50 , Learning rate: 0.0001\n",
      "WARNING:tensorflow:Please add `keras.layers.InputLayer` instead of `keras.Input` to Sequential model. `keras.Input` is intended to be used by Functional model.\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00054: early stopping\n",
      "6/6 [==============================] - 0s 3ms/step - loss: 0.0365 - mean_squared_error: 0.0365\n",
      "Loss: 0.036465492099523544 , Elapsed time: 50.39575743675232\n",
      "-------------------------------------------------\n",
      "\n",
      "5  \t4     \t0.0311722\t0.0582763\t0.181635 \n",
      "\n",
      "--------------- Starting trial: 24 ---------------\n",
      "Deep layers: 3 , Number of neurons: 50 , Learning rate: 1e-05\n",
      "WARNING:tensorflow:Please add `keras.layers.InputLayer` instead of `keras.Input` to Sequential model. `keras.Input` is intended to be used by Functional model.\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.0438 - mean_squared_error: 0.0438\n",
      "Loss: 0.0438154935836792 , Elapsed time: 455.3460958003998\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 25 ---------------\n",
      "Deep layers: 2 , Number of neurons: 50 , Learning rate: 0.0001\n",
      "WARNING:tensorflow:Please add `keras.layers.InputLayer` instead of `keras.Input` to Sequential model. `keras.Input` is intended to be used by Functional model.\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00108: early stopping\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.0337 - mean_squared_error: 0.0337\n",
      "Loss: 0.03372810035943985 , Elapsed time: 89.64575481414795\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 26 ---------------\n",
      "Deep layers: 3 , Number of neurons: 50 , Learning rate: 1e-05\n",
      "WARNING:tensorflow:Please add `keras.layers.InputLayer` instead of `keras.Input` to Sequential model. `keras.Input` is intended to be used by Functional model.\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00455: early stopping\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.0398 - mean_squared_error: 0.0398\n",
      "Loss: 0.0397566556930542 , Elapsed time: 398.9016227722168\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 27 ---------------\n",
      "Deep layers: 2 , Number of neurons: 50 , Learning rate: 0.0001\n",
      "WARNING:tensorflow:Please add `keras.layers.InputLayer` instead of `keras.Input` to Sequential model. `keras.Input` is intended to be used by Functional model.\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00114: early stopping\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.0347 - mean_squared_error: 0.0347\n",
      "Loss: 0.03466687723994255 , Elapsed time: 82.39364385604858\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 28 ---------------\n",
      "Deep layers: 2 , Number of neurons: 50 , Learning rate: 0.0001\n",
      "WARNING:tensorflow:Please add `keras.layers.InputLayer` instead of `keras.Input` to Sequential model. `keras.Input` is intended to be used by Functional model.\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00084: early stopping\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.0376 - mean_squared_error: 0.0376\n",
      "Loss: 0.0375756174325943 , Elapsed time: 82.9157702922821\n",
      "-------------------------------------------------\n",
      "\n",
      "6  \t5     \t0.0311722\t0.0367858\t0.0438155\n",
      "\n",
      "--------------- Starting trial: 29 ---------------\n",
      "Deep layers: 3 , Number of neurons: 50 , Learning rate: 0.0001\n",
      "WARNING:tensorflow:Please add `keras.layers.InputLayer` instead of `keras.Input` to Sequential model. `keras.Input` is intended to be used by Functional model.\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00091: early stopping\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.0300 - mean_squared_error: 0.0300\n",
      "Loss: 0.030039310455322266 , Elapsed time: 80.36278748512268\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 30 ---------------\n",
      "Deep layers: 2 , Number of neurons: 50 , Learning rate: 1e-05\n",
      "WARNING:tensorflow:Please add `keras.layers.InputLayer` instead of `keras.Input` to Sequential model. `keras.Input` is intended to be used by Functional model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 0s 2ms/step - loss: 0.0480 - mean_squared_error: 0.0480\n",
      "Loss: 0.04796483740210533 , Elapsed time: 415.5781686306\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 31 ---------------\n",
      "Deep layers: 3 , Number of neurons: 50 , Learning rate: 1e-05\n",
      "WARNING:tensorflow:Please add `keras.layers.InputLayer` instead of `keras.Input` to Sequential model. `keras.Input` is intended to be used by Functional model.\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00406: early stopping\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.0388 - mean_squared_error: 0.0388\n",
      "Loss: 0.03883045166730881 , Elapsed time: 360.5635333061218\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--------------- Starting trial: 32 ---------------\n",
      "Deep layers: 3 , Number of neurons: 50 , Learning rate: 0.0001\n",
      "WARNING:tensorflow:Please add `keras.layers.InputLayer` instead of `keras.Input` to Sequential model. `keras.Input` is intended to be used by Functional model.\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00069: early stopping\n",
      "6/6 [==============================] - 0s 3ms/step - loss: 0.0346 - mean_squared_error: 0.0346\n",
      "Loss: 0.03460942581295967 , Elapsed time: 64.54143381118774\n",
      "-------------------------------------------------\n",
      "\n",
      "7  \t4     \t0.0300393\t0.0362139\t0.0479648\n",
      "-- Best Individual =  [1, 0, 1, 0, 0, 0, 1, 1, 0, 1]\n",
      "-- Best Fitness =  0.031172221526503563\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total elapsed time: 80.38276205062866 minutes\n"
     ]
    }
   ],
   "source": [
    "population_size = 6   # max of individuals per generation\n",
    "max_generations = 7   # number of generations\n",
    "gene_length = 10       # lenght of the gene, depends on how many hiperparameters are tested  \n",
    "k = 1;                 # num. of finalist individuals\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    t = time.time(); \n",
    "    datos = [];\n",
    "    ss = [i for i in range(1,population_size*(max_generations+1))]\n",
    "    best_population = geneticAlgorithm_with_elitism(population_size, max_generations, gene_length, k)\n",
    "    print(\"Total elapsed time:\", (time.time()-t)/60, \"minutes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k= 1 \n",
      "Deep layers: 3 , Number of neurons: 50\n",
      "Batch size 1 , Learning rate: 0.0001\n"
     ]
    }
   ],
   "source": [
    "best_deep_layers   = []\n",
    "best_num_units     = []\n",
    "best_learning_rate = []\n",
    "best_batch_size    = []\n",
    "# best_activation_f  = []\n",
    "best_f_names       = []\n",
    "\n",
    "t = 0\n",
    "\n",
    "for bi in best_population:\n",
    "    deep_layers_bits   = BitArray(bi[0:1])    # (8)\n",
    "    num_units_bits     = BitArray(bi[1:2])    # (16)\n",
    "    learning_rate_bits = BitArray(bi[2:3])   # (8)\n",
    "    batch_size_bits    = BitArray(bi[3:4])  # (4)\n",
    "#     activation_f_bits  = BitArray(bi[12:13])  # (2)\n",
    "    t += 1 \n",
    "    \n",
    "    best_deep_layers.append(SC_DEEP[deep_layers_bits.uint])\n",
    "    best_num_units.append(SC_NUM_UNITS[num_units_bits.uint])\n",
    "    best_learning_rate.append(SC_LEARNING[learning_rate_bits.uint])\n",
    "    best_batch_size.append(SC_BATCH[batch_size_bits.uint])\n",
    "#     best_activation_f.append(SC_ACTIVATION[activation_f_bits.uint])\n",
    "#     best_f_names.append(f_names[activation_f_bits.uint])\n",
    "    print('k=',t,'\\nDeep layers:', best_deep_layers[-1], ', Number of neurons:', best_num_units[-1])\n",
    "    print('Batch size', best_batch_size[-1], ', Learning rate:', best_learning_rate[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Deep layers</th>\n",
       "      <th>Num units</th>\n",
       "      <th>Learning rate</th>\n",
       "      <th>Batch size</th>\n",
       "      <th>Loss</th>\n",
       "      <th>Score</th>\n",
       "      <th>Elapsed time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>50</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>4</td>\n",
       "      <td>0.181635</td>\n",
       "      <td>0.181635</td>\n",
       "      <td>127.360515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>50</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>4</td>\n",
       "      <td>0.148823</td>\n",
       "      <td>0.148823</td>\n",
       "      <td>110.559746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>50</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>1</td>\n",
       "      <td>0.057897</td>\n",
       "      <td>0.057897</td>\n",
       "      <td>332.726361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>100</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>1</td>\n",
       "      <td>0.047978</td>\n",
       "      <td>0.047978</td>\n",
       "      <td>280.105206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>50</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>1</td>\n",
       "      <td>0.047965</td>\n",
       "      <td>0.047965</td>\n",
       "      <td>415.578169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2</td>\n",
       "      <td>100</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>1</td>\n",
       "      <td>0.047314</td>\n",
       "      <td>0.047314</td>\n",
       "      <td>335.548384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2</td>\n",
       "      <td>100</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>1</td>\n",
       "      <td>0.044532</td>\n",
       "      <td>0.044532</td>\n",
       "      <td>365.855918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>3</td>\n",
       "      <td>50</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>1</td>\n",
       "      <td>0.043815</td>\n",
       "      <td>0.043815</td>\n",
       "      <td>455.346096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2</td>\n",
       "      <td>50</td>\n",
       "      <td>0.00010</td>\n",
       "      <td>1</td>\n",
       "      <td>0.041780</td>\n",
       "      <td>0.041780</td>\n",
       "      <td>74.542575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>3</td>\n",
       "      <td>50</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>1</td>\n",
       "      <td>0.039757</td>\n",
       "      <td>0.039757</td>\n",
       "      <td>398.901623</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2</td>\n",
       "      <td>50</td>\n",
       "      <td>0.00010</td>\n",
       "      <td>1</td>\n",
       "      <td>0.039160</td>\n",
       "      <td>0.039160</td>\n",
       "      <td>50.616425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>3</td>\n",
       "      <td>50</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>1</td>\n",
       "      <td>0.038830</td>\n",
       "      <td>0.038830</td>\n",
       "      <td>360.563533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>3</td>\n",
       "      <td>100</td>\n",
       "      <td>0.00010</td>\n",
       "      <td>1</td>\n",
       "      <td>0.038475</td>\n",
       "      <td>0.038475</td>\n",
       "      <td>38.771790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2</td>\n",
       "      <td>50</td>\n",
       "      <td>0.00010</td>\n",
       "      <td>1</td>\n",
       "      <td>0.037576</td>\n",
       "      <td>0.037576</td>\n",
       "      <td>82.915770</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>3</td>\n",
       "      <td>50</td>\n",
       "      <td>0.00010</td>\n",
       "      <td>1</td>\n",
       "      <td>0.036465</td>\n",
       "      <td>0.036465</td>\n",
       "      <td>50.395757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2</td>\n",
       "      <td>100</td>\n",
       "      <td>0.00010</td>\n",
       "      <td>4</td>\n",
       "      <td>0.035999</td>\n",
       "      <td>0.035999</td>\n",
       "      <td>52.461192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2</td>\n",
       "      <td>100</td>\n",
       "      <td>0.00010</td>\n",
       "      <td>4</td>\n",
       "      <td>0.035806</td>\n",
       "      <td>0.035806</td>\n",
       "      <td>41.571047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2</td>\n",
       "      <td>50</td>\n",
       "      <td>0.00010</td>\n",
       "      <td>1</td>\n",
       "      <td>0.034667</td>\n",
       "      <td>0.034667</td>\n",
       "      <td>82.393644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2</td>\n",
       "      <td>50</td>\n",
       "      <td>0.00010</td>\n",
       "      <td>1</td>\n",
       "      <td>0.034616</td>\n",
       "      <td>0.034616</td>\n",
       "      <td>104.042904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>3</td>\n",
       "      <td>50</td>\n",
       "      <td>0.00010</td>\n",
       "      <td>1</td>\n",
       "      <td>0.034609</td>\n",
       "      <td>0.034609</td>\n",
       "      <td>64.541434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>2</td>\n",
       "      <td>50</td>\n",
       "      <td>0.00010</td>\n",
       "      <td>1</td>\n",
       "      <td>0.034314</td>\n",
       "      <td>0.034314</td>\n",
       "      <td>92.512754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>2</td>\n",
       "      <td>100</td>\n",
       "      <td>0.00010</td>\n",
       "      <td>4</td>\n",
       "      <td>0.033859</td>\n",
       "      <td>0.033859</td>\n",
       "      <td>45.684244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>2</td>\n",
       "      <td>50</td>\n",
       "      <td>0.00010</td>\n",
       "      <td>1</td>\n",
       "      <td>0.033728</td>\n",
       "      <td>0.033728</td>\n",
       "      <td>89.645755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>2</td>\n",
       "      <td>50</td>\n",
       "      <td>0.00010</td>\n",
       "      <td>1</td>\n",
       "      <td>0.033712</td>\n",
       "      <td>0.033712</td>\n",
       "      <td>113.153568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>2</td>\n",
       "      <td>50</td>\n",
       "      <td>0.00010</td>\n",
       "      <td>1</td>\n",
       "      <td>0.033671</td>\n",
       "      <td>0.033671</td>\n",
       "      <td>82.119107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>2</td>\n",
       "      <td>100</td>\n",
       "      <td>0.00010</td>\n",
       "      <td>1</td>\n",
       "      <td>0.033326</td>\n",
       "      <td>0.033326</td>\n",
       "      <td>64.827331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>2</td>\n",
       "      <td>50</td>\n",
       "      <td>0.00010</td>\n",
       "      <td>1</td>\n",
       "      <td>0.032980</td>\n",
       "      <td>0.032980</td>\n",
       "      <td>91.146799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>2</td>\n",
       "      <td>50</td>\n",
       "      <td>0.00010</td>\n",
       "      <td>1</td>\n",
       "      <td>0.032555</td>\n",
       "      <td>0.032555</td>\n",
       "      <td>92.466106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>2</td>\n",
       "      <td>50</td>\n",
       "      <td>0.00010</td>\n",
       "      <td>1</td>\n",
       "      <td>0.032400</td>\n",
       "      <td>0.032400</td>\n",
       "      <td>95.770529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>2</td>\n",
       "      <td>50</td>\n",
       "      <td>0.00010</td>\n",
       "      <td>1</td>\n",
       "      <td>0.031896</td>\n",
       "      <td>0.031896</td>\n",
       "      <td>88.466728</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>3</td>\n",
       "      <td>50</td>\n",
       "      <td>0.00010</td>\n",
       "      <td>1</td>\n",
       "      <td>0.031172</td>\n",
       "      <td>0.031172</td>\n",
       "      <td>61.558206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>3</td>\n",
       "      <td>50</td>\n",
       "      <td>0.00010</td>\n",
       "      <td>1</td>\n",
       "      <td>0.030039</td>\n",
       "      <td>0.030039</td>\n",
       "      <td>80.362787</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Deep layers  Num units  Learning rate  Batch size      Loss     Score  \\\n",
       "0             2         50        0.00001           4  0.181635  0.181635   \n",
       "1             2         50        0.00001           4  0.148823  0.148823   \n",
       "2             2         50        0.00001           1  0.057897  0.057897   \n",
       "3             2        100        0.00001           1  0.047978  0.047978   \n",
       "4             2         50        0.00001           1  0.047965  0.047965   \n",
       "5             2        100        0.00001           1  0.047314  0.047314   \n",
       "6             2        100        0.00001           1  0.044532  0.044532   \n",
       "7             3         50        0.00001           1  0.043815  0.043815   \n",
       "8             2         50        0.00010           1  0.041780  0.041780   \n",
       "9             3         50        0.00001           1  0.039757  0.039757   \n",
       "10            2         50        0.00010           1  0.039160  0.039160   \n",
       "11            3         50        0.00001           1  0.038830  0.038830   \n",
       "12            3        100        0.00010           1  0.038475  0.038475   \n",
       "13            2         50        0.00010           1  0.037576  0.037576   \n",
       "14            3         50        0.00010           1  0.036465  0.036465   \n",
       "15            2        100        0.00010           4  0.035999  0.035999   \n",
       "16            2        100        0.00010           4  0.035806  0.035806   \n",
       "17            2         50        0.00010           1  0.034667  0.034667   \n",
       "18            2         50        0.00010           1  0.034616  0.034616   \n",
       "19            3         50        0.00010           1  0.034609  0.034609   \n",
       "20            2         50        0.00010           1  0.034314  0.034314   \n",
       "21            2        100        0.00010           4  0.033859  0.033859   \n",
       "22            2         50        0.00010           1  0.033728  0.033728   \n",
       "23            2         50        0.00010           1  0.033712  0.033712   \n",
       "24            2         50        0.00010           1  0.033671  0.033671   \n",
       "25            2        100        0.00010           1  0.033326  0.033326   \n",
       "26            2         50        0.00010           1  0.032980  0.032980   \n",
       "27            2         50        0.00010           1  0.032555  0.032555   \n",
       "28            2         50        0.00010           1  0.032400  0.032400   \n",
       "29            2         50        0.00010           1  0.031896  0.031896   \n",
       "30            3         50        0.00010           1  0.031172  0.031172   \n",
       "31            3         50        0.00010           1  0.030039  0.030039   \n",
       "\n",
       "    Elapsed time  \n",
       "0     127.360515  \n",
       "1     110.559746  \n",
       "2     332.726361  \n",
       "3     280.105206  \n",
       "4     415.578169  \n",
       "5     335.548384  \n",
       "6     365.855918  \n",
       "7     455.346096  \n",
       "8      74.542575  \n",
       "9     398.901623  \n",
       "10     50.616425  \n",
       "11    360.563533  \n",
       "12     38.771790  \n",
       "13     82.915770  \n",
       "14     50.395757  \n",
       "15     52.461192  \n",
       "16     41.571047  \n",
       "17     82.393644  \n",
       "18    104.042904  \n",
       "19     64.541434  \n",
       "20     92.512754  \n",
       "21     45.684244  \n",
       "22     89.645755  \n",
       "23    113.153568  \n",
       "24     82.119107  \n",
       "25     64.827331  \n",
       "26     91.146799  \n",
       "27     92.466106  \n",
       "28     95.770529  \n",
       "29     88.466728  \n",
       "30     61.558206  \n",
       "31     80.362787  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filename = \"historial_genetic_jla.txt\"\n",
    "df = pd.DataFrame(datos, columns = [\"Deep layers\", \"Num units\", \"Learning rate\", \"Batch size\", \"Loss\", \"Score\", \"Elapsed time\"])\n",
    "\n",
    "df.sort_values(by=[\"Loss\", \"Elapsed time\"], ascending=[0,1], ignore_index=True, inplace=True)\n",
    "\n",
    "df.to_csv(filename, header=True, index=False, sep='\\t', mode='w') # a=append, w=overwrite\n",
    "df.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time 80.375 minutes\n"
     ]
    }
   ],
   "source": [
    "total_time = float(np.sum(df[[\"Elapsed time\"]])/60)\n",
    "\n",
    "print(\"Elapsed time {:.3f} minutes\".format(total_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
